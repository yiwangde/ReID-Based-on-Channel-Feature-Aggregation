Traceback (most recent call last):
  File "train.py", line 6, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'
Traceback (most recent call last):
  File "train.py", line 6, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'
This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7faa884dddd0>]
2.503671407699585
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU): LeakyReLU(negative_slope=0.01)
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU): LeakyReLU(negative_slope=0.01)
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU): LeakyReLU(negative_slope=0.01)
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (LeakyReLU): LeakyReLU(negative_slope=0.01)
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU): LeakyReLU(negative_slope=0.01)
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU): LeakyReLU(negative_slope=0.01)
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU): LeakyReLU(negative_slope=0.01)
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (LeakyReLU): LeakyReLU(negative_slope=0.01)
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU): LeakyReLU(negative_slope=0.01)
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU): LeakyReLU(negative_slope=0.01)
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU): LeakyReLU(negative_slope=0.01)
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU): LeakyReLU(negative_slope=0.01)
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU): LeakyReLU(negative_slope=0.01)
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (LeakyReLU): LeakyReLU(negative_slope=0.01)
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU): LeakyReLU(negative_slope=0.01)
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU): LeakyReLU(negative_slope=0.01)
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=751, bias=True)
    )
  )
)
Epoch 0/79
----------
/home/luguangjian/.conda/envs/test_code/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Traceback (most recent call last):
  File "train.py", line 425, in <module>
    model = train_model(model, optimizer_ft, exp_lr_scheduler, criterion, num_epochs=80, local_rank=opt.local_rank)
  File "train.py", line 233, in train_model
    outputs = model(inputs)
  File "/home/luguangjian/.conda/envs/test_code/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/luguangjian/tmp2/reid/model.py", line 125, in forward
    x = self.model.layer1(x)
  File "/home/luguangjian/.conda/envs/test_code/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/luguangjian/.conda/envs/test_code/lib/python3.7/site-packages/torch/nn/modules/container.py", line 100, in forward
    input = module(input)
  File "/home/luguangjian/.conda/envs/test_code/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/luguangjian/tmp2/reid/resnest/resnet.py", line 117, in forward
    out = self.conv2(out)
  File "/home/luguangjian/.conda/envs/test_code/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/luguangjian/tmp2/reid/resnest/splat.py", line 56, in forward
    x = self.LeakyReLU_Sin(x)
  File "/home/luguangjian/.conda/envs/test_code/lib/python3.7/site-packages/torch/nn/modules/module.py", line 594, in __getattr__
    type(self).__name__, name))
AttributeError: 'SplAtConv2d' object has no attribute 'LeakyReLU_Sin'
This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7f51d89e5d50>]
2.4542360305786133
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=751, bias=True)
    )
  )
)
Epoch 0/79
----------
/home/luguangjian/.conda/envs/test_code/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 0.9394 Acc: 0.1061
val Loss: 3.7610 Acc: 0.2370
Training complete in 3m 7s

Epoch 1/79
----------
train Loss: 1.0405 Acc: 0.3764
val Loss: 2.1267 Acc: 0.5047
Training complete in 5m 45s

Epoch 2/79
----------
train Loss: 1.0444 Acc: 0.5513
val Loss: 1.5297 Acc: 0.6192
Training complete in 8m 30s

Epoch 3/79
----------
train Loss: 1.1082 Acc: 0.6254
val Loss: 1.2239 Acc: 0.6791
Training complete in 11m 23s

Epoch 4/79
----------
train Loss: 1.2162 Acc: 0.6720
val Loss: 0.7715 Acc: 0.7816
Training complete in 14m 12s

Epoch 5/79
----------
train Loss: 1.0669 Acc: 0.7308
val Loss: 0.6323 Acc: 0.8056
Training complete in 16m 48s

Epoch 6/79
----------
train Loss: 0.8789 Acc: 0.7743
val Loss: 0.8619 Acc: 0.7736
Training complete in 19m 40s

Epoch 7/79
----------
train Loss: 0.7147 Acc: 0.8144
val Loss: 0.4243 Acc: 0.8615
Training complete in 22m 46s

Epoch 8/79
----------
train Loss: 0.6101 Acc: 0.8366
val Loss: 0.4319 Acc: 0.8695
Training complete in 25m 46s

Epoch 9/79
----------
train Loss: 0.5775 Acc: 0.8513
val Loss: 0.4014 Acc: 0.8748
Training complete in 28m 48s

Epoch 10/79
----------
train Loss: 0.5265 Acc: 0.8624
val Loss: 0.3182 Acc: 0.9081
Training complete in 31m 45s

Epoch 11/79
----------
train Loss: 0.5092 Acc: 0.8675
val Loss: 0.4561 Acc: 0.8762
Training complete in 34m 41s

Epoch 12/79
----------
train Loss: 0.4919 Acc: 0.8728
val Loss: 0.3428 Acc: 0.8895
Training complete in 37m 45s

Epoch 13/79
----------
train Loss: 0.4524 Acc: 0.8832
val Loss: 0.3685 Acc: 0.8921
Training complete in 40m 47s

Epoch 14/79
----------
train Loss: 0.4259 Acc: 0.8896
val Loss: 0.4264 Acc: 0.8881
Training complete in 43m 50s

Epoch 15/79
----------
train Loss: 0.3904 Acc: 0.8986
val Loss: 0.3091 Acc: 0.9108
Training complete in 46m 54s

Epoch 16/79
----------
train Loss: 0.3770 Acc: 0.9024
val Loss: 0.2934 Acc: 0.9028
Training complete in 49m 32s

Epoch 17/79
----------
train Loss: 0.3953 Acc: 0.8948
val Loss: 0.3588 Acc: 0.8895
Training complete in 52m 10s

Epoch 18/79
----------
train Loss: 0.3791 Acc: 0.9009
val Loss: 0.3999 Acc: 0.8975
Training complete in 55m 10s

Epoch 19/79
----------
train Loss: 0.3560 Acc: 0.9078
val Loss: 0.3157 Acc: 0.9068
Training complete in 57m 52s

Epoch 20/79
----------
train Loss: 0.3415 Acc: 0.9155
val Loss: 0.2746 Acc: 0.9228
Training complete in 60m 55s

Epoch 21/79
----------
train Loss: 0.3156 Acc: 0.9189
val Loss: 0.2741 Acc: 0.9214
Training complete in 63m 53s

Epoch 22/79
----------
train Loss: 0.3083 Acc: 0.9229
val Loss: 0.2719 Acc: 0.9254
Training complete in 66m 29s

Epoch 23/79
----------
train Loss: 0.3214 Acc: 0.9173
val Loss: 0.2968 Acc: 0.9214
Training complete in 69m 6s

Epoch 24/79
----------
train Loss: 0.3275 Acc: 0.9139
val Loss: 0.3198 Acc: 0.9001
Training complete in 72m 8s

Epoch 25/79
----------
train Loss: 0.3322 Acc: 0.9142
val Loss: 0.2663 Acc: 0.9268
Training complete in 74m 58s

Epoch 26/79
----------
train Loss: 0.3165 Acc: 0.9192
val Loss: 0.2734 Acc: 0.9281
Training complete in 77m 40s

Epoch 27/79
----------
train Loss: 0.3176 Acc: 0.9205
val Loss: 0.3025 Acc: 0.9268
Training complete in 80m 16s

Epoch 28/79
----------
train Loss: 0.2821 Acc: 0.9269
val Loss: 0.2920 Acc: 0.9068
Training complete in 82m 53s

Epoch 29/79
----------
train Loss: 0.2654 Acc: 0.9315
val Loss: 0.2126 Acc: 0.9294
Training complete in 85m 53s

Epoch 30/79
----------
train Loss: 0.2722 Acc: 0.9305
val Loss: 0.2538 Acc: 0.9228
Training complete in 88m 31s

Epoch 31/79
----------
train Loss: 0.2900 Acc: 0.9257
val Loss: 0.3556 Acc: 0.8975
Training complete in 91m 9s

Epoch 32/79
----------
train Loss: 0.2858 Acc: 0.9276
val Loss: 0.2862 Acc: 0.9188
Training complete in 94m 10s

Epoch 33/79
----------
train Loss: 0.2518 Acc: 0.9383
val Loss: 0.2431 Acc: 0.9201
Training complete in 97m 8s

Epoch 34/79
----------
train Loss: 0.2739 Acc: 0.9302
val Loss: 0.2239 Acc: 0.9254
Training complete in 100m 3s

Epoch 35/79
----------
train Loss: 0.2848 Acc: 0.9285
val Loss: 0.2234 Acc: 0.9321
Training complete in 103m 6s

Epoch 36/79
----------
train Loss: 0.2680 Acc: 0.9330
val Loss: 0.2416 Acc: 0.9228
Training complete in 106m 7s

Epoch 37/79
----------
train Loss: 0.2400 Acc: 0.9394
val Loss: 0.2375 Acc: 0.9241
Training complete in 108m 43s

Epoch 38/79
----------
train Loss: 0.2272 Acc: 0.9457
val Loss: 0.2451 Acc: 0.9241
Training complete in 111m 39s

Epoch 39/79
----------
train Loss: 0.1306 Acc: 0.9693
val Loss: 0.1473 Acc: 0.9587
Training complete in 114m 24s

Epoch 40/79
----------
train Loss: 0.0883 Acc: 0.9815
val Loss: 0.1303 Acc: 0.9627
Training complete in 117m 41s

Epoch 41/79
----------
train Loss: 0.0714 Acc: 0.9851
val Loss: 0.1191 Acc: 0.9627
Training complete in 120m 45s

Epoch 42/79
----------
train Loss: 0.0639 Acc: 0.9889
val Loss: 0.1140 Acc: 0.9587
Training complete in 123m 23s

Epoch 43/79
----------
train Loss: 0.0601 Acc: 0.9904
val Loss: 0.1111 Acc: 0.9627
Training complete in 126m 1s

Epoch 44/79
----------
train Loss: 0.0515 Acc: 0.9914
val Loss: 0.1119 Acc: 0.9640
Training complete in 129m 4s

Epoch 45/79
----------
train Loss: 0.0537 Acc: 0.9911
val Loss: 0.1119 Acc: 0.9640
Training complete in 131m 56s

Epoch 46/79
----------
train Loss: 0.0508 Acc: 0.9927
val Loss: 0.1057 Acc: 0.9640
Training complete in 134m 33s

Epoch 47/79
----------
train Loss: 0.0493 Acc: 0.9923
val Loss: 0.1055 Acc: 0.9654
Training complete in 137m 34s

Epoch 48/79
----------
train Loss: 0.0468 Acc: 0.9935
val Loss: 0.1124 Acc: 0.9627
Training complete in 140m 11s

Epoch 49/79
----------
train Loss: 0.0468 Acc: 0.9930
val Loss: 0.1088 Acc: 0.9640
Training complete in 142m 49s

Epoch 50/79
----------
train Loss: 0.0468 Acc: 0.9928
val Loss: 0.1113 Acc: 0.9627
Training complete in 145m 49s

Epoch 51/79
----------
train Loss: 0.0476 Acc: 0.9933
val Loss: 0.1130 Acc: 0.9680
Training complete in 148m 26s

Epoch 52/79
----------
train Loss: 0.0467 Acc: 0.9932
val Loss: 0.1119 Acc: 0.9654
Training complete in 151m 3s

Epoch 53/79
----------
train Loss: 0.0481 Acc: 0.9934
val Loss: 0.1032 Acc: 0.9720
Training complete in 154m 3s

Epoch 54/79
----------
train Loss: 0.0441 Acc: 0.9950
val Loss: 0.1044 Acc: 0.9694
Training complete in 156m 41s

Epoch 55/79
----------
train Loss: 0.0453 Acc: 0.9939
val Loss: 0.0992 Acc: 0.9734
Training complete in 159m 43s

Epoch 56/79
----------
train Loss: 0.0442 Acc: 0.9953
val Loss: 0.1016 Acc: 0.9734
Training complete in 162m 20s

Epoch 57/79
----------
train Loss: 0.0429 Acc: 0.9958
val Loss: 0.1022 Acc: 0.9707
Training complete in 165m 20s

Epoch 58/79
----------
train Loss: 0.0438 Acc: 0.9952
val Loss: 0.1117 Acc: 0.9694
Training complete in 168m 10s

Epoch 59/79
----------
train Loss: 0.0428 Acc: 0.9954
val Loss: 0.1071 Acc: 0.9707
Training complete in 170m 48s

Epoch 60/79
----------
train Loss: 0.0458 Acc: 0.9947
val Loss: 0.1078 Acc: 0.9720
Training complete in 173m 23s

Epoch 61/79
----------
train Loss: 0.0483 Acc: 0.9948
val Loss: 0.1120 Acc: 0.9707
Training complete in 176m 23s

Epoch 62/79
----------
train Loss: 0.0442 Acc: 0.9955
val Loss: 0.1111 Acc: 0.9707
Training complete in 179m 6s

Epoch 63/79
----------
train Loss: 0.0475 Acc: 0.9956
val Loss: 0.1173 Acc: 0.9720
Training complete in 182m 9s

Epoch 64/79
----------
train Loss: 0.0412 Acc: 0.9966
val Loss: 0.1148 Acc: 0.9707
Training complete in 185m 9s

Epoch 65/79
----------
train Loss: 0.0428 Acc: 0.9963
val Loss: 0.1093 Acc: 0.9734
Training complete in 188m 11s

Epoch 66/79
----------
train Loss: 0.0428 Acc: 0.9962
val Loss: 0.1144 Acc: 0.9680
Training complete in 190m 47s

Epoch 67/79
----------
train Loss: 0.0420 Acc: 0.9970
val Loss: 0.1145 Acc: 0.9667
Training complete in 193m 23s

Epoch 68/79
----------
train Loss: 0.0429 Acc: 0.9970
val Loss: 0.1181 Acc: 0.9680
Training complete in 196m 24s

Epoch 69/79
----------
train Loss: 0.0435 Acc: 0.9964
val Loss: 0.1145 Acc: 0.9694
Training complete in 199m 9s

Epoch 70/79
----------
train Loss: 0.0419 Acc: 0.9966
val Loss: 0.1113 Acc: 0.9680
Training complete in 201m 46s

Epoch 71/79
----------
train Loss: 0.0406 Acc: 0.9972
val Loss: 0.1122 Acc: 0.9720
Training complete in 204m 49s

Epoch 72/79
----------
train Loss: 0.0404 Acc: 0.9972
val Loss: 0.1141 Acc: 0.9680
Training complete in 208m 34s

Epoch 73/79
----------
train Loss: 0.0434 Acc: 0.9969
val Loss: 0.1141 Acc: 0.9680
Training complete in 212m 11s

Epoch 74/79
----------
train Loss: 0.0402 Acc: 0.9973
val Loss: 0.1130 Acc: 0.9707
Training complete in 215m 16s

Epoch 75/79
----------
train Loss: 0.0424 Acc: 0.9965
val Loss: 0.1164 Acc: 0.9720
Training complete in 218m 24s

Epoch 76/79
----------
train Loss: 0.0406 Acc: 0.9970
val Loss: 0.1065 Acc: 0.9707
Training complete in 221m 12s

Epoch 77/79
----------
train Loss: 0.0419 Acc: 0.9968
val Loss: 0.1166 Acc: 0.9707
Training complete in 224m 14s

Epoch 78/79
----------
train Loss: 0.0407 Acc: 0.9978
val Loss: 0.1145 Acc: 0.9707
Training complete in 227m 17s

Epoch 79/79
----------
train Loss: 0.0394 Acc: 0.9975
val Loss: 0.1058 Acc: 0.9734
Training complete in 230m 24s

Training complete in 230m 24s
This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
We use the scale: 1
-------test-----------
test.py:50: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(stream)
This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
We use the scale: 1
-------test-----------
test.py:50: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(stream)
8
16
24
32
40
48
56
64
72
80
88
96
104
112
120
128
136
144
152
160
168
176
184
192
200
208
216
224
232
240
248
256
264
272
280
288
296
304
312
320
328
336
344
352
360
368
376
384
392
400
408
416
424
432
440
448
456
464
472
480
488
496
504
512
520
528
536
544
552
560
568
576
584
592
600
608
616
624
632
640
648
656
664
672
680
688
696
704
712
720
728
736
744
752
760
768
776
784
792
800
808
816
824
832
840
848
856
864
872
880
888
896
904
912
920
928
936
944
952
960
968
976
984
992
1000
1008
1016
1024
1032
1040
1048
1056
1064
1072
1080
1088
1096
1104
1112
1120
1128
1136
1144
1152
1160
1168
1176
1184
1192
1200
1208
1216
1224
1232
1240
1248
1256
1264
1272
1280
1288
1296
1304
1312
1320
1328
1336
1344
1352
1360
1368
1376
1384
1392
1400
1408
1416
1424
1432
1440
1448
1456
1464
1472
1480
1488
1496
1504
1512
1520
1528
1536
1544
1552
1560
1568
1576
1584
1592
1600
1608
1616
1624
1632
1640
1648
1656
1664
1672
1680
1688
1696
1704
1712
1720
1728
1736
1744
1752
1760
1768
1776
1784
1792
1800
1808
1816
1824
1832
1840
1848
1856
1864
1872
1880
1888
1896
1904
1912
1920
1928
1936
1944
1952
1960
1968
1976
1984
1992
2000
2008
2016
2024
2032
2040
2048
2056
2064
2072
2080
2088
2096
2104
2112
2120
2128
2136
2144
2152
2160
2168
2176
2184
2192
2200
2208
2216
2224
2232
2240
2248
2256
2264
2272
2280
2288
2296
2304
2312
2320
2328
2336
2344
2352
2360
2368
2376
2384
2392
2400
2408
2416
2424
2432
2440
2448
2456
2464
2472
2480
2488
2496
2504
2512
2520
2528
2536
2544
2552
2560
2568
2576
2584
2592
2600
2608
2616
2624
2632
2640
2648
2656
2664
2672
2680
2688
2696
2704
2712
2720
2728
2736
2744
2752
2760
2768
2776
2784
2792
2800
2808
2816
2824
2832
2840
2848
2856
2864
2872
2880
2888
2896
2904
2912
2920
2928
2936
2944
2952
2960
2968
2976
2984
2992
3000
3008
3016
3024
3032
3040
3048
3056
3064
3072
3080
3088
3096
3104
3112
3120
3128
3136
3144
3152
3160
3168
3176
3184
3192
3200
3208
3216
3224
3232
3240
3248
3256
3264
3272
3280
3288
3296
3304
3312
3320
3328
3336
3344
3352
3360
3368
3376
3384
3392
3400
3408
3416
3424
3432
3440
3448
3456
3464
3472
3480
3488
3496
3504
3512
3520
3528
3536
3544
3552
3560
3568
3576
3584
3592
3600
3608
3616
3624
3632
3640
3648
3656
3664
3672
3680
3688
3696
3704
3712
3720
3728
3736
3744
3752
3760
3768
3776
3784
3792
3800
3808
3816
3824
3832
3840
3848
3856
3864
3872
3880
3888
3896
3904
3912
3920
3928
3936
3944
3952
3960
3968
3976
3984
3992
4000
4008
4016
4024
4032
4040
4048
4056
4064
4072
4080
4088
4096
4104
4112
4120
4128
4136
4144
4152
4160
4168
4176
4184
4192
4200
4208
4216
4224
4232
4240
4248
4256
4264
4272
4280
4288
4296
4304
4312
4320
4328
4336
4344
4352
4360
4368
4376
4384
4392
4400
4408
4416
4424
4432
4440
4448
4456
4464
4472
4480
4488
4496
4504
4512
4520
4528
4536
4544
4552
4560
4568
4576
4584
4592
4600
4608
4616
4624
4632
4640
4648
4656
4664
4672
4680
4688
4696
4704
4712
4720
4728
4736
4744
4752
4760
4768
4776
4784
4792
4800
4808
4816
4824
4832
4840
4848
4856
4864
4872
4880
4888
4896
4904
4912
4920
4928
4936
4944
4952
4960
4968
4976
4984
4992
5000
5008
5016
5024
5032
5040
5048
5056
5064
5072
5080
5088
5096
5104
5112
5120
5128
5136
5144
5152
5160
5168
5176
5184
5192
5200
5208
5216
5224
5232
5240
5248
5256
5264
5272
5280
5288
5296
5304
5312
5320
5328
5336
5344
5352
5360
5368
5376
5384
5392
5400
5408
5416
5424
5432
5440
5448
5456
5464
5472
5480
5488
5496
5504
5512
5520
5528
5536
5544
5552
5560
5568
5576
5584
5592
5600
5608
5616
5624
5632
5640
5648
5656
5664
5672
5680
5688
5696
5704
5712
5720
5728
5736
5744
5752
5760
5768
5776
5784
5792
5800
5808
5816
5824
5832
5840
5848
5856
5864
5872
5880
5888
5896
5904
5912
5920
5928
5936
5944
5952
5960
5968
5976
5984
5992
6000
6008
6016
6024
6032
6040
6048
6056
6064
6072
6080
6088
6096
6104
6112
6120
6128
6136
6144
6152
6160
6168
6176
6184
6192
6200
6208
6216
6224
6232
6240
6248
6256
6264
6272
6280
6288
6296
6304
6312
6320
6328
6336
6344
6352
6360
6368
6376
6384
6392
6400
6408
6416
6424
6432
6440
6448
6456
6464
6472
6480
6488
6496
6504
6512
6520
6528
6536
6544
6552
6560
6568
6576
6584
6592
6600
6608
6616
6624
6632
6640
6648
6656
6664
6672
6680
6688
6696
6704
6712
6720
6728
6736
6744
6752
6760
6768
6776
6784
6792
6800
6808
6816
6824
6832
6840
6848
6856
6864
6872
6880
6888
6896
6904
6912
6920
6928
6936
6944
6952
6960
6968
6976
6984
6992
7000
7008
7016
7024
7032
7040
7048
7056
7064
7072
7080
7088
7096
7104
7112
7120
7128
7136
7144
7152
7160
7168
7176
7184
7192
7200
7208
7216
7224
7232
7240
7248
7256
7264
7272
7280
7288
7296
7304
7312
7320
7328
7336
7344
7352
7360
7368
7376
7384
7392
7400
7408
7416
7424
7432
7440
7448
7456
7464
7472
7480
7488
7496
7504
7512
7520
7528
7536
7544
7552
7560
7568
7576
7584
7592
7600
7608
7616
7624
7632
7640
7648
7656
7664
7672
7680
7688
7696
7704
7712
7720
7728
7736
7744
7752
7760
7768
7776
7784
7792
7800
7808
7816
7824
7832
7840
7848
7856
7864
7872
7880
7888
7896
7904
7912
7920
7928
7936
7944
7952
7960
7968
7976
7984
7992
8000
8008
8016
8024
8032
8040
8048
8056
8064
8072
8080
8088
8096
8104
8112
8120
8128
8136
8144
8152
8160
8168
8176
8184
8192
8200
8208
8216
8224
8232
8240
8248
8256
8264
8272
8280
8288
8296
8304
8312
8320
8328
8336
8344
8352
8360
8368
8376
8384
8392
8400
8408
8416
8424
8432
8440
8448
8456
8464
8472
8480
8488
8496
8504
8512
8520
8528
8536
8544
8552
8560
8568
8576
8584
8592
8600
8608
8616
8624
8632
8640
8648
8656
8664
8672
8680
8688
8696
8704
8712
8720
8728
8736
8744
8752
8760
8768
8776
8784
8792
8800
8808
8816
8824
8832
8840
8848
8856
8864
8872
8880
8888
8896
8904
8912
8920
8928
8936
8944
8952
8960
8968
8976
8984
8992
9000
9008
9016
9024
9032
9040
9048
9056
9064
9072
9080
9088
9096
9104
9112
9120
9128
9136
9144
9152
9160
9168
9176
9184
9192
9200
9208
9216
9224
9232
9240
9248
9256
9264
9272
9280
9288
9296
9304
9312
9320
9328
9336
9344
9352
9360
9368
9376
9384
9392
9400
9408
9416
9424
9432
9440
9448
9456
9464
9472
9480
9488
9496
9504
9512
9520
9528
9536
9544
9552
9560
9568
9576
9584
9592
9600
9608
9616
9624
9632
9640
9648
9656
9664
9672
9680
9688
9696
9704
9712
9720
9728
9736
9744
9752
9760
9768
9776
9784
9792
9800
9808
9816
9824
9832
9840
9848
9856
9864
9872
9880
9888
9896
9904
9912
9920
9928
9936
9944
9952
9960
9968
9976
9984
9992
10000
10008
10016
10024
10032
10040
10048
10056
10064
10072
10080
10088
10096
10104
10112
10120
10128
10136
10144
10152
10160
10168
10176
10184
10192
10200
10208
10216
10224
10232
10240
10248
10256
10264
10272
10280
10288
10296
10304
10312
10320
10328
10336
10344
10352
10360
10368
10376
10384
10392
10400
10408
10416
10424
10432
10440
10448
10456
10464
10472
10480
10488
10496
10504
10512
10520
10528
10536
10544
10552
10560
10568
10576
10584
10592
10600
10608
10616
10624
10632
10640
10648
10656
10664
10672
10680
10688
10696
10704
10712
10720
10728
10736
10744
10752
10760
10768
10776
10784
10792
10800
10808
10816
10824
10832
10840
10848
10856
10864
10872
10880
10888
10896
10904
10912
10920
10928
10936
10944
10952
10960
10968
10976
10984
10992
11000
11008
11016
11024
11032
11040
11048
11056
11064
11072
11080
11088
11096
11104
11112
11120
11128
11136
11144
11152
11160
11168
11176
11184
11192
11200
11208
11216
11224
11232
11240
11248
11256
11264
11272
11280
11288
11296
11304
11312
11320
11328
11336
11344
11352
11360
11368
11376
11384
11392
11400
11408
11416
11424
11432
11440
11448
11456
11464
11472
11480
11488
11496
11504
11512
11520
11528
11536
11544
11552
11560
11568
11576
11584
11592
11600
11608
11616
11624
11632
11640
11648
11656
11664
11672
11680
11688
11696
11704
11712
11720
11728
11736
11744
11752
11760
11768
11776
11784
11792
11800
11808
11816
11824
11832
11840
11848
11856
11864
11872
11880
11888
11896
11904
11912
11920
11928
11936
11944
11952
11960
11968
11976
11984
11992
12000
12008
12016
12024
12032
12040
12048
12056
12064
12072
12080
12088
12096
12104
12112
12120
12128
12136
12144
12152
12160
12168
12176
12184
12192
12200
12208
12216
12224
12232
12240
12248
12256
12264
12272
12280
12288
12296
12304
12312
12320
12328
12336
12344
12352
12360
12368
12376
12384
12392
12400
12408
12416
12424
12432
12440
12448
12456
12464
12472
12480
12488
12496
12504
12512
12520
12528
12536
12544
12552
12560
12568
12576
12584
12592
12600
12608
12616
12624
12632
12640
12648
12656
12664
12672
12680
12688
12696
12704
12712
12720
12728
12736
12744
12752
12760
12768
127768
16
24
32
40
48
56
64
72
80
88
96
104
112
120
128
136
144
152
160
168
176
184
192
200
208
216
224
232
240
248
256
264
272
280
288
296
304
312
320
328
336
344
352
360
368
376
384
392
400
408
416
424
432
440
448
456
464
472
480
488
496
504
512
520
528
536
544
552
560
568
576
584
592
600
608
616
624
632
640
648
656
664
672
680
688
696
704
712
720
728
736
744
752
760
768
776
784
792
800
808
816
824
832
840
848
856
864
872
880
888
896
904
912
920
928
936
944
952
960
968
976
984
992
1000
1008
1016
1024
1032
1040
1048
1056
1064
1072
1080
1088
1096
1104
1112
1120
1128
1136
1144
1152
1160
1168
1176
1184
1192
1200
1208
1216
1224
1232
1240
1248
1256
1264
1272
1280
1288
1296
1304
1312
1320
1328
1336
1344
1352
1360
1368
1376
1384
1392
1400
1408
1416
1424
1432
1440
1448
1456
1464
1472
1480
1488
1496
1504
1512
1520
1528
1536
1544
1552
1560
1568
1576
1584
1592
1600
1608
1616
1624
1632
1640
1648
1656
1664
1672
1680
1688
1696
1704
1712
1720
1728
1736
1744
1752
1760
1768
1776
1784
1792
1800
1808
1816
1824
1832
1840
1848
1856
1864
1872
1880
1888
1896
1904
1912
1920
1928
1936
1944
1952
1960
1968
1976
1984
1992
2000
2008
2016
2024
2032
2040
2048
2056
2064
2072
2080
2088
2096
2104
2112
2120
2128
2136
2144
2152
2160
2168
2176
2184
2192
2200
2208
2216
2224
2232
2240
2248
2256
2264
2272
2280
2288
2296
2304
2312
2320
2328
2336
2344
2352
2360
2368
2376
2384
2392
2400
2408
2416
2424
2432
2440
2448
2456
2464
2472
2480
2488
2496
2504
2512
2520
2528
2536
2544
2552
2560
2568
2576
2584
2592
2600
2608
2616
2624
2632
2640
2648
2656
2664
2672
2680
2688
2696
2704
2712
2720
2728
2736
2744
2752
2760
2768
2776
2784
2792
2800
2808
2816
2824
2832
2840
2848
2856
2864
2872
2880
2888
2896
2904
2912
2920
2928
2936
2944
2952
2960
2968
2976
2984
2992
3000
3008
3016
3024
3032
3040
3048
3056
3064
3072
3080
3088
3096
3104
3112
3120
3128
3136
3144
3152
3160
3168
3176
3184
3192
3200
3208
3216
3224
3232
3240
3248
3256
3264
3272
3280
3288
3296
3304
3312
3320
3328
3336
3344
3352
3360
3368
3376
3384
3392
3400
3408
3416
3424
3432
3440
3448
3456
3464
3472
3480
3488
3496
3504
3512
3520
3528
3536
3544
3552
3560
3568
3576
3584
3592
3600
3608
3616
3624
3632
3640
3648
3656
3664
3672
3680
3688
3696
3704
3712
3720
3728
3736
3744
3752
3760
3768
3776
3784
3792
3800
3808
3816
3824
3832
3840
3848
3856
3864
3872
3880
3888
3896
3904
3912
3920
3928
3936
3944
3952
3960
3968
3976
3984
3992
4000
4008
4016
4024
4032
4040
4048
4056
4064
4072
4080
4088
4096
4104
4112
4120
4128
4136
4144
4152
4160
4168
4176
4184
4192
4200
4208
4216
4224
4232
4240
4248
4256
4264
4272
4280
4288
4296
4304
4312
4320
4328
4336
4344
4352
4360
4368
4376
4384
4392
4400
4408
4416
4424
4432
4440
4448
4456
4464
4472
4480
4488
4496
4504
4512
4520
4528
4536
4544
4552
4560
4568
4576
4584
4592
4600
4608
4616
4624
4632
4640
4648
4656
4664
4672
4680
4688
4696
4704
4712
4720
4728
4736
4744
4752
4760
4768
4776
4784
4792
4800
4808
4816
4824
4832
4840
4848
4856
4864
4872
4880
4888
4896
4904
4912
4920
4928
4936
4944
4952
4960
4968
4976
4984
4992
5000
5008
5016
5024
5032
5040
5048
5056
5064
5072
5080
5088
5096
5104
5112
5120
5128
5136
5144
5152
5160
5168
5176
5184
5192
5200
5208
5216
5224
5232
5240
5248
5256
5264
5272
5280
5288
5296
5304
5312
5320
5328
5336
5344
5352
5360
5368
5376
5384
5392
5400
5408
5416
5424
5432
5440
5448
5456
5464
5472
5480
5488
5496
5504
5512
5520
5528
5536
5544
5552
5560
5568
5576
5584
5592
5600
5608
5616
5624
5632
5640
5648
5656
5664
5672
5680
5688
5696
5704
5712
5720
5728
5736
5744
5752
5760
5768
5776
5784
5792
5800
5808
5816
5824
5832
5840
5848
5856
5864
5872
5880
5888
5896
5904
5912
5920
5928
5936
5944
5952
5960
5968
5976
5984
5992
6000
6008
6016
6024
6032
6040
6048
6056
6064
6072
6080
6088
6096
6104
6112
6120
6128
6136
6144
6152
6160
6168
6176
6184
6192
6200
6208
6216
6224
6232
6240
6248
6256
6264
6272
6280
6288
6296
6304
6312
6320
6328
6336
6344
6352
6360
6368
6376
6384
6392
6400
6408
6416
6424
6432
6440
6448
6456
6464
6472
6480
6488
6496
6504
6512
6520
6528
6536
6544
6552
6560
6568
6576
6584
6592
6600
6608
6616
6624
6632
6640
6648
6656
6664
6672
6680
6688
6696
6704
6712
6720
6728
6736
6744
6752
6760
6768
6776
6784
6792
6800
6808
6816
6824
6832
6840
6848
6856
6864
6872
6880
6888
6896
6904
6912
6920
6928
6936
6944
6952
6960
6968
6976
6984
6992
7000
7008
7016
7024
7032
7040
7048
7056
7064
7072
7080
7088
7096
7104
7112
7120
7128
7136
7144
7152
7160
7168
7176
7184
7192
7200
7208
7216
7224
7232
7240
7248
7256
7264
7272
7280
7288
7296
7304
7312
7320
7328
7336
7344
7352
7360
7368
7376
7384
7392
7400
7408
7416
7424
7432
7440
7448
7456
7464
7472
7480
7488
7496
7504
7512
7520
7528
7536
7544
7552
7560
7568
7576
7584
7592
7600
7608
7616
7624
7632
7640
7648
7656
7664
7672
7680
7688
7696
7704
7712
7720
7728
7736
7744
7752
7760
7768
7776
7784
7792
7800
7808
7816
7824
7832
7840
7848
7856
7864
7872
7880
7888
7896
7904
7912
7920
7928
7936
7944
7952
7960
7968
7976
7984
7992
8000
8008
8016
8024
8032
8040
8048
8056
8064
8072
8080
8088
8096
8104
8112
8120
8128
8136
8144
8152
8160
8168
8176
8184
8192
8200
8208
8216
8224
8232
8240
8248
8256
8264
8272
8280
8288
8296
8304
8312
8320
8328
8336
8344
8352
8360
8368
8376
8384
8392
8400
8408
8416
8424
8432
8440
8448
8456
8464
8472
8480
8488
8496
8504
8512
8520
8528
8536
8544
8552
8560
8568
8576
8584
8592
8600
8608
8616
8624
8632
8640
8648
8656
8664
8672
8680
8688
8696
8704
8712
8720
8728
8736
8744
8752
8760
8768
8776
8784
8792
8800
8808
8816
8824
8832
8840
8848
8856
8864
8872
8880
8888
8896
8904
8912
8920
8928
8936
8944
8952
8960
8968
8976
8984
8992
9000
9008
9016
9024
9032
9040
9048
9056
9064
9072
9080
9088
9096
9104
9112
9120
9128
9136
9144
9152
9160
9168
9176
9184
9192
9200
9208
9216
9224
9232
9240
9248
9256
9264
9272
9280
9288
9296
9304
9312
9320
9328
9336
9344
9352
9360
9368
9376
9384
9392
9400
9408
9416
9424
9432
9440
9448
9456
9464
9472
9480
9488
9496
9504
9512
9520
9528
9536
9544
9552
9560
9568
9576
9584
9592
9600
9608
9616
9624
9632
9640
9648
9656
9664
9672
9680
9688
9696
9704
9712
9720
9728
9736
9744
9752
9760
9768
9776
9784
9792
9800
9808
9816
9824
9832
9840
9848
9856
9864
9872
9880
9888
9896
9904
9912
9920
9928
9936
9944
9952
9960
9968
9976
9984
9992
10000
10008
10016
10024
10032
10040
10048
10056
10064
10072
10080
10088
10096
10104
10112
10120
10128
10136
10144
10152
10160
10168
10176
10184
10192
10200
10208
10216
10224
10232
10240
10248
10256
10264
10272
10280
10288
10296
10304
10312
10320
10328
10336
10344
10352
10360
10368
10376
10384
10392
10400
10408
10416
10424
10432
10440
10448
10456
10464
10472
10480
10488
10496
10504
10512
10520
10528
10536
10544
10552
10560
10568
10576
10584
10592
10600
10608
10616
10624
10632
10640
10648
10656
10664
10672
10680
10688
10696
10704
10712
10720
10728
10736
10744
10752
10760
10768
10776
10784
10792
10800
10808
10816
10824
10832
10840
10848
10856
10864
10872
10880
10888
10896
10904
10912
10920
10928
10936
10944
10952
10960
10968
10976
10984
10992
11000
11008
11016
11024
11032
11040
11048
11056
11064
11072
11080
11088
11096
11104
11112
11120
11128
11136
11144
11152
11160
11168
11176
11184
11192
11200
11208
11216
11224
11232
11240
11248
11256
11264
11272
11280
11288
11296
11304
11312
11320
11328
11336
11344
11352
11360
11368
11376
11384
11392
11400
11408
11416
11424
11432
11440
11448
11456
11464
11472
11480
11488
11496
11504
11512
11520
11528
11536
11544
11552
11560
11568
11576
11584
11592
11600
11608
11616
11624
11632
11640
11648
11656
11664
11672
11680
11688
11696
11704
11712
11720
11728
11736
11744
11752
11760
11768
11776
11784
11792
11800
11808
11816
11824
11832
11840
11848
11856
11864
11872
11880
11888
11896
11904
11912
11920
11928
11936
11944
11952
11960
11968
11976
11984
11992
12000
12008
12016
12024
12032
12040
12048
12056
12064
12072
12080
12088
12096
12104
12112
12120
12128
12136
12144
12152
12160
12168
12176
12184
12192
12200
12208
12216
12224
12232
12240
12248
12256
12264
12272
12280
12288
12296
12304
12312
12320
12328
12336
12344
12352
12360
12368
12376
12384
12392
12400
12408
12416
12424
12432
12440
12448
12456
12464
12472
12480
12488
12496
12504
12512
12520
12528
12536
12544
12552
12560
12568
12576
12584
12592
12600
12608
12616
12624
12632
12640
12648
12656
12664
12672
12680
12688
12696
12704
12712
12720
12728
12736
12744
12752
12760
12768
12776
12784
12792
12800
12808
12816
12824
12832
12840
12848
12856
12864
12872
12880
12888
12896
12904
12912
12920
12928
12936
12944
12952
12960
12968
12976
12984
12992
13000
13008
13016
13024
13032
13040
13048
13056
13064
13072
13080
13088
13096
13104
13112
13120
13128
13136
13144
13152
13160
13168
13176
13184
13192
13200
13208
13216
13224
13232
13240
13248
13256
13264
13272
13280
13288
13296
13304
13312
13320
13328
13336
13344
13352
13360
13368
13376
13384
13392
13400
13408
13416
13424
13432
13440
13448
13456
13464
13472
13480
13488
13496
13504
13512
13520
13528
13536
13544
13552
13560
13568
13576
13584
13592
13600
13608
13616
13624
13632
13640
13648
13656
13664
13672
13680
13688
13696
13704
13712
13720
13728
13736
13744
13752
13760
13768
13776
13784
13792
13800
13808
13816
13824
13832
13840
13848
13856
13864
13872
13880
13888
13896
13904
13912
13920
13928
13936
13944
13952
13960
13968
13976
13984
13992
14000
14008
14016
14024
14032
14040
14048
14056
14064
14072
14080
14088
14096
14104
14112
14120
14128
14136
14144
14152
14160
14168
14176
14184
14192
14200
14208
14216
14224
14232
14240
14248
14256
14264
14272
14280
14288
14296
14304
14312
14320
14328
14336
14344
14352
14360
14368
14376
14384
14392
14400
14408
14416
14424
14432
14440
14448
14456
14464
14472
14480
14488
14496
14504
14512
14520
14528
14536
14544
14552
14560
14568
14576
14584
14592
14600
14608
14616
14624
14632
14640
14648
14656
14664
14672
14680
14688
14696
14704
14712
14720
14728
14736
14744
14752
14760
14768
14776
14784
14792
14800
14808
14816
14824
14832
14840
14848
14856
14864
14872
14880
14888
14896
14904
14912
14920
14928
14936
14944
14952
14960
14968
14976
14984
14992
15000
15008
15016
15024
15032
15040
15048
15056
15064
15072
15080
15088
15096
15104
15112
15120
15128
15136
15144
15152
15160
15168
15176
15184
15192
15200
15208
15216
15224
15232
15240
15248
15256
15264
15272
15280
15288
15296
15304
15312
15320
15328
15336
15344
15352
15360
15368
15376
15384
15392
15400
15408
15416
15424
15432
15440
15448
15456
15464
15472
15480
15488
15496
15504
15512
15520
15528
15536
15544
15552
15560
15568
15576
15584
15592
15600
15608
15616
15624
15632
15640
15648
15656
15664
15672
15680
15688
15696
15704
15712
15720
15728
15736
15744
15752
15760
15768
15776
15784
15792
15800
15808
15816
15824
15832
15840
15848
15856
15864
15872
15880
15888
15896
15904
15912
15920
15928
15936
15944
15952
15960
15968
15976
15984
15992
16000
16008
16016
16024
16032
16040
16048
16056
16064
16072
16080
16088
16096
16104
16112
16120
16128
16136
16144
16152
16160
16168
16176
16184
16192
16200
16208
16216
16224
16232
16240
16248
16256
16264
16272
16280
16288
16296
16304
16312
16320
16328
16336
16344
16352
16360
16368
16376
16384
16392
16400
16408
16416
16424
16432
16440
16448
16456
16464
16472
16480
16488
16496
16504
16512
16520
16528
16536
16544
16552
16560
16568
16576
16584
16592
16600
16608
16616
16624
16632
16640
16648
16656
16664
16672
16680
16688
16696
16704
16712
16720
16728
16736
16744
16752
16760
16768
16776
16784
16792
16800
16808
16816
16824
16832
16840
16848
16856
16864
16872
16880
16888
16896
16904
16912
16920
16928
16936
16944
16952
16960
16968
16976
16984
16992
17000
17008
17016
17024
17032
17040
17048
17056
17064
17072
17080
17088
17096
17104
17112
17120
17128
17136
17144
17152
17160
17168
17176
17184
17192
17200
17208
17216
17224
17232
17240
17248
17256
17264
17272
17280
17288
17296
17304
17312
17320
17328
17336
17344
17352
17360
17368
17376
17384
17392
17400
17408
17416
17424
17432
17440
17448
17456
17464
17472
17480
17488
17496
17504
17512
17520
17528
17536
17544
17552
17560
17568
17576
17584
17592
17600
17608
17616
17624
17632
17640
17648
17656
17664
17672
17680
17688
17696
17704
17712
17720
17728
17736
17744
17752
17760
17768
17776
17784
17792
17800
17808
17816
17824
17832
17840
17848
17856
17864
17872
17880
17888
17896
17904
17912
17920
17928
17936
17944
17952
17960
17968
17976
17984
17992
18000
18008
18016
18024
18032
18040
18048
18056
18064
18072
18080
18088
18096
18104
18112
18120
18128
18136
18144
18152
18160
18168
18176
18184
18192
18200
18208
18216
18224
18232
18240
18248
18256
18264
18272
18280
18288
18296
18304
18312
18320
18328
18336
18344
18352
18360
18368
18376
18384
18392
18400
18408
18416
18424
18432
18440
18448
18456
18464
18472
18480
18488
18496
18504
18512
18520
18528
18536
18544
18552
18560
18568
18576
18584
18592
18600
18608
18616
18624
18632
18640
18648
18656
18664
18672
18680
18688
18696
18704
18712
18720
18728
18736
18744
18752
18760
18768
18776
18784
18792
18800
18808
18816
18824
18832
18840
18848
18856
18864
18872
18880
18888
18896
18904
18912
18920
18928
18936
18944
18952
18960
18968
18976
18984
18992
19000
19008
19016
19024
19032
19040
19048
19056
19064
19072
19080
19088
19096
19104
19112
19120
19128
19136
19144
19152
19160
19168
19176
19184
19192
19200
19208
19216
19224
19232
19240
19248
19256
19264
19272
19280
19288
19296
19304
19312
19320
19328
19336
19344
19352
19360
19368
19376
19384
19392
19400
19408
19416
19424
19432
19440
19448
19456
19464
19472
19480
19488
19496
19504
19512
19520
19528
19536
19544
19552
19560
19568
19576
19584
19592
19600
19608
19616
19624
19632
19640
19648
19656
19664
19672
19680
19688
19696
19704
19712
19720
19728
19732

12784
12792
12800
12808
12816
12824
12832
12840
12848
12856
12864
12872
12880
12888
12896
12904
12912
12920
12928
12936
12944
12952
12960
12968
12976
12984
12992
13000
13008
13016
13024
13032
13040
13048
13056
13064
13072
13080
13088
13096
13104
13112
13120
13128
13136
13144
13152
13160
13168
13176
13184
13192
13200
13208
13216
13224
13232
13240
13248
13256
13264
13272
13280
13288
13296
13304
13312
13320
13328
13336
13344
13352
13360
13368
13376
13384
13392
13400
13408
13416
13424
13432
13440
13448
13456
13464
13472
13480
13488
13496
13504
13512
13520
13528
13536
13544
13552
13560
13568
13576
13584
13592
13600
13608
13616
13624
13632
13640
13648
13656
13664
13672
13680
13688
13696
13704
13712
13720
13728
13736
13744
13752
13760
13768
13776
13784
13792
13800
13808
13816
13824
13832
13840
13848
13856
13864
13872
13880
13888
13896
13904
13912
13920
13928
13936
13944
13952
13960
13968
13976
13984
13992
14000
14008
14016
14024
14032
14040
14048
14056
14064
14072
14080
14088
14096
14104
14112
14120
14128
14136
14144
14152
14160
14168
14176
14184
14192
14200
14208
14216
14224
14232
14240
14248
14256
14264
14272
14280
14288
14296
14304
14312
14320
14328
14336
14344
14352
14360
14368
14376
14384
14392
14400
14408
14416
14424
14432
14440
14448
14456
14464
14472
14480
14488
14496
14504
14512
14520
14528
14536
14544
14552
14560
14568
14576
14584
14592
14600
14608
14616
14624
14632
14640
14648
14656
14664
14672
14680
14688
14696
14704
14712
14720
14728
14736
14744
14752
14760
14768
14776
14784
14792
14800
14808
14816
14824
14832
14840
14848
14856
14864
14872
14880
14888
14896
14904
14912
14920
14928
14936
14944
14952
14960
14968
14976
14984
14992
15000
15008
15016
15024
15032
15040
15048
15056
15064
15072
15080
15088
15096
15104
15112
15120
15128
15136
15144
15152
15160
15168
15176
15184
15192
15200
15208
15216
15224
15232
15240
15248
15256
15264
15272
15280
15288
15296
15304
15312
15320
15328
15336
15344
15352
15360
15368
15376
15384
15392
15400
15408
15416
15424
15432
15440
15448
15456
15464
15472
15480
15488
15496
15504
15512
15520
15528
15536
15544
15552
15560
15568
15576
15584
15592
15600
15608
15616
15624
15632
15640
15648
15656
15664
15672
15680
15688
15696
15704
15712
15720
15728
15736
15744
15752
15760
15768
15776
15784
15792
15800
15808
15816
15824
15832
15840
15848
15856
15864
15872
15880
15888
15896
15904
15912
15920
15928
15936
15944
15952
15960
15968
15976
15984
15992
16000
16008
16016
16024
16032
16040
16048
16056
16064
16072
16080
16088
16096
16104
16112
16120
16128
16136
16144
16152
16160
16168
16176
16184
16192
16200
16208
16216
16224
16232
16240
16248
16256
16264
16272
16280
16288
16296
16304
16312
16320
16328
16336
16344
16352
16360
16368
16376
16384
16392
16400
16408
16416
16424
16432
16440
16448
16456
16464
16472
16480
16488
16496
16504
16512
16520
16528
16536
16544
16552
16560
16568
16576
16584
16592
16600
16608
16616
16624
16632
16640
16648
16656
16664
16672
16680
16688
16696
16704
16712
16720
16728
16736
16744
16752
16760
16768
16776
16784
16792
16800
16808
16816
16824
16832
16840
16848
16856
16864
16872
16880
16888
16896
16904
16912
16920
16928
16936
16944
16952
16960
16968
16976
16984
16992
17000
17008
17016
17024
17032
17040
17048
17056
17064
17072
17080
17088
17096
17104
17112
17120
17128
17136
17144
17152
17160
17168
17176
17184
17192
17200
17208
17216
17224
17232
17240
17248
17256
17264
17272
17280
17288
17296
17304
17312
17320
17328
17336
17344
17352
17360
17368
17376
17384
17392
17400
17408
17416
17424
17432
17440
17448
17456
17464
17472
17480
17488
17496
17504
17512
17520
17528
17536
17544
17552
17560
17568
17576
17584
17592
17600
17608
17616
17624
17632
17640
17648
17656
17664
17672
17680
17688
17696
17704
17712
17720
17728
17736
17744
17752
17760
17768
17776
17784
17792
17800
17808
17816
17824
17832
17840
17848
17856
17864
17872
17880
17888
17896
17904
17912
17920
17928
17936
17944
17952
17960
17968
17976
17984
17992
18000
18008
18016
18024
18032
18040
18048
18056
18064
18072
18080
18088
18096
18104
18112
18120
18128
18136
18144
18152
18160
18168
18176
18184
18192
18200
18208
18216
18224
18232
18240
18248
18256
18264
18272
18280
18288
18296
18304
18312
18320
18328
18336
18344
18352
18360
18368
18376
18384
18392
18400
18408
18416
18424
18432
18440
18448
18456
18464
18472
18480
18488
18496
18504
18512
18520
18528
18536
18544
18552
18560
18568
18576
18584
18592
18600
18608
18616
18624
18632
18640
18648
18656
18664
18672
18680
18688
18696
18704
18712
18720
18728
18736
18744
18752
18760
18768
18776
18784
18792
18800
18808
18816
18824
18832
18840
18848
18856
18864
18872
18880
18888
18896
18904
18912
18920
18928
18936
18944
18952
18960
18968
18976
18984
18992
19000
19008
19016
19024
19032
19040
19048
19056
19064
19072
19080
19088
19096
19104
19112
19120
19128
19136
19144
19152
19160
19168
19176
19184
19192
19200
19208
19216
19224
19232
19240
19248
19256
19264
19272
19280
19288
19296
19304
19312
19320
19328
19336
19344
19352
19360
19368
19376
19384
19392
19400
19408
19416
19424
19432
19440
19448
19456
19464
19472
19480
19488
19496
19504
19512
19520
19528
19536
19544
19552
19560
19568
19576
19584
19592
19600
19608
19616
19624
19632
19640
19648
19656
19664
19672
19680
19688
19696
19704
19712
19720
19728
19732
torch.Size([3368, 512])
Rank@1:0.927553 Rank@5:0.976841 Rank@10:0.985748 mAP:0.812101
8
16
24
32
40
48
56
64
72
80
88
96
104
112
120
128
136
144
152
160
168
176
184
192
200
208
216
224
232
240
248
256
264
272
280
288
296
304
312
320
328
336
344
352
360
368
376
384
392
400
408
416
424
432
440
448
456
464
472
480
488
496
504
512
520
528
536
544
552
560
568
576
584
592
600
608
616
624
632
640
648
656
664
672
680
688
696
704
712
720
728
736
744
752
760
768
776
784
792
800
808
816
824
832
840
848
856
864
872
880
888
896
904
912
920
928
936
944
952
960
968
976
984
992
1000
1008
1016
1024
1032
1040
1048
1056
1064
1072
1080
1088
1096
1104
1112
1120
1128
1136
1144
1152
1160
1168
1176
1184
1192
1200
1208
1216
1224
1232
1240
1248
1256
1264
1272
1280
1288
1296
1304
1312
1320
1328
1336
1344
1352
1360
1368
1376
1384
1392
1400
1408
1416
1424
1432
1440
1448
1456
1464
1472
1480
1488
1496
1504
1512
1520
1528
1536
1544
1552
1560
1568
1576
1584
1592
1600
1608
1616
1624
1632
1640
1648
1656
1664
1672
1680
1688
1696
1704
1712
1720
1728
1736
1744
1752
1760
1768
1776
1784
1792
1800
1808
1816
1824
1832
1840
1848
1856
1864
1872
1880
1888
1896
1904
1912
1920
1928
1936
1944
1952
1960
1968
1976
1984
1992
2000
2008
2016
2024
2032
2040
2048
2056
2064
2072
2080
2088
2096
2104
2112
2120
2128
2136
2144
2152
2160
2168
2176
2184
2192
2200
2208
2216
2224
2232
2240
2248
2256
2264
2272
2280
2288
2296
2304
2312
2320
2328
2336
2344
2352
2360
2368
2376
2384
2392
2400
2408
2416
2424
2432
2440
2448
2456
2464
2472
2480
2488
2496
2504
2512
2520
2528
2536
2544
2552
2560
2568
2576
2584
2592
2600
2608
2616
2624
2632
2640
2648
2656
2664
2672
2680
2688
2696
2704
2712
2720
2728
2736
2744
2752
2760
2768
2776
2784
2792
2800
2808
2816
2824
2832
2840
2848
2856
2864
2872
2880
2888
2896
2904
2912
2920
2928
2936
2944
2952
2960
2968
2976
2984
2992
3000
3008
3016
3024
3032
3040
3048
3056
3064
3072
3080
3088
3096
3104
3112
3120
3128
3136
3144
3152
3160
3168
3176
3184
3192
3200
3208
3216
3224
3232
3240
3248
3256
3264
3272
3280
3288
3296
3304
3312
3320
3328
3336
3344
3352
3360
3368
resnest50_leaky_sin_2
torch.Size([3368, 512])
Rank@1:0.927553 Rank@5:0.976841 Rank@10:0.985748 mAP:0.812101
8
16
24
32
40
48
56
64
72
80
88
96
104
112
120
128
136
144
152
160
168
176
184
192
200
208
216
224
232
240
248
256
264
272
280
288
296
304
312
320
328
336
344
352
360
368
376
384
392
400
408
416
424
432
440
448
456
464
472
480
488
496
504
512
520
528
536
544
552
560
568
576
584
592
600
608
616
624
632
640
648
656
664
672
680
688
696
704
712
720
728
736
744
752
760
768
776
784
792
800
808
816
824
832
840
848
856
864
872
880
888
896
904
912
920
928
936
944
952
960
968
976
984
992
1000
1008
1016
1024
1032
1040
1048
1056
1064
1072
1080
1088
1096
1104
1112
1120
1128
1136
1144
1152
1160
1168
1176
1184
1192
1200
1208
1216
1224
1232
1240
1248
1256
1264
1272
1280
1288
1296
1304
1312
1320
1328
1336
1344
1352
1360
1368
1376
1384
1392
1400
1408
1416
1424
1432
1440
1448
1456
1464
1472
1480
1488
1496
1504
1512
1520
1528
1536
1544
1552
1560
1568
1576
1584
1592
1600
1608
1616
1624
1632
1640
1648
1656
1664
1672
1680
1688
1696
1704
1712
1720
1728
1736
1744
1752
1760
1768
1776
1784
1792
1800
1808
1816
1824
1832
1840
1848
1856
1864
1872
1880
1888
1896
1904
1912
1920
1928
1936
1944
1952
1960
1968
1976
1984
1992
2000
2008
2016
2024
2032
2040
2048
2056
2064
2072
2080
2088
2096
2104
2112
2120
2128
2136
2144
2152
2160
2168
2176
2184
2192
2200
2208
2216
2224
2232
2240
2248
2256
2264
2272
2280
2288
2296
2304
2312
2320
2328
2336
2344
2352
2360
2368
2376
2384
2392
2400
2408
2416
2424
2432
2440
2448
2456
2464
2472
2480
2488
2496
2504
2512
2520
2528
2536
2544
2552
2560
2568
2576
2584
2592
2600
2608
2616
2624
2632
2640
2648
2656
2664
2672
2680
2688
2696
2704
2712
2720
2728
2736
2744
2752
2760
2768
2776
2784
2792
2800
2808
2816
2824
2832
2840
2848
2856
2864
2872
2880
2888
2896
2904
2912
2920
2928
2936
2944
2952
2960
2968
2976
2984
2992
3000
3008
3016
3024
3032
3040
3048
3056
3064
3072
3080
3088
3096
3104
3112
3120
3128
3136
3144
3152
3160
3168
3176
3184
3192
3200
3208
3216
3224
3232
3240
3248
3256
3264
3272
3280
3288
3296
3304
3312
3320
3328
3336
3344
3352
3360
3368
resnest50_leaky_sin_2
Traceback (most recent call last):
  File "train.py", line 19, in <module>
    from model import ft_net, ft_net_dense, ft_net_NAS, PCB, ft_resnest50, ft_resnest269
  File "/home/luguangjian/tmp2/reid/model.py", line 7, in <module>
    from resnest import resnest
  File "/home/luguangjian/tmp2/reid/resnest/__init__.py", line 1, in <module>
    from .resnest import *
  File "/home/luguangjian/tmp2/reid/resnest/resnest.py", line 11, in <module>
    from .resnet import ResNet, Bottleneck
  File "/home/luguangjian/tmp2/reid/resnest/resnet.py", line 15, in <module>
    from .sum_relu import Sum_ReLU
ModuleNotFoundError: No module named 'resnest.sum_relu'
This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7f942275b450>]
2.4052140712738037
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=751, bias=True)
    )
  )
)
Epoch 0/79
----------
/home/luguangjian/.conda/envs/test_code/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Traceback (most recent call last):
  File "train.py", line 425, in <module>
    model = train_model(model, optimizer_ft, exp_lr_scheduler, criterion, num_epochs=80, local_rank=opt.local_rank)
  File "train.py", line 233, in train_model
    outputs = model(inputs)
  File "/home/luguangjian/.conda/envs/test_code/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/luguangjian/tmp2/reid/model.py", line 125, in forward
    x = self.model.layer1(x)
  File "/home/luguangjian/.conda/envs/test_code/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/luguangjian/.conda/envs/test_code/lib/python3.7/site-packages/torch/nn/modules/container.py", line 100, in forward
    input = module(input)
  File "/home/luguangjian/.conda/envs/test_code/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/luguangjian/tmp2/reid/resnest/resnet.py", line 119, in forward
    out = self.Sum_ReLU_2(out)
  File "/home/luguangjian/.conda/envs/test_code/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/luguangjian/tmp2/reid/resnest/sum_relu_2.py", line 15, in forward
    return torch.where(x > 0, x+abs(x)/torch.sum(abs(x)) * x, 0)
TypeError: where(): argument 'other' (position 3) must be Tensor, not int
This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7fa073eba390>]
2.455618381500244
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=751, bias=True)
    )
  )
)
Epoch 0/79
----------
/home/luguangjian/.conda/envs/test_code/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 0.9290 Acc: 0.1117
val Loss: 3.6281 Acc: 0.2623
Training complete in 3m 37s

Epoch 1/79
----------
train Loss: 1.0216 Acc: 0.3936
val Loss: 1.9576 Acc: 0.5300
Training complete in 7m 14s

Epoch 2/79
----------
train Loss: 1.0437 Acc: 0.5522
val Loss: 1.4627 Acc: 0.6152
Training complete in 10m 41s

Epoch 3/79
----------
train Loss: 1.1319 Acc: 0.6189
val Loss: 1.2636 Acc: 0.6644
Training complete in 14m 51s

Epoch 4/79
----------
train Loss: 1.2233 Acc: 0.6656
val Loss: 1.0811 Acc: 0.7217
Training complete in 19m 24s

Epoch 5/79
----------
train Loss: 1.0771 Acc: 0.7251
val Loss: 0.6648 Acc: 0.8043
Training complete in 23m 59s

Epoch 6/79
----------
train Loss: 0.8753 Acc: 0.7747
val Loss: 0.5986 Acc: 0.8309
Training complete in 28m 32s

Epoch 7/79
----------
train Loss: 0.7124 Acc: 0.8167
val Loss: 0.6155 Acc: 0.8336
Training complete in 33m 10s

Epoch 8/79
----------
train Loss: 0.6509 Acc: 0.8338
val Loss: 0.4969 Acc: 0.8562
Training complete in 37m 47s

Epoch 9/79
----------
train Loss: 0.6020 Acc: 0.8460
val Loss: 0.3822 Acc: 0.8762
Training complete in 42m 22s

Epoch 10/79
----------
train Loss: 0.5376 Acc: 0.8606
val Loss: 0.4059 Acc: 0.8908
Training complete in 46m 56s

Epoch 11/79
----------
train Loss: 0.5150 Acc: 0.8670
val Loss: 0.3620 Acc: 0.8842
Training complete in 51m 30s

Epoch 12/79
----------
train Loss: 0.4666 Acc: 0.8815
val Loss: 0.3194 Acc: 0.9028
Training complete in 56m 3s

Epoch 13/79
----------
train Loss: 0.4401 Acc: 0.8834
val Loss: 0.3826 Acc: 0.8975
Training complete in 60m 36s

Epoch 14/79
----------
train Loss: 0.3800 Acc: 0.8968
val Loss: 0.3764 Acc: 0.8948
Training complete in 65m 9s

Epoch 15/79
----------
train Loss: 0.4107 Acc: 0.8917
val Loss: 0.3628 Acc: 0.8842
Training complete in 69m 46s

Epoch 16/79
----------
train Loss: 0.4001 Acc: 0.8960
val Loss: 0.3903 Acc: 0.8868
Training complete in 74m 19s

Epoch 17/79
----------
train Loss: 0.3710 Acc: 0.9055
val Loss: 0.3851 Acc: 0.8961
Training complete in 78m 59s

Epoch 18/79
----------
train Loss: 0.3555 Acc: 0.9062
val Loss: 0.3087 Acc: 0.9068
Training complete in 83m 31s

Epoch 19/79
----------
train Loss: 0.3628 Acc: 0.9032
val Loss: 0.2932 Acc: 0.9174
Training complete in 88m 5s

Epoch 20/79
----------
train Loss: 0.3400 Acc: 0.9096
val Loss: 0.3605 Acc: 0.9015
Training complete in 92m 39s

Epoch 21/79
----------
train Loss: 0.3394 Acc: 0.9114
val Loss: 0.2635 Acc: 0.9174
Training complete in 97m 11s

Epoch 22/79
----------
train Loss: 0.3200 Acc: 0.9161
val Loss: 0.2890 Acc: 0.9228
Training complete in 101m 45s

Epoch 23/79
----------
train Loss: 0.3199 Acc: 0.9180
val Loss: 0.3218 Acc: 0.9134
Training complete in 106m 18s

Epoch 24/79
----------
train Loss: 0.3341 Acc: 0.9142
val Loss: 0.3341 Acc: 0.9081
Training complete in 110m 51s

Epoch 25/79
----------
train Loss: 0.3023 Acc: 0.9232
val Loss: 0.2761 Acc: 0.9108
Training complete in 115m 25s

Epoch 26/79
----------
train Loss: 0.3003 Acc: 0.9233
val Loss: 0.2516 Acc: 0.9268
Training complete in 119m 58s

Epoch 27/79
----------
train Loss: 0.2992 Acc: 0.9233
val Loss: 0.2640 Acc: 0.9214
Training complete in 124m 31s

Epoch 28/79
----------
train Loss: 0.2870 Acc: 0.9260
val Loss: 0.2828 Acc: 0.9268
Training complete in 129m 4s

Epoch 29/79
----------
train Loss: 0.2731 Acc: 0.9314
val Loss: 0.2260 Acc: 0.9268
Training complete in 133m 39s

Epoch 30/79
----------
train Loss: 0.2688 Acc: 0.9309
val Loss: 0.2970 Acc: 0.9095
Training complete in 138m 14s

Epoch 31/79
----------
train Loss: 0.2816 Acc: 0.9301
val Loss: 0.2949 Acc: 0.9188
Training complete in 142m 53s

Epoch 32/79
----------
train Loss: 0.2879 Acc: 0.9267
val Loss: 0.2570 Acc: 0.9308
Training complete in 147m 32s

Epoch 33/79
----------
train Loss: 0.2837 Acc: 0.9283
val Loss: 0.3421 Acc: 0.9148
Training complete in 152m 5s

Epoch 34/79
----------
train Loss: 0.2627 Acc: 0.9346
val Loss: 0.2631 Acc: 0.9161
Training complete in 156m 39s

Epoch 35/79
----------
train Loss: 0.2798 Acc: 0.9288
val Loss: 0.2834 Acc: 0.9228
Training complete in 161m 14s

Epoch 36/79
----------
train Loss: 0.2576 Acc: 0.9347
val Loss: 0.2657 Acc: 0.9268
Training complete in 165m 47s

Epoch 37/79
----------
train Loss: 0.2655 Acc: 0.9319
val Loss: 0.2832 Acc: 0.9201
Training complete in 170m 37s

Epoch 38/79
----------
train Loss: 0.2290 Acc: 0.9442
val Loss: 0.2797 Acc: 0.9214
Training complete in 175m 10s

Epoch 39/79
----------
train Loss: 0.1323 Acc: 0.9691
val Loss: 0.1821 Acc: 0.9521
Training complete in 179m 45s

Epoch 40/79
----------
train Loss: 0.0814 Acc: 0.9834
val Loss: 0.1524 Acc: 0.9534
Training complete in 184m 22s

Epoch 41/79
----------
train Loss: 0.0816 Acc: 0.9840
val Loss: 0.1403 Acc: 0.9534
Training complete in 188m 58s

Epoch 42/79
----------
train Loss: 0.0637 Acc: 0.9881
val Loss: 0.1287 Acc: 0.9614
Training complete in 193m 31s

Epoch 43/79
----------
train Loss: 0.0577 Acc: 0.9897
val Loss: 0.1145 Acc: 0.9601
Training complete in 198m 6s

Epoch 44/79
----------
train Loss: 0.0591 Acc: 0.9891
val Loss: 0.1218 Acc: 0.9640
Training complete in 202m 41s

Epoch 45/79
----------
train Loss: 0.0526 Acc: 0.9915
val Loss: 0.1144 Acc: 0.9640
Training complete in 207m 21s

Epoch 46/79
----------
train Loss: 0.0508 Acc: 0.9918
val Loss: 0.1127 Acc: 0.9614
Training complete in 211m 57s

Epoch 47/79
----------
train Loss: 0.0514 Acc: 0.9921
val Loss: 0.1181 Acc: 0.9627
Training complete in 216m 31s

Epoch 48/79
----------
train Loss: 0.0471 Acc: 0.9935
val Loss: 0.1163 Acc: 0.9627
Training complete in 221m 5s

Epoch 49/79
----------
train Loss: 0.0520 Acc: 0.9915
val Loss: 0.1133 Acc: 0.9680
Training complete in 225m 39s

Epoch 50/79
----------
train Loss: 0.0471 Acc: 0.9929
val Loss: 0.1089 Acc: 0.9640
Training complete in 230m 13s

Epoch 51/79
----------
train Loss: 0.0475 Acc: 0.9938
val Loss: 0.1082 Acc: 0.9667
Training complete in 234m 47s

Epoch 52/79
----------
train Loss: 0.0477 Acc: 0.9937
val Loss: 0.1115 Acc: 0.9614
Training complete in 239m 20s

Epoch 53/79
----------
train Loss: 0.0482 Acc: 0.9938
val Loss: 0.1029 Acc: 0.9680
Training complete in 243m 55s

Epoch 54/79
----------
train Loss: 0.0456 Acc: 0.9938
val Loss: 0.1103 Acc: 0.9640
Training complete in 248m 29s

Epoch 55/79
----------
train Loss: 0.0459 Acc: 0.9936
val Loss: 0.1148 Acc: 0.9667
Training complete in 253m 4s

Epoch 56/79
----------
train Loss: 0.0463 Acc: 0.9943
val Loss: 0.1134 Acc: 0.9640
Training complete in 257m 39s

Epoch 57/79
----------
train Loss: 0.0457 Acc: 0.9959
val Loss: 0.1096 Acc: 0.9654
Training complete in 262m 11s

Epoch 58/79
----------
train Loss: 0.0436 Acc: 0.9952
val Loss: 0.1083 Acc: 0.9680
Training complete in 266m 44s

Epoch 59/79
----------
train Loss: 0.0446 Acc: 0.9957
val Loss: 0.1129 Acc: 0.9640
Training complete in 271m 19s

Epoch 60/79
----------
train Loss: 0.0423 Acc: 0.9964
val Loss: 0.1062 Acc: 0.9720
Training complete in 275m 53s

Epoch 61/79
----------
train Loss: 0.0405 Acc: 0.9965
val Loss: 0.1104 Acc: 0.9680
Training complete in 280m 27s

Epoch 62/79
----------
train Loss: 0.0436 Acc: 0.9955
val Loss: 0.1152 Acc: 0.9654
Training complete in 285m 0s

Epoch 63/79
----------
train Loss: 0.0452 Acc: 0.9952
val Loss: 0.1123 Acc: 0.9667
Training complete in 289m 36s

Epoch 64/79
----------
train Loss: 0.0434 Acc: 0.9961
val Loss: 0.1167 Acc: 0.9667
Training complete in 294m 10s

Epoch 65/79
----------
train Loss: 0.0406 Acc: 0.9970
val Loss: 0.1031 Acc: 0.9680
Training complete in 298m 44s

Epoch 66/79
----------
train Loss: 0.0427 Acc: 0.9968
val Loss: 0.1132 Acc: 0.9694
Training complete in 303m 17s

Epoch 67/79
----------
train Loss: 0.0408 Acc: 0.9964
val Loss: 0.1098 Acc: 0.9667
Training complete in 307m 51s

Epoch 68/79
----------
train Loss: 0.0405 Acc: 0.9966
val Loss: 0.1186 Acc: 0.9680
Training complete in 312m 24s

Epoch 69/79
----------
train Loss: 0.0435 Acc: 0.9963
val Loss: 0.1159 Acc: 0.9667
Training complete in 316m 59s

Epoch 70/79
----------
train Loss: 0.0447 Acc: 0.9958
val Loss: 0.1191 Acc: 0.9654
Training complete in 321m 33s

Epoch 71/79
----------
train Loss: 0.0448 Acc: 0.9963
val Loss: 0.1122 Acc: 0.9667
Training complete in 326m 8s

Epoch 72/79
----------
train Loss: 0.0426 Acc: 0.9970
val Loss: 0.1179 Acc: 0.9680
Training complete in 330m 44s

Epoch 73/79
----------
train Loss: 0.0440 Acc: 0.9961
val Loss: 0.1212 Acc: 0.9680
Training complete in 335m 20s

Epoch 74/79
----------
train Loss: 0.0415 Acc: 0.9970
val Loss: 0.1180 Acc: 0.9680
Training complete in 339m 54s

Epoch 75/79
----------
train Loss: 0.0408 Acc: 0.9970
val Loss: 0.1172 Acc: 0.9640
Training complete in 344m 27s

Epoch 76/79
----------
train Loss: 0.0422 Acc: 0.9972
val Loss: 0.1187 Acc: 0.9654
Training complete in 349m 1s

Epoch 77/79
----------
train Loss: 0.0428 Acc: 0.9968
val Loss: 0.1206 Acc: 0.9654
Training complete in 353m 36s

Epoch 78/79
----------
train Loss: 0.0397 Acc: 0.9976
val Loss: 0.1222 Acc: 0.9694
Training complete in 358m 12s

Epoch 79/79
----------
train Loss: 0.0424 Acc: 0.9972
val Loss: 0.1224 Acc: 0.9654
Training complete in 362m 47s

Training complete in 362m 47s
usage: train.py [-h] [--gpu_ids GPU_IDS] [--name NAME] [--data_dir DATA_DIR]
                [--train_all] [--color_jitter] [--batchsize BATCHSIZE]
                [--stride STRIDE] [--erasing_p ERASING_P] [--use_dense]
                [--use_resnest50] [--use_resnest269] [--use_NAS]
                [--warm_epoch WARM_EPOCH] [--lr LR] [--droprate DROPRATE]
                [--PCB] [--rank RANK] [--world_size WORLD_SIZE]
                [--local_rank LOCAL_RANK] [--fp16]
train.py: error: unrecognized arguments: --_resnest50
This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7f7e0b4de350>]
2.8467884063720703
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=751, bias=True)
    )
  )
)
Epoch 0/79
----------
/home/luguangjian/.conda/envs/test_code/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Traceback (most recent call last):
  File "train.py", line 425, in <module>
    model = train_model(model, optimizer_ft, exp_lr_scheduler, criterion, num_epochs=80, local_rank=opt.local_rank)
  File "train.py", line 262, in train_model
    loss.backward()
  File "/home/luguangjian/.conda/envs/test_code/lib/python3.7/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/luguangjian/.conda/envs/test_code/lib/python3.7/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
KeyboardInterrupt
This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7f25f8341210>]
2.96000599861145
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (LeakyReLU_Sin): LeakyReLU_Sin()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU): Sum_ReLU()
        (Sum_ReLU_2): Sum_ReLU_2()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=751, bias=True)
    )
  )
)
Epoch 0/79
----------
/home/luguangjian/.conda/envs/test_code/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 0.9643 Acc: 0.0997
val Loss: 3.7212 Acc: 0.2597
Training complete in 3m 20s

Epoch 1/79
----------
train Loss: 0.9710 Acc: 0.4183
val Loss: 1.8716 Acc: 0.5526
Training complete in 6m 37s

Epoch 2/79
----------
train Loss: 1.0142 Acc: 0.5584
val Loss: 1.5347 Acc: 0.6032
Training complete in 9m 54s

Epoch 3/79
----------
train Loss: 1.0976 Acc: 0.6268
val Loss: 1.0079 Acc: 0.7377
Training complete in 13m 11s

Epoch 4/79
----------
train Loss: 1.1818 Acc: 0.6773
val Loss: 0.9151 Acc: 0.7563
Training complete in 15m 39s

Epoch 5/79
----------
train Loss: 1.0678 Acc: 0.7316
val Loss: 0.8801 Acc: 0.7670
Training complete in 18m 25s

Epoch 6/79
----------
train Loss: 0.8647 Acc: 0.7795
val Loss: 0.5194 Acc: 0.8509
Training complete in 21m 36s

Epoch 7/79
----------
train Loss: 0.7165 Acc: 0.8126
val Loss: 0.5581 Acc: 0.8509
Training complete in 24m 9s

Epoch 8/79
----------
train Loss: 0.6274 Acc: 0.8362
val Loss: 0.4240 Acc: 0.8748
Training complete in 26m 15s

Epoch 9/79
----------
train Loss: 0.5509 Acc: 0.8542
val Loss: 0.3682 Acc: 0.8961
Training complete in 28m 38s

Epoch 10/79
----------
train Loss: 0.5001 Acc: 0.8717
val Loss: 0.3287 Acc: 0.9001
Training complete in 30m 28s

Epoch 11/79
----------
train Loss: 0.4706 Acc: 0.8762
val Loss: 0.3389 Acc: 0.9015
Training complete in 32m 54s

Epoch 12/79
----------
train Loss: 0.4783 Acc: 0.8746
val Loss: 0.3439 Acc: 0.9015
Training complete in 35m 18s

Epoch 13/79
----------
train Loss: 0.4445 Acc: 0.8885
val Loss: 0.3128 Acc: 0.8948
Training complete in 37m 40s

Epoch 14/79
----------
train Loss: 0.4018 Acc: 0.8963
val Loss: 0.3614 Acc: 0.8961
Training complete in 40m 6s

Epoch 15/79
----------
train Loss: 0.4282 Acc: 0.8903
val Loss: 0.3502 Acc: 0.9028
Training complete in 42m 12s

Epoch 16/79
----------
train Loss: 0.3862 Acc: 0.9038
val Loss: 0.3498 Acc: 0.8908
Training complete in 44m 44s

Epoch 17/79
----------
train Loss: 0.3855 Acc: 0.9001
val Loss: 0.3275 Acc: 0.9095
Training complete in 46m 48s

Epoch 18/79
----------
train Loss: 0.3678 Acc: 0.9052
val Loss: 0.3258 Acc: 0.9121
Training complete in 48m 53s

Epoch 19/79
----------
train Loss: 0.3356 Acc: 0.9141
val Loss: 0.2791 Acc: 0.9214
Training complete in 51m 2s

Epoch 20/79
----------
train Loss: 0.3193 Acc: 0.9181
val Loss: 0.2304 Acc: 0.9348
Training complete in 53m 7s

Epoch 21/79
----------
train Loss: 0.2942 Acc: 0.9250
val Loss: 0.2708 Acc: 0.9201
Training complete in 55m 34s

Epoch 22/79
----------
train Loss: 0.3044 Acc: 0.9207
val Loss: 0.2929 Acc: 0.9148
Training complete in 57m 46s

Epoch 23/79
----------
train Loss: 0.3036 Acc: 0.9242
val Loss: 0.3113 Acc: 0.9161
Training complete in 60m 4s

Epoch 24/79
----------
train Loss: 0.3273 Acc: 0.9156
val Loss: 0.3144 Acc: 0.9214
Training complete in 62m 31s

Epoch 25/79
----------
train Loss: 0.2797 Acc: 0.9301
val Loss: 0.2712 Acc: 0.9294
Training complete in 64m 59s

Epoch 26/79
----------
train Loss: 0.2836 Acc: 0.9299
val Loss: 0.2977 Acc: 0.9201
Training complete in 67m 23s

Epoch 27/79
----------
train Loss: 0.2711 Acc: 0.9316
val Loss: 0.2973 Acc: 0.9068
Training complete in 69m 27s

Epoch 28/79
----------
train Loss: 0.2873 Acc: 0.9279
val Loss: 0.2615 Acc: 0.9254
Training complete in 71m 54s

Epoch 29/79
----------
train Loss: 0.2920 Acc: 0.9271
val Loss: 0.2666 Acc: 0.9241
Training complete in 73m 57s

Epoch 30/79
----------
train Loss: 0.2749 Acc: 0.9302
val Loss: 0.2621 Acc: 0.9281
Training complete in 75m 49s

Epoch 31/79
----------
train Loss: 0.2802 Acc: 0.9311
val Loss: 0.2932 Acc: 0.9214
Training complete in 77m 53s

Epoch 32/79
----------
train Loss: 0.2651 Acc: 0.9354
val Loss: 0.2602 Acc: 0.9081
Training complete in 80m 17s

Epoch 33/79
----------
train Loss: 0.2273 Acc: 0.9438
val Loss: 0.2817 Acc: 0.9148
Training complete in 82m 36s

Epoch 34/79
----------
train Loss: 0.2397 Acc: 0.9411
val Loss: 0.2934 Acc: 0.9228
Training complete in 84m 32s

Epoch 35/79
----------
train Loss: 0.2307 Acc: 0.9430
val Loss: 0.2621 Acc: 0.9268
Training complete in 86m 40s

Epoch 36/79
----------
train Loss: 0.2635 Acc: 0.9324
val Loss: 0.2589 Acc: 0.9241
Training complete in 89m 3s

Epoch 37/79
----------
train Loss: 0.2651 Acc: 0.9323
val Loss: 0.2914 Acc: 0.9201
Training complete in 91m 30s

Epoch 38/79
----------
train Loss: 0.2735 Acc: 0.9331
val Loss: 0.2609 Acc: 0.9268
Training complete in 94m 3s

Epoch 39/79
----------
train Loss: 0.1551 Acc: 0.9619
val Loss: 0.1405 Acc: 0.9561
Training complete in 96m 34s

Epoch 40/79
----------
train Loss: 0.0903 Acc: 0.9808
val Loss: 0.1266 Acc: 0.9587
Training complete in 99m 4s

Epoch 41/79
----------
train Loss: 0.0792 Acc: 0.9849
val Loss: 0.1184 Acc: 0.9627
Training complete in 101m 16s

Epoch 42/79
----------
train Loss: 0.0682 Acc: 0.9870
val Loss: 0.1128 Acc: 0.9654
Training complete in 103m 41s

Epoch 43/79
----------
train Loss: 0.0611 Acc: 0.9888
val Loss: 0.1032 Acc: 0.9680
Training complete in 106m 2s

Epoch 44/79
----------
train Loss: 0.0590 Acc: 0.9891
val Loss: 0.0959 Acc: 0.9654
Training complete in 108m 8s

Epoch 45/79
----------
train Loss: 0.0531 Acc: 0.9913
val Loss: 0.0985 Acc: 0.9654
Training complete in 110m 14s

Epoch 46/79
----------
train Loss: 0.0515 Acc: 0.9918
val Loss: 0.1026 Acc: 0.9654
Training complete in 112m 35s

Epoch 47/79
----------
train Loss: 0.0493 Acc: 0.9912
val Loss: 0.1071 Acc: 0.9654
Training complete in 114m 41s

Epoch 48/79
----------
train Loss: 0.0475 Acc: 0.9929
val Loss: 0.1055 Acc: 0.9627
Training complete in 116m 47s

Epoch 49/79
----------
train Loss: 0.0499 Acc: 0.9913
val Loss: 0.0984 Acc: 0.9720
Training complete in 119m 14s

Epoch 50/79
----------
train Loss: 0.0471 Acc: 0.9924
val Loss: 0.0906 Acc: 0.9720
Training complete in 121m 37s

Epoch 51/79
----------
train Loss: 0.0447 Acc: 0.9947
val Loss: 0.0958 Acc: 0.9747
Training complete in 124m 2s

Epoch 52/79
----------
train Loss: 0.0456 Acc: 0.9947
val Loss: 0.0987 Acc: 0.9707
Training complete in 126m 8s

Epoch 53/79
----------
train Loss: 0.0466 Acc: 0.9932
val Loss: 0.1026 Acc: 0.9720
Training complete in 128m 13s

Epoch 54/79
----------
train Loss: 0.0435 Acc: 0.9949
val Loss: 0.0995 Acc: 0.9734
Training complete in 130m 40s

Epoch 55/79
----------
train Loss: 0.0465 Acc: 0.9943
val Loss: 0.1040 Acc: 0.9694
Training complete in 132m 46s

Epoch 56/79
----------
train Loss: 0.0455 Acc: 0.9941
val Loss: 0.1019 Acc: 0.9707
Training complete in 134m 53s

Epoch 57/79
----------
train Loss: 0.0445 Acc: 0.9945
val Loss: 0.0941 Acc: 0.9694
Training complete in 137m 32s

Epoch 58/79
----------
train Loss: 0.0487 Acc: 0.9924
val Loss: 0.0960 Acc: 0.9694
Training complete in 139m 29s

Epoch 59/79
----------
train Loss: 0.0456 Acc: 0.9951
val Loss: 0.1013 Acc: 0.9707
Training complete in 141m 56s

Epoch 60/79
----------
train Loss: 0.0452 Acc: 0.9941
val Loss: 0.1048 Acc: 0.9707
Training complete in 144m 0s

Epoch 61/79
----------
train Loss: 0.0441 Acc: 0.9948
val Loss: 0.1045 Acc: 0.9707
Training complete in 146m 4s

Epoch 62/79
----------
train Loss: 0.0435 Acc: 0.9956
val Loss: 0.1054 Acc: 0.9694
Training complete in 148m 29s

Epoch 63/79
----------
train Loss: 0.0410 Acc: 0.9964
val Loss: 0.1039 Acc: 0.9680
Training complete in 150m 33s

Epoch 64/79
----------
train Loss: 0.0449 Acc: 0.9942
val Loss: 0.0983 Acc: 0.9707
Training complete in 152m 53s

Epoch 65/79
----------
train Loss: 0.0409 Acc: 0.9963
val Loss: 0.1033 Acc: 0.9707
Training complete in 155m 0s

Epoch 66/79
----------
train Loss: 0.0415 Acc: 0.9966
val Loss: 0.1079 Acc: 0.9680
Training complete in 157m 6s

Epoch 67/79
----------
train Loss: 0.0417 Acc: 0.9957
val Loss: 0.1051 Acc: 0.9694
Training complete in 159m 27s

Epoch 68/79
----------
train Loss: 0.0404 Acc: 0.9963
val Loss: 0.1016 Acc: 0.9734
Training complete in 161m 33s

Epoch 69/79
----------
train Loss: 0.0401 Acc: 0.9961
val Loss: 0.1096 Acc: 0.9720
Training complete in 164m 17s

Epoch 70/79
----------
train Loss: 0.0413 Acc: 0.9967
val Loss: 0.1099 Acc: 0.9707
Training complete in 167m 10s

Epoch 71/79
----------
train Loss: 0.0400 Acc: 0.9965
val Loss: 0.1079 Acc: 0.9734
Training complete in 169m 15s

Epoch 72/79
----------
train Loss: 0.0413 Acc: 0.9966
val Loss: 0.1115 Acc: 0.9707
Training complete in 171m 41s

Epoch 73/79
----------
train Loss: 0.0411 Acc: 0.9970
val Loss: 0.1065 Acc: 0.9707
Training complete in 174m 22s

Epoch 74/79
----------
train Loss: 0.0412 Acc: 0.9961
val Loss: 0.0979 Acc: 0.9774
Training complete in 177m 22s

Epoch 75/79
----------
train Loss: 0.0411 Acc: 0.9970
val Loss: 0.1067 Acc: 0.9747
Training complete in 179m 48s

Epoch 76/79
----------
train Loss: 0.0397 Acc: 0.9970
val Loss: 0.1133 Acc: 0.9707
Training complete in 182m 16s

Epoch 77/79
----------
train Loss: 0.0396 Acc: 0.9973
val Loss: 0.1091 Acc: 0.9694
Training complete in 184m 41s

Epoch 78/79
----------
train Loss: 0.0419 Acc: 0.9963
val Loss: 0.1106 Acc: 0.9720
Training complete in 186m 46s

Epoch 79/79
----------
train Loss: 0.0387 Acc: 0.9975
val Loss: 0.1115 Acc: 0.9734
Training complete in 188m 53s

Training complete in 188m 53s
usage: train.py [-h] [--gpu_ids GPU_IDS] [--name NAME] [--data_dir DATA_DIR]
                [--train_all] [--color_jitter] [--batchsize BATCHSIZE]
                [--stride STRIDE] [--erasing_p ERASING_P] [--use_dense]
                [--use_resnest50] [--use_resnest269] [--use_NAS]
                [--warm_epoch WARM_EPOCH] [--lr LR] [--droprate DROPRATE]
                [--PCB] [--rank RANK] [--world_size WORLD_SIZE]
                [--local_rank LOCAL_RANK] [--fp16]
train.py: error: unrecognized arguments: --
This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7f16ee263f50>]
2.875009059906006
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=751, bias=True)
    )
  )
)
Epoch 0/79
----------
/home/luguangjian/.conda/envs/test_code/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 0.9572 Acc: 0.0958
val Loss: 3.7793 Acc: 0.2370
Training complete in 4m 38s

Epoch 1/79
----------
train Loss: 0.9699 Acc: 0.4246
val Loss: 1.9071 Acc: 0.5273
Training complete in 9m 10s

Epoch 2/79
----------
train Loss: 1.0148 Acc: 0.5555
val Loss: 1.4044 Acc: 0.6445
Training complete in 13m 47s

Epoch 3/79
----------
train Loss: 1.0836 Acc: 0.6372
val Loss: 1.1457 Acc: 0.7124
Training complete in 19m 15s

Epoch 4/79
----------
train Loss: 1.2069 Acc: 0.6693
val Loss: 0.9931 Acc: 0.7150
Training complete in 26m 21s

Epoch 5/79
----------
train Loss: 1.0402 Acc: 0.7384
val Loss: 0.5874 Acc: 0.8322
Training complete in 33m 27s

Epoch 6/79
----------
train Loss: 0.8289 Acc: 0.7879
val Loss: 0.6226 Acc: 0.8322
Training complete in 40m 32s

Epoch 7/79
----------
train Loss: 0.7029 Acc: 0.8192
val Loss: 0.5234 Acc: 0.8455
Training complete in 47m 37s

Epoch 8/79
----------
train Loss: 0.5934 Acc: 0.8493
val Loss: 0.4557 Acc: 0.8668
Training complete in 54m 44s

Epoch 9/79
----------
train Loss: 0.5541 Acc: 0.8551
val Loss: 0.4188 Acc: 0.8762
Training complete in 61m 52s

Epoch 10/79
----------
train Loss: 0.5019 Acc: 0.8689
val Loss: 0.4071 Acc: 0.8895
Training complete in 68m 56s

Epoch 11/79
----------
train Loss: 0.5031 Acc: 0.8680
val Loss: 0.3877 Acc: 0.8842
Training complete in 75m 60s

Epoch 12/79
----------
train Loss: 0.4359 Acc: 0.8863
val Loss: 0.3532 Acc: 0.8988
Training complete in 83m 5s

Epoch 13/79
----------
train Loss: 0.4060 Acc: 0.8956
val Loss: 0.3343 Acc: 0.8961
Training complete in 90m 10s

Epoch 14/79
----------
train Loss: 0.4088 Acc: 0.8928
val Loss: 0.3506 Acc: 0.9028
Training complete in 97m 16s

Epoch 15/79
----------
train Loss: 0.3955 Acc: 0.8937
val Loss: 0.3046 Acc: 0.9095
Training complete in 104m 22s

Epoch 16/79
----------
train Loss: 0.3758 Acc: 0.9020
val Loss: 0.3730 Acc: 0.8988
Training complete in 111m 29s

Epoch 17/79
----------
train Loss: 0.3612 Acc: 0.9082
val Loss: 0.3544 Acc: 0.9001
Training complete in 118m 33s

Epoch 18/79
----------
train Loss: 0.3730 Acc: 0.9061
val Loss: 0.2911 Acc: 0.9041
Training complete in 125m 40s

Epoch 19/79
----------
train Loss: 0.3219 Acc: 0.9169
val Loss: 0.3068 Acc: 0.9174
Training complete in 132m 48s

Epoch 20/79
----------
train Loss: 0.3446 Acc: 0.9128
val Loss: 0.3024 Acc: 0.9028
Training complete in 139m 54s

Epoch 21/79
----------
train Loss: 0.3176 Acc: 0.9185
val Loss: 0.2591 Acc: 0.9241
Training complete in 146m 58s

Epoch 22/79
----------
train Loss: 0.3315 Acc: 0.9144
val Loss: 0.2601 Acc: 0.9308
Training complete in 154m 5s

Epoch 23/79
----------
train Loss: 0.3034 Acc: 0.9214
val Loss: 0.2505 Acc: 0.9228
Training complete in 161m 9s

Epoch 24/79
----------
train Loss: 0.3035 Acc: 0.9250
val Loss: 0.3200 Acc: 0.9001
Training complete in 168m 15s

Epoch 25/79
----------
train Loss: 0.2971 Acc: 0.9242
val Loss: 0.3057 Acc: 0.9268
Training complete in 175m 19s

Epoch 26/79
----------
train Loss: 0.2974 Acc: 0.9261
val Loss: 0.3158 Acc: 0.9134
Training complete in 182m 26s

Epoch 27/79
----------
train Loss: 0.2761 Acc: 0.9303
val Loss: 0.2371 Acc: 0.9254
Training complete in 189m 30s

Epoch 28/79
----------
train Loss: 0.2780 Acc: 0.9289
val Loss: 0.2955 Acc: 0.9188
Training complete in 196m 35s

Epoch 29/79
----------
train Loss: 0.2837 Acc: 0.9305
val Loss: 0.2957 Acc: 0.9161
Training complete in 203m 43s

Epoch 30/79
----------
train Loss: 0.2802 Acc: 0.9291
val Loss: 0.2798 Acc: 0.9201
Training complete in 210m 48s

Epoch 31/79
----------
train Loss: 0.2378 Acc: 0.9421
val Loss: 0.2309 Acc: 0.9334
Training complete in 217m 54s

Epoch 32/79
----------
train Loss: 0.2631 Acc: 0.9358
val Loss: 0.3031 Acc: 0.9148
Training complete in 224m 59s

Epoch 33/79
----------
train Loss: 0.2762 Acc: 0.9298
val Loss: 0.2908 Acc: 0.9174
Training complete in 232m 5s

Epoch 34/79
----------
train Loss: 0.2966 Acc: 0.9281
val Loss: 0.2310 Acc: 0.9361
Training complete in 239m 12s

Epoch 35/79
----------
train Loss: 0.2638 Acc: 0.9335
val Loss: 0.2653 Acc: 0.9308
Training complete in 246m 16s

Epoch 36/79
----------
train Loss: 0.2535 Acc: 0.9371
val Loss: 0.2452 Acc: 0.9254
Training complete in 253m 22s

Epoch 37/79
----------
train Loss: 0.2432 Acc: 0.9407
val Loss: 0.2247 Acc: 0.9334
Training complete in 260m 26s

Epoch 38/79
----------
train Loss: 0.2431 Acc: 0.9392
val Loss: 0.2577 Acc: 0.9308
Training complete in 267m 33s

Epoch 39/79
----------
train Loss: 0.1477 Acc: 0.9665
val Loss: 0.1542 Acc: 0.9561
Training complete in 274m 40s

Epoch 40/79
----------
train Loss: 0.0812 Acc: 0.9833
val Loss: 0.1448 Acc: 0.9574
Training complete in 281m 46s

Epoch 41/79
----------
train Loss: 0.0747 Acc: 0.9849
val Loss: 0.1193 Acc: 0.9574
Training complete in 288m 52s

Epoch 42/79
----------
train Loss: 0.0698 Acc: 0.9862
val Loss: 0.1251 Acc: 0.9601
Training complete in 295m 58s

Epoch 43/79
----------
train Loss: 0.0561 Acc: 0.9887
val Loss: 0.1179 Acc: 0.9614
Training complete in 303m 0s

Epoch 44/79
----------
train Loss: 0.0603 Acc: 0.9892
val Loss: 0.1147 Acc: 0.9614
Training complete in 310m 7s

Epoch 45/79
----------
train Loss: 0.0532 Acc: 0.9915
val Loss: 0.1160 Acc: 0.9627
Training complete in 317m 12s

Epoch 46/79
----------
train Loss: 0.0535 Acc: 0.9909
val Loss: 0.1250 Acc: 0.9627
Training complete in 324m 18s

Epoch 47/79
----------
train Loss: 0.0526 Acc: 0.9913
val Loss: 0.1193 Acc: 0.9627
Training complete in 331m 23s

Epoch 48/79
----------
train Loss: 0.0549 Acc: 0.9906
val Loss: 0.1150 Acc: 0.9627
Training complete in 338m 29s

Epoch 49/79
----------
train Loss: 0.0485 Acc: 0.9929
val Loss: 0.1123 Acc: 0.9654
Training complete in 345m 36s

Epoch 50/79
----------
train Loss: 0.0491 Acc: 0.9923
val Loss: 0.1125 Acc: 0.9654
Training complete in 352m 39s

Epoch 51/79
----------
train Loss: 0.0508 Acc: 0.9923
val Loss: 0.1167 Acc: 0.9640
Training complete in 359m 45s

Epoch 52/79
----------
train Loss: 0.0474 Acc: 0.9933
val Loss: 0.1119 Acc: 0.9627
Training complete in 366m 51s

Epoch 53/79
----------
train Loss: 0.0435 Acc: 0.9948
val Loss: 0.1099 Acc: 0.9654
Training complete in 373m 58s

Epoch 54/79
----------
train Loss: 0.0470 Acc: 0.9934
val Loss: 0.1116 Acc: 0.9680
Training complete in 382m 29s

Epoch 55/79
----------
train Loss: 0.0439 Acc: 0.9948
val Loss: 0.1074 Acc: 0.9680
Training complete in 391m 23s

Epoch 56/79
----------
train Loss: 0.0451 Acc: 0.9937
val Loss: 0.1088 Acc: 0.9640
Training complete in 398m 29s

Epoch 57/79
----------
train Loss: 0.0437 Acc: 0.9941
val Loss: 0.1174 Acc: 0.9667
Training complete in 404m 3s

Epoch 58/79
----------
train Loss: 0.0430 Acc: 0.9955
val Loss: 0.1127 Acc: 0.9640
Training complete in 408m 24s

Epoch 59/79
----------
train Loss: 0.0421 Acc: 0.9956
val Loss: 0.1084 Acc: 0.9654
Training complete in 410m 56s

Epoch 60/79
----------
train Loss: 0.0474 Acc: 0.9936
val Loss: 0.1117 Acc: 0.9694
Training complete in 413m 28s

Epoch 61/79
----------
train Loss: 0.0413 Acc: 0.9960
val Loss: 0.1127 Acc: 0.9680
Training complete in 416m 21s

Epoch 62/79
----------
train Loss: 0.0427 Acc: 0.9956
val Loss: 0.1148 Acc: 0.9654
Training complete in 419m 11s

Epoch 63/79
----------
train Loss: 0.0376 Acc: 0.9970
val Loss: 0.1109 Acc: 0.9694
Training complete in 421m 42s

Epoch 64/79
----------
train Loss: 0.0424 Acc: 0.9960
val Loss: 0.1071 Acc: 0.9707
Training complete in 424m 35s

Epoch 65/79
----------
train Loss: 0.0434 Acc: 0.9956
val Loss: 0.1127 Acc: 0.9680
Training complete in 427m 21s

Epoch 66/79
----------
train Loss: 0.0405 Acc: 0.9965
val Loss: 0.1183 Acc: 0.9680
Training complete in 430m 15s

Epoch 67/79
----------
train Loss: 0.0417 Acc: 0.9968
val Loss: 0.1177 Acc: 0.9680
Training complete in 433m 9s

Epoch 68/79
----------
train Loss: 0.0406 Acc: 0.9966
val Loss: 0.1173 Acc: 0.9694
Training complete in 435m 51s

Epoch 69/79
----------
train Loss: 0.0427 Acc: 0.9960
val Loss: 0.1203 Acc: 0.9720
Training complete in 438m 22s

Epoch 70/79
----------
train Loss: 0.0431 Acc: 0.9964
val Loss: 0.1180 Acc: 0.9694
Training complete in 440m 52s

Epoch 71/79
----------
train Loss: 0.0408 Acc: 0.9964
val Loss: 0.1201 Acc: 0.9694
Training complete in 443m 21s

Epoch 72/79
----------
train Loss: 0.0398 Acc: 0.9971
val Loss: 0.1124 Acc: 0.9720
Training complete in 447m 42s

Epoch 73/79
----------
train Loss: 0.0413 Acc: 0.9973
val Loss: 0.1163 Acc: 0.9707
Training complete in 453m 16s

Epoch 74/79
----------
train Loss: 0.0420 Acc: 0.9964
val Loss: 0.1272 Acc: 0.9720
Training complete in 456m 45s

Epoch 75/79
----------
train Loss: 0.0415 Acc: 0.9970
val Loss: 0.1151 Acc: 0.9720
Training complete in 459m 26s

Epoch 76/79
----------
train Loss: 0.0414 Acc: 0.9967
val Loss: 0.1177 Acc: 0.9707
Training complete in 461m 58s

Epoch 77/79
----------
train Loss: 0.0406 Acc: 0.9970
val Loss: 0.1064 Acc: 0.9720
Training complete in 464m 32s

Epoch 78/79
----------
train Loss: 0.0399 Acc: 0.9972
val Loss: 0.1147 Acc: 0.9694
Training complete in 467m 4s

Epoch 79/79
----------
train Loss: 0.0384 Acc: 0.9975
val Loss: 0.1146 Acc: 0.9720
Training complete in 470m 0s

Training complete in 470m 0s
This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7fc38de94110>]
5.1412951946258545
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/79
----------
/home/luguangjian/.conda/envs/test_code/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 0.8633 Acc: 0.1764
val Loss: 2.5821 Acc: 0.4558
Training complete in 5m 35s

Epoch 1/79
----------
train Loss: 0.7799 Acc: 0.5296
val Loss: 1.0724 Acc: 0.7080
Training complete in 11m 6s

Epoch 2/79
----------
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7f1d5417e710>]
2.512450695037842
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/79
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 0.8551 Acc: 0.1797
val Loss: 2.6658 Acc: 0.4402
Training complete in 3m 31s

Epoch 1/79
----------
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7f9bf5e5b668>]
5.5238940715789795
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/79
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Traceback (most recent call last):
  File "train.py", line 425, in <module>
    model = train_model(model, optimizer_ft, exp_lr_scheduler, criterion, num_epochs=80, local_rank=opt.local_rank)
  File "train.py", line 233, in train_model
    outputs = model(inputs)
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/module.py", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/luguangjian/tmp12/reid/model.py", line 125, in forward
    x = self.model.layer1(x)
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/module.py", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/container.py", line 92, in forward
    input = module(input)
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/module.py", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/luguangjian/tmp12/reid/resnest/resnet.py", line 124, in forward
    out = self.conv2(out)
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/module.py", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/luguangjian/tmp12/reid/resnest/splat.py", line 60, in forward
    x = self.leakyrelu(x)
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/module.py", line 585, in __getattr__
    type(self).__name__, name))
AttributeError: 'SplAtConv2d' object has no attribute 'leakyrelu'
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7f603f4ab5f8>]
5.518061399459839
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (Sum_ReLU): Sum_ReLU()
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/79
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Traceback (most recent call last):
  File "train.py", line 425, in <module>
    model = train_model(model, optimizer_ft, exp_lr_scheduler, criterion, num_epochs=80, local_rank=opt.local_rank)
  File "train.py", line 233, in train_model
    outputs = model(inputs)
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/module.py", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/luguangjian/tmp12/reid/model.py", line 125, in forward
    x = self.model.layer1(x)
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/module.py", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/container.py", line 92, in forward
    input = module(input)
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/module.py", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/luguangjian/tmp12/reid/resnest/resnet.py", line 156, in forward
    out = self.Sum_ReLU(out)
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/module.py", line 585, in __getattr__
    type(self).__name__, name))
AttributeError: 'Bottleneck' object has no attribute 'Sum_ReLU'
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7fbc59fbd4e0>]
5.245745420455933
Traceback (most recent call last):
  File "train.py", line 345, in <module>
    model = ft_resnest50(len(class_names), opt.droprate)
  File "/home/luguangjian/tmp12/reid/model.py", line 99, in __init__
    model_ft = resnest.resnest50(pretrained=True)
  File "/home/luguangjian/tmp12/reid/resnest/resnest.py", line 59, in resnest50
    avd=True, avd_first=False, **kwargs)
  File "/home/luguangjian/tmp12/reid/resnest/resnet.py", line 228, in __init__
    self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer, is_first=False)
  File "/home/luguangjian/tmp12/reid/resnest/resnet.py", line 292, in _make_layer
    last_gamma=self.last_gamma))
  File "/home/luguangjian/tmp12/reid/resnest/resnet.py", line 67, in __init__
    dropblock_prob=dropblock_prob)
  File "/home/luguangjian/tmp12/reid/resnest/splat.py", line 27, in __init__
    self.Sum_ReLU = Sum_ReLU()
NameError: name 'Sum_ReLU' is not defined
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7f126ff13390>]
5.626896381378174
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/79
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 0.8650 Acc: 0.1707
val Loss: 2.7972 Acc: 0.4160
Training complete in 13m 28s

Epoch 1/79
----------
train Loss: 0.8356 Acc: 0.4979
val Loss: 1.2106 Acc: 0.6809
Training complete in 28m 54s

Epoch 2/79
----------
train Loss: 0.8699 Acc: 0.6241
val Loss: 0.9297 Acc: 0.7578
Training complete in 44m 18s

Epoch 3/79
----------
train Loss: 0.9797 Acc: 0.6794
val Loss: 0.6253 Acc: 0.8376
Training complete in 59m 45s

Epoch 4/79
----------
train Loss: 1.0605 Acc: 0.7197
val Loss: 0.5894 Acc: 0.8376
Training complete in 75m 8s

Epoch 5/79
----------
train Loss: 0.9882 Acc: 0.7579
val Loss: 0.4365 Acc: 0.8832
Training complete in 90m 34s

Epoch 6/79
----------
train Loss: 0.7651 Acc: 0.8091
val Loss: 0.3316 Acc: 0.9174
Training complete in 105m 58s

Epoch 7/79
----------
train Loss: 0.6773 Acc: 0.8308
val Loss: 0.3226 Acc: 0.9131
Training complete in 121m 23s

Epoch 8/79
----------
train Loss: 0.6165 Acc: 0.8471
val Loss: 0.3311 Acc: 0.9117
Training complete in 136m 49s

Epoch 9/79
----------
train Loss: 0.5399 Acc: 0.8641
val Loss: 0.3262 Acc: 0.9103
Training complete in 152m 20s

Epoch 10/79
----------
train Loss: 0.5008 Acc: 0.8722
val Loss: 0.2505 Acc: 0.9259
Training complete in 167m 51s

Epoch 11/79
----------
train Loss: 0.4655 Acc: 0.8847
val Loss: 0.2592 Acc: 0.9302
Training complete in 183m 22s

Epoch 12/79
----------
train Loss: 0.4434 Acc: 0.8876
val Loss: 0.2572 Acc: 0.9330
Training complete in 198m 46s

Epoch 13/79
----------
train Loss: 0.4222 Acc: 0.8945
val Loss: 0.3178 Acc: 0.9046
Training complete in 214m 11s

Epoch 14/79
----------
train Loss: 0.4194 Acc: 0.8921
val Loss: 0.2265 Acc: 0.9373
Training complete in 229m 38s

Epoch 15/79
----------
train Loss: 0.4140 Acc: 0.8964
val Loss: 0.2274 Acc: 0.9359
Training complete in 245m 3s

Epoch 16/79
----------
train Loss: 0.3870 Acc: 0.9021
val Loss: 0.1953 Acc: 0.9373
Training complete in 260m 28s

Epoch 17/79
----------
train Loss: 0.3626 Acc: 0.9090
val Loss: 0.1876 Acc: 0.9459
Training complete in 275m 54s

Epoch 18/79
----------
train Loss: 0.3669 Acc: 0.9069
val Loss: 0.2268 Acc: 0.9302
Training complete in 291m 16s

Epoch 19/79
----------
train Loss: 0.3677 Acc: 0.9076
val Loss: 0.2434 Acc: 0.9345
Training complete in 306m 45s

Epoch 20/79
----------
train Loss: 0.3441 Acc: 0.9147
val Loss: 0.2061 Acc: 0.9430
Training complete in 322m 10s

Epoch 21/79
----------
train Loss: 0.3287 Acc: 0.9171
val Loss: 0.2097 Acc: 0.9345
Training complete in 337m 34s

Epoch 22/79
----------
train Loss: 0.3222 Acc: 0.9185
val Loss: 0.2254 Acc: 0.9359
Training complete in 352m 58s

Epoch 23/79
----------
train Loss: 0.3360 Acc: 0.9156
val Loss: 0.2012 Acc: 0.9416
Training complete in 368m 22s

Epoch 24/79
----------
train Loss: 0.3178 Acc: 0.9187
val Loss: 0.1960 Acc: 0.9473
Training complete in 383m 45s

Epoch 25/79
----------
train Loss: 0.3005 Acc: 0.9248
val Loss: 0.1926 Acc: 0.9430
Training complete in 399m 9s

Epoch 26/79
----------
train Loss: 0.2993 Acc: 0.9229
val Loss: 0.2110 Acc: 0.9345
Training complete in 414m 34s

Epoch 27/79
----------
train Loss: 0.3080 Acc: 0.9255
val Loss: 0.2184 Acc: 0.9473
Training complete in 429m 60s

Epoch 28/79
----------
train Loss: 0.3128 Acc: 0.9217
val Loss: 0.2363 Acc: 0.9387
Training complete in 445m 24s

Epoch 29/79
----------
train Loss: 0.2676 Acc: 0.9346
val Loss: 0.2073 Acc: 0.9416
Training complete in 460m 52s

Epoch 30/79
----------
train Loss: 0.2471 Acc: 0.9392
val Loss: 0.2008 Acc: 0.9487
Training complete in 476m 18s

Epoch 31/79
----------
train Loss: 0.2626 Acc: 0.9329
val Loss: 0.2159 Acc: 0.9345
Training complete in 491m 44s

Epoch 32/79
----------
train Loss: 0.2796 Acc: 0.9311
val Loss: 0.2069 Acc: 0.9416
Training complete in 507m 9s

Epoch 33/79
----------
train Loss: 0.2857 Acc: 0.9298
val Loss: 0.2110 Acc: 0.9416
Training complete in 522m 35s

Epoch 34/79
----------
train Loss: 0.3053 Acc: 0.9251
val Loss: 0.1925 Acc: 0.9444
Training complete in 537m 60s

Epoch 35/79
----------
train Loss: 0.2639 Acc: 0.9362
val Loss: 0.2098 Acc: 0.9516
Training complete in 553m 26s

Epoch 36/79
----------
train Loss: 0.2532 Acc: 0.9407
val Loss: 0.1918 Acc: 0.9530
Training complete in 568m 53s

Epoch 37/79
----------
train Loss: 0.2471 Acc: 0.9429
val Loss: 0.2080 Acc: 0.9416
Training complete in 584m 17s

Epoch 38/79
----------
train Loss: 0.2791 Acc: 0.9311
val Loss: 0.1887 Acc: 0.9430
Training complete in 599m 43s

Epoch 39/79
----------
train Loss: 0.1542 Acc: 0.9639
val Loss: 0.1019 Acc: 0.9687
Training complete in 615m 12s

Epoch 40/79
----------
train Loss: 0.0935 Acc: 0.9798
val Loss: 0.0962 Acc: 0.9715
Training complete in 630m 39s

Epoch 41/79
----------
train Loss: 0.0762 Acc: 0.9834
val Loss: 0.0908 Acc: 0.9772
Training complete in 646m 3s

Epoch 42/79
----------
train Loss: 0.0652 Acc: 0.9878
val Loss: 0.0917 Acc: 0.9758
Training complete in 661m 29s

Epoch 43/79
----------
train Loss: 0.0627 Acc: 0.9887
val Loss: 0.0722 Acc: 0.9744
Training complete in 676m 53s

Epoch 44/79
----------
train Loss: 0.0592 Acc: 0.9903
val Loss: 0.0786 Acc: 0.9786
Training complete in 692m 17s

Epoch 45/79
----------
train Loss: 0.0527 Acc: 0.9916
val Loss: 0.0849 Acc: 0.9744
Training complete in 707m 43s

Epoch 46/79
----------
train Loss: 0.0562 Acc: 0.9910
val Loss: 0.0755 Acc: 0.9758
Training complete in 723m 6s

Epoch 47/79
----------
train Loss: 0.0521 Acc: 0.9918
val Loss: 0.0774 Acc: 0.9744
Training complete in 735m 17s

Epoch 48/79
----------
train Loss: 0.0485 Acc: 0.9932
val Loss: 0.0743 Acc: 0.9772
Training complete in 746m 48s

Epoch 49/79
----------
train Loss: 0.0474 Acc: 0.9934
val Loss: 0.0717 Acc: 0.9772
Training complete in 755m 7s

Epoch 50/79
----------
train Loss: 0.0477 Acc: 0.9933
val Loss: 0.0793 Acc: 0.9772
Training complete in 762m 44s

Epoch 51/79
----------
train Loss: 0.0487 Acc: 0.9932
val Loss: 0.0749 Acc: 0.9786
Training complete in 770m 21s

Epoch 52/79
----------
train Loss: 0.0472 Acc: 0.9936
val Loss: 0.0731 Acc: 0.9786
Training complete in 777m 58s

Epoch 53/79
----------
train Loss: 0.0476 Acc: 0.9939
val Loss: 0.0758 Acc: 0.9786
Training complete in 785m 36s

Epoch 54/79
----------
train Loss: 0.0518 Acc: 0.9926
val Loss: 0.0699 Acc: 0.9786
Training complete in 790m 2s

Epoch 55/79
----------
train Loss: 0.0464 Acc: 0.9946
val Loss: 0.0719 Acc: 0.9786
Training complete in 793m 55s

Epoch 56/79
----------
train Loss: 0.0488 Acc: 0.9942
val Loss: 0.0732 Acc: 0.9786
Training complete in 798m 3s

Epoch 57/79
----------
train Loss: 0.0436 Acc: 0.9951
val Loss: 0.0784 Acc: 0.9801
Training complete in 801m 56s

Epoch 58/79
----------
train Loss: 0.0435 Acc: 0.9951
val Loss: 0.0819 Acc: 0.9772
Training complete in 806m 12s

Epoch 59/79
----------
train Loss: 0.0466 Acc: 0.9945
val Loss: 0.0806 Acc: 0.9772
Training complete in 810m 40s

Epoch 60/79
----------
train Loss: 0.0468 Acc: 0.9949
val Loss: 0.0888 Acc: 0.9801
Training complete in 814m 28s

Epoch 61/79
----------
train Loss: 0.0468 Acc: 0.9953
val Loss: 0.0876 Acc: 0.9801
Training complete in 818m 43s

Epoch 62/79
----------
train Loss: 0.0440 Acc: 0.9958
val Loss: 0.0823 Acc: 0.9786
Training complete in 822m 27s

Epoch 63/79
----------
train Loss: 0.0433 Acc: 0.9963
val Loss: 0.0803 Acc: 0.9772
Training complete in 826m 42s

Epoch 64/79
----------
train Loss: 0.0433 Acc: 0.9958
val Loss: 0.0788 Acc: 0.9801
Training complete in 830m 55s

Epoch 65/79
----------
train Loss: 0.0434 Acc: 0.9965
val Loss: 0.0771 Acc: 0.9786
Training complete in 834m 57s

Epoch 66/79
----------
train Loss: 0.0442 Acc: 0.9954
val Loss: 0.0878 Acc: 0.9772
Training complete in 839m 8s

Epoch 67/79
----------
train Loss: 0.0432 Acc: 0.9961
val Loss: 0.0785 Acc: 0.9801
Training complete in 842m 53s

Epoch 68/79
----------
train Loss: 0.0445 Acc: 0.9961
val Loss: 0.0868 Acc: 0.9744
Training complete in 846m 45s

Epoch 69/79
----------
train Loss: 0.0423 Acc: 0.9969
val Loss: 0.0838 Acc: 0.9786
Training complete in 850m 56s

Epoch 70/79
----------
train Loss: 0.0437 Acc: 0.9962
val Loss: 0.0795 Acc: 0.9772
Training complete in 854m 43s

Epoch 71/79
----------
train Loss: 0.0456 Acc: 0.9962
val Loss: 0.0847 Acc: 0.9815
Training complete in 863m 26s

Epoch 72/79
----------
train Loss: 0.0429 Acc: 0.9964
val Loss: 0.0835 Acc: 0.9801
Training complete in 873m 47s

Epoch 73/79
----------
train Loss: 0.0410 Acc: 0.9972
val Loss: 0.0782 Acc: 0.9786
Training complete in 880m 52s

Epoch 74/79
----------
train Loss: 0.0406 Acc: 0.9976
val Loss: 0.0809 Acc: 0.9772
Training complete in 891m 15s

Epoch 75/79
----------
train Loss: 0.0423 Acc: 0.9962
val Loss: 0.0779 Acc: 0.9772
Training complete in 895m 9s

Epoch 76/79
----------
train Loss: 0.0414 Acc: 0.9968
val Loss: 0.0811 Acc: 0.9786
Training complete in 899m 36s

Epoch 77/79
----------
train Loss: 0.0408 Acc: 0.9970
val Loss: 0.0835 Acc: 0.9772
Training complete in 904m 13s

Epoch 78/79
----------
train Loss: 0.0405 Acc: 0.9972
val Loss: 0.0782 Acc: 0.9786
Training complete in 908m 40s

Epoch 79/79
----------
train Loss: 0.0410 Acc: 0.9970
val Loss: 0.0747 Acc: 0.9801
Training complete in 913m 3s

Training complete in 913m 3s
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7f9dd2d0d588>]
3.7472128868103027
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/79
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 0.8564 Acc: 0.1781
val Loss: 2.7330 Acc: 0.4487
Training complete in 7m 32s

Epoch 1/79
----------
train Loss: 0.8091 Acc: 0.5125
val Loss: 1.2498 Acc: 0.6624
Training complete in 14m 59s

Epoch 2/79
----------
train Loss: 0.8668 Acc: 0.6308
val Loss: 0.7719 Acc: 0.7977
Training complete in 22m 27s

Epoch 3/79
----------
train Loss: 0.9518 Acc: 0.6864
val Loss: 0.8141 Acc: 0.7849
Training complete in 29m 53s

Epoch 4/79
----------
train Loss: 1.0580 Acc: 0.7163
val Loss: 0.5639 Acc: 0.8490
Training complete in 37m 20s

Epoch 5/79
----------
train Loss: 0.9907 Acc: 0.7533
val Loss: 0.4190 Acc: 0.8875
Training complete in 44m 48s

Epoch 6/79
----------
train Loss: 0.7609 Acc: 0.8116
val Loss: 0.4492 Acc: 0.8903
Training complete in 52m 13s

Epoch 7/79
----------
train Loss: 0.6951 Acc: 0.8299
val Loss: 0.3203 Acc: 0.9003
Training complete in 59m 40s

Epoch 8/79
----------
train Loss: 0.5719 Acc: 0.8552
val Loss: 0.2496 Acc: 0.9202
Training complete in 67m 8s

Epoch 9/79
----------
train Loss: 0.5587 Acc: 0.8593
val Loss: 0.2709 Acc: 0.9174
Training complete in 74m 36s

Epoch 10/79
----------
train Loss: 0.4937 Acc: 0.8738
val Loss: 0.2588 Acc: 0.9330
Training complete in 82m 2s

Epoch 11/79
----------
train Loss: 0.4848 Acc: 0.8762
val Loss: 0.2591 Acc: 0.9217
Training complete in 89m 29s

Epoch 12/79
----------
train Loss: 0.4604 Acc: 0.8812
val Loss: 0.2181 Acc: 0.9330
Training complete in 96m 57s

Epoch 13/79
----------
train Loss: 0.4195 Acc: 0.8920
val Loss: 0.2335 Acc: 0.9288
Training complete in 104m 24s

Epoch 14/79
----------
train Loss: 0.4033 Acc: 0.8972
val Loss: 0.2185 Acc: 0.9359
Training complete in 111m 50s

Epoch 15/79
----------
train Loss: 0.3972 Acc: 0.8970
val Loss: 0.2308 Acc: 0.9259
Training complete in 119m 17s

Epoch 16/79
----------
train Loss: 0.3888 Acc: 0.9001
val Loss: 0.2100 Acc: 0.9345
Training complete in 126m 45s

Epoch 17/79
----------
train Loss: 0.3622 Acc: 0.9085
val Loss: 0.1874 Acc: 0.9487
Training complete in 134m 10s

Epoch 18/79
----------
train Loss: 0.3287 Acc: 0.9162
val Loss: 0.1594 Acc: 0.9573
Training complete in 141m 36s

Epoch 19/79
----------
train Loss: 0.3730 Acc: 0.9052
val Loss: 0.2280 Acc: 0.9487
Training complete in 147m 58s

Epoch 20/79
----------
train Loss: 0.3417 Acc: 0.9139
val Loss: 0.1963 Acc: 0.9402
Training complete in 151m 56s

Epoch 21/79
----------
train Loss: 0.3563 Acc: 0.9095
val Loss: 0.2040 Acc: 0.9387
Training complete in 156m 19s

Epoch 22/79
----------
train Loss: 0.3017 Acc: 0.9265
val Loss: 0.1676 Acc: 0.9516
Training complete in 160m 45s

Epoch 23/79
----------
train Loss: 0.3074 Acc: 0.9242
val Loss: 0.1702 Acc: 0.9487
Training complete in 164m 58s

Epoch 24/79
----------
train Loss: 0.3127 Acc: 0.9243
val Loss: 0.2092 Acc: 0.9487
Training complete in 169m 21s

Epoch 25/79
----------
train Loss: 0.2821 Acc: 0.9300
val Loss: 0.1801 Acc: 0.9430
Training complete in 173m 50s

Epoch 26/79
----------
train Loss: 0.2991 Acc: 0.9238
val Loss: 0.2516 Acc: 0.9345
Training complete in 178m 10s

Epoch 27/79
----------
train Loss: 0.3340 Acc: 0.9176
val Loss: 0.2258 Acc: 0.9430
Training complete in 182m 11s

Epoch 28/79
----------
train Loss: 0.3018 Acc: 0.9244
val Loss: 0.2331 Acc: 0.9402
Training complete in 186m 11s

Epoch 29/79
----------
train Loss: 0.2755 Acc: 0.9312
val Loss: 0.1465 Acc: 0.9558
Training complete in 190m 6s

Epoch 30/79
----------
train Loss: 0.2602 Acc: 0.9342
val Loss: 0.2098 Acc: 0.9444
Training complete in 194m 23s

Epoch 31/79
----------
train Loss: 0.2795 Acc: 0.9320
val Loss: 0.1403 Acc: 0.9630
Training complete in 198m 47s

Epoch 32/79
----------
train Loss: 0.2548 Acc: 0.9369
val Loss: 0.1814 Acc: 0.9487
Training complete in 202m 37s

Epoch 33/79
----------
train Loss: 0.2753 Acc: 0.9316
val Loss: 0.2015 Acc: 0.9487
Training complete in 206m 29s

Epoch 34/79
----------
train Loss: 0.2656 Acc: 0.9331
val Loss: 0.1464 Acc: 0.9558
Training complete in 210m 56s

Epoch 35/79
----------
train Loss: 0.2534 Acc: 0.9369
val Loss: 0.2080 Acc: 0.9530
Training complete in 215m 20s

Epoch 36/79
----------
train Loss: 0.2634 Acc: 0.9385
val Loss: 0.1459 Acc: 0.9601
Training complete in 219m 11s

Epoch 37/79
----------
train Loss: 0.2564 Acc: 0.9362
val Loss: 0.2153 Acc: 0.9359
Training complete in 223m 18s

Epoch 38/79
----------
train Loss: 0.2530 Acc: 0.9384
val Loss: 0.1818 Acc: 0.9473
Training complete in 227m 44s

Epoch 39/79
----------
train Loss: 0.1495 Acc: 0.9664
val Loss: 0.0989 Acc: 0.9715
Training complete in 232m 1s

Epoch 40/79
----------
train Loss: 0.0844 Acc: 0.9830
val Loss: 0.0989 Acc: 0.9715
Training complete in 236m 6s

Epoch 41/79
----------
train Loss: 0.0785 Acc: 0.9841
val Loss: 0.0890 Acc: 0.9715
Training complete in 240m 29s

Epoch 42/79
----------
train Loss: 0.0632 Acc: 0.9875
val Loss: 0.0845 Acc: 0.9701
Training complete in 244m 58s

Epoch 43/79
----------
train Loss: 0.0638 Acc: 0.9883
val Loss: 0.0798 Acc: 0.9715
Training complete in 249m 23s

Epoch 44/79
----------
train Loss: 0.0561 Acc: 0.9903
val Loss: 0.0769 Acc: 0.9715
Training complete in 253m 59s

Epoch 45/79
----------
train Loss: 0.0554 Acc: 0.9914
val Loss: 0.0829 Acc: 0.9715
Training complete in 258m 21s

Epoch 46/79
----------
train Loss: 0.0523 Acc: 0.9915
val Loss: 0.0823 Acc: 0.9729
Training complete in 262m 38s

Epoch 47/79
----------
train Loss: 0.0541 Acc: 0.9912
val Loss: 0.0789 Acc: 0.9715
Training complete in 266m 59s

Epoch 48/79
----------
train Loss: 0.0489 Acc: 0.9920
val Loss: 0.0756 Acc: 0.9701
Training complete in 271m 16s

Epoch 49/79
----------
train Loss: 0.0465 Acc: 0.9936
val Loss: 0.0790 Acc: 0.9687
Training complete in 275m 8s

Epoch 50/79
----------
train Loss: 0.0457 Acc: 0.9936
val Loss: 0.0792 Acc: 0.9729
Training complete in 279m 31s

Epoch 51/79
----------
train Loss: 0.0485 Acc: 0.9934
val Loss: 0.0768 Acc: 0.9729
Training complete in 284m 43s

Epoch 52/79
----------
train Loss: 0.0465 Acc: 0.9949
val Loss: 0.0803 Acc: 0.9715
Training complete in 289m 5s

Epoch 53/79
----------
train Loss: 0.0453 Acc: 0.9943
val Loss: 0.0780 Acc: 0.9729
Training complete in 293m 15s

Epoch 54/79
----------
train Loss: 0.0435 Acc: 0.9951
val Loss: 0.0783 Acc: 0.9715
Training complete in 297m 14s

Epoch 55/79
----------
train Loss: 0.0482 Acc: 0.9935
val Loss: 0.0834 Acc: 0.9687
Training complete in 301m 35s

Epoch 56/79
----------
train Loss: 0.0446 Acc: 0.9951
val Loss: 0.0749 Acc: 0.9715
Training complete in 308m 56s

Epoch 57/79
----------
train Loss: 0.0448 Acc: 0.9953
val Loss: 0.0768 Acc: 0.9701
Training complete in 318m 4s

Epoch 58/79
----------
train Loss: 0.0476 Acc: 0.9949
val Loss: 0.0751 Acc: 0.9701
Training complete in 325m 35s

Epoch 59/79
----------
train Loss: 0.0430 Acc: 0.9950
val Loss: 0.0782 Acc: 0.9715
Training complete in 333m 6s

Epoch 60/79
----------
train Loss: 0.0457 Acc: 0.9951
val Loss: 0.0750 Acc: 0.9744
Training complete in 340m 37s

Epoch 61/79
----------
train Loss: 0.0436 Acc: 0.9956
val Loss: 0.0755 Acc: 0.9729
Training complete in 348m 8s

Epoch 62/79
----------
train Loss: 0.0430 Acc: 0.9957
val Loss: 0.0791 Acc: 0.9729
Training complete in 355m 37s

Epoch 63/79
----------
train Loss: 0.0421 Acc: 0.9963
val Loss: 0.0817 Acc: 0.9744
Training complete in 363m 9s

Epoch 64/79
----------
train Loss: 0.0418 Acc: 0.9966
val Loss: 0.0775 Acc: 0.9729
Training complete in 370m 40s

Epoch 65/79
----------
train Loss: 0.0429 Acc: 0.9962
val Loss: 0.0795 Acc: 0.9744
Training complete in 378m 10s

Epoch 66/79
----------
train Loss: 0.0431 Acc: 0.9963
val Loss: 0.0770 Acc: 0.9715
Training complete in 385m 43s

Epoch 67/79
----------
train Loss: 0.0430 Acc: 0.9958
val Loss: 0.0756 Acc: 0.9729
Training complete in 393m 14s

Epoch 68/79
----------
train Loss: 0.0432 Acc: 0.9963
val Loss: 0.0825 Acc: 0.9715
Training complete in 400m 44s

Epoch 69/79
----------
train Loss: 0.0437 Acc: 0.9961
val Loss: 0.0846 Acc: 0.9744
Training complete in 408m 17s

Epoch 70/79
----------
train Loss: 0.0430 Acc: 0.9967
val Loss: 0.0799 Acc: 0.9744
Training complete in 415m 49s

Epoch 71/79
----------
train Loss: 0.0421 Acc: 0.9961
val Loss: 0.0804 Acc: 0.9715
Training complete in 423m 19s

Epoch 72/79
----------
train Loss: 0.0414 Acc: 0.9965
val Loss: 0.0917 Acc: 0.9701
Training complete in 430m 53s

Epoch 73/79
----------
train Loss: 0.0399 Acc: 0.9972
val Loss: 0.0837 Acc: 0.9729
Training complete in 438m 24s

Epoch 74/79
----------
train Loss: 0.0423 Acc: 0.9968
val Loss: 0.0801 Acc: 0.9701
Training complete in 445m 54s

Epoch 75/79
----------
train Loss: 0.0403 Acc: 0.9970
val Loss: 0.0822 Acc: 0.9715
Training complete in 453m 26s

Epoch 76/79
----------
train Loss: 0.0413 Acc: 0.9973
val Loss: 0.0859 Acc: 0.9715
Training complete in 460m 57s

Epoch 77/79
----------
train Loss: 0.0433 Acc: 0.9968
val Loss: 0.0809 Acc: 0.9744
Training complete in 468m 28s

Epoch 78/79
----------
train Loss: 0.0420 Acc: 0.9968
val Loss: 0.0775 Acc: 0.9715
Training complete in 475m 59s

Epoch 79/79
----------
train Loss: 0.0407 Acc: 0.9973
val Loss: 0.0821 Acc: 0.9701
Training complete in 483m 32s

Training complete in 483m 32s
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7f58e5fca470>]
2.5560123920440674
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/79
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 0.8657 Acc: 0.1693
val Loss: 2.9226 Acc: 0.4088
Training complete in 4m 34s

Epoch 1/79
----------
train Loss: 0.8288 Acc: 0.5035
val Loss: 1.0943 Acc: 0.7208
Training complete in 9m 3s

Epoch 2/79
----------
train Loss: 0.8455 Acc: 0.6359
val Loss: 0.7144 Acc: 0.8177
Training complete in 13m 39s

Epoch 3/79
----------
train Loss: 0.9228 Acc: 0.6939
val Loss: 0.7024 Acc: 0.8077
Training complete in 18m 12s

Epoch 4/79
----------
train Loss: 0.9694 Acc: 0.7433
val Loss: 0.4810 Acc: 0.8718
Training complete in 22m 33s

Epoch 5/79
----------
train Loss: 0.8695 Acc: 0.7882
val Loss: 0.3440 Acc: 0.9088
Training complete in 26m 31s

Epoch 6/79
----------
train Loss: 0.6374 Acc: 0.8394
val Loss: 0.2672 Acc: 0.9231
Training complete in 30m 51s

Epoch 7/79
----------
train Loss: 0.5118 Acc: 0.8731
val Loss: 0.2700 Acc: 0.9345
Training complete in 35m 23s

Epoch 8/79
----------
train Loss: 0.4298 Acc: 0.8920
val Loss: 0.2526 Acc: 0.9274
Training complete in 39m 57s

Epoch 9/79
----------
train Loss: 0.3645 Acc: 0.9088
val Loss: 0.1915 Acc: 0.9473
Training complete in 44m 25s

Epoch 10/79
----------
train Loss: 0.3166 Acc: 0.9207
val Loss: 0.1454 Acc: 0.9601
Training complete in 48m 22s

Epoch 11/79
----------
train Loss: 0.2777 Acc: 0.9324
val Loss: 0.1870 Acc: 0.9544
Training complete in 52m 27s

Epoch 12/79
----------
train Loss: 0.2278 Acc: 0.9456
val Loss: 0.1126 Acc: 0.9644
Training complete in 56m 55s

Epoch 13/79
----------
train Loss: 0.1993 Acc: 0.9530
val Loss: 0.1302 Acc: 0.9630
Training complete in 61m 12s

Epoch 14/79
----------
train Loss: 0.1764 Acc: 0.9604
val Loss: 0.1148 Acc: 0.9644
Training complete in 65m 51s

Epoch 15/79
----------
train Loss: 0.1589 Acc: 0.9676
val Loss: 0.1107 Acc: 0.9644
Training complete in 70m 25s

Epoch 16/79
----------
train Loss: 0.1464 Acc: 0.9684
val Loss: 0.1365 Acc: 0.9558
Training complete in 74m 53s

Epoch 17/79
----------
train Loss: 0.1345 Acc: 0.9736
val Loss: 0.1188 Acc: 0.9630
Training complete in 79m 24s

Epoch 18/79
----------
train Loss: 0.1313 Acc: 0.9746
val Loss: 0.1126 Acc: 0.9587
Training complete in 83m 51s

Epoch 19/79
----------
train Loss: 0.1104 Acc: 0.9793
val Loss: 0.1136 Acc: 0.9644
Training complete in 87m 47s

Epoch 20/79
----------
train Loss: 0.1043 Acc: 0.9820
val Loss: 0.1006 Acc: 0.9715
Training complete in 92m 22s

Epoch 21/79
----------
train Loss: 0.1023 Acc: 0.9825
val Loss: 0.1074 Acc: 0.9644
Training complete in 96m 50s

Epoch 22/79
----------
train Loss: 0.0976 Acc: 0.9838
val Loss: 0.1000 Acc: 0.9658
Training complete in 101m 23s

Epoch 23/79
----------
train Loss: 0.0872 Acc: 0.9859
val Loss: 0.0998 Acc: 0.9715
Training complete in 105m 32s

Epoch 24/79
----------
train Loss: 0.0856 Acc: 0.9870
val Loss: 0.0957 Acc: 0.9701
Training complete in 110m 2s

Epoch 25/79
----------
train Loss: 0.0779 Acc: 0.9889
val Loss: 0.0781 Acc: 0.9758
Training complete in 114m 27s

Epoch 26/79
----------
train Loss: 0.0761 Acc: 0.9889
val Loss: 0.0771 Acc: 0.9772
Training complete in 118m 37s

Epoch 27/79
----------
train Loss: 0.0757 Acc: 0.9889
val Loss: 0.0909 Acc: 0.9701
Training complete in 122m 51s

Epoch 28/79
----------
train Loss: 0.0626 Acc: 0.9925
val Loss: 0.0781 Acc: 0.9786
Training complete in 127m 34s

Epoch 29/79
----------
train Loss: 0.0671 Acc: 0.9910
val Loss: 0.0858 Acc: 0.9729
Training complete in 132m 9s

Epoch 30/79
----------
train Loss: 0.0643 Acc: 0.9927
val Loss: 0.0819 Acc: 0.9758
Training complete in 136m 39s

Epoch 31/79
----------
train Loss: 0.0570 Acc: 0.9943
val Loss: 0.0720 Acc: 0.9786
Training complete in 140m 46s

Epoch 32/79
----------
train Loss: 0.0565 Acc: 0.9939
val Loss: 0.0941 Acc: 0.9729
Training complete in 144m 34s

Epoch 33/79
----------
train Loss: 0.0599 Acc: 0.9936
val Loss: 0.0960 Acc: 0.9729
Training complete in 149m 5s

Epoch 34/79
----------
train Loss: 0.0540 Acc: 0.9951
val Loss: 0.0835 Acc: 0.9715
Training complete in 153m 35s

Epoch 35/79
----------
train Loss: 0.0564 Acc: 0.9942
val Loss: 0.0790 Acc: 0.9744
Training complete in 157m 45s

Epoch 36/79
----------
train Loss: 0.0478 Acc: 0.9961
val Loss: 0.0784 Acc: 0.9786
Training complete in 162m 8s

Epoch 37/79
----------
train Loss: 0.0502 Acc: 0.9956
val Loss: 0.0754 Acc: 0.9786
Training complete in 166m 34s

Epoch 38/79
----------
train Loss: 0.0518 Acc: 0.9950
val Loss: 0.0754 Acc: 0.9786
Training complete in 170m 44s

Epoch 39/79
----------
train Loss: 0.0480 Acc: 0.9965
val Loss: 0.0796 Acc: 0.9772
Training complete in 175m 17s

Epoch 40/79
----------
train Loss: 0.0491 Acc: 0.9954
val Loss: 0.0688 Acc: 0.9801
Training complete in 179m 43s

Epoch 41/79
----------
train Loss: 0.0494 Acc: 0.9956
val Loss: 0.0709 Acc: 0.9815
Training complete in 184m 14s

Epoch 42/79
----------
train Loss: 0.0464 Acc: 0.9960
val Loss: 0.0673 Acc: 0.9801
Training complete in 188m 17s

Epoch 43/79
----------
train Loss: 0.0469 Acc: 0.9962
val Loss: 0.0790 Acc: 0.9786
Training complete in 192m 54s

Epoch 44/79
----------
train Loss: 0.0429 Acc: 0.9973
val Loss: 0.0806 Acc: 0.9758
Training complete in 196m 53s

Epoch 45/79
----------
train Loss: 0.0432 Acc: 0.9964
val Loss: 0.0724 Acc: 0.9815
Training complete in 200m 52s

Epoch 46/79
----------
train Loss: 0.0407 Acc: 0.9972
val Loss: 0.0706 Acc: 0.9801
Training complete in 205m 25s

Epoch 47/79
----------
train Loss: 0.0442 Acc: 0.9966
val Loss: 0.0769 Acc: 0.9786
Training complete in 210m 1s

Epoch 48/79
----------
train Loss: 0.0462 Acc: 0.9965
val Loss: 0.0722 Acc: 0.9815
Training complete in 214m 28s

Epoch 49/79
----------
train Loss: 0.0422 Acc: 0.9972
val Loss: 0.0795 Acc: 0.9801
Training complete in 218m 56s

Epoch 50/79
----------
train Loss: 0.0415 Acc: 0.9970
val Loss: 0.0774 Acc: 0.9829
Training complete in 223m 7s

Epoch 51/79
----------
train Loss: 0.0393 Acc: 0.9973
val Loss: 0.0737 Acc: 0.9786
Training complete in 227m 31s

Epoch 52/79
----------
train Loss: 0.0393 Acc: 0.9975
val Loss: 0.0762 Acc: 0.9786
Training complete in 231m 50s

Epoch 53/79
----------
train Loss: 0.0382 Acc: 0.9978
val Loss: 0.0817 Acc: 0.9758
Training complete in 236m 24s

Epoch 54/79
----------
train Loss: 0.0407 Acc: 0.9972
val Loss: 0.0854 Acc: 0.9772
Training complete in 240m 56s

Epoch 55/79
----------
train Loss: 0.0385 Acc: 0.9979
val Loss: 0.0811 Acc: 0.9815
Training complete in 245m 30s

Epoch 56/79
----------
train Loss: 0.0378 Acc: 0.9980
val Loss: 0.0849 Acc: 0.9772
Training complete in 249m 26s

Epoch 57/79
----------
train Loss: 0.0376 Acc: 0.9979
val Loss: 0.0829 Acc: 0.9772
Training complete in 253m 21s

Epoch 58/79
----------
train Loss: 0.0388 Acc: 0.9977
val Loss: 0.0767 Acc: 0.9772
Training complete in 257m 17s

Epoch 59/79
----------
train Loss: 0.0378 Acc: 0.9975
val Loss: 0.0871 Acc: 0.9772
Training complete in 261m 50s

Epoch 60/79
----------
train Loss: 0.0374 Acc: 0.9977
val Loss: 0.0831 Acc: 0.9801
Training complete in 266m 29s

Epoch 61/79
----------
train Loss: 0.0382 Acc: 0.9973
val Loss: 0.0780 Acc: 0.9786
Training complete in 270m 43s

Epoch 62/79
----------
train Loss: 0.0382 Acc: 0.9976
val Loss: 0.0757 Acc: 0.9815
Training complete in 275m 5s

Epoch 63/79
----------
train Loss: 0.0374 Acc: 0.9976
val Loss: 0.0788 Acc: 0.9801
Training complete in 279m 23s

Epoch 64/79
----------
train Loss: 0.0361 Acc: 0.9977
val Loss: 0.0840 Acc: 0.9772
Training complete in 283m 19s

Epoch 65/79
----------
train Loss: 0.0352 Acc: 0.9984
val Loss: 0.0744 Acc: 0.9772
Training complete in 287m 42s

Epoch 66/79
----------
train Loss: 0.0356 Acc: 0.9982
val Loss: 0.0801 Acc: 0.9786
Training complete in 292m 15s

Epoch 67/79
----------
train Loss: 0.0355 Acc: 0.9979
val Loss: 0.0804 Acc: 0.9772
Training complete in 296m 44s

Epoch 68/79
----------
train Loss: 0.0344 Acc: 0.9980
val Loss: 0.0696 Acc: 0.9801
Training complete in 301m 11s

Epoch 69/79
----------
train Loss: 0.0347 Acc: 0.9983
val Loss: 0.0784 Acc: 0.9801
Training complete in 305m 12s

Epoch 70/79
----------
train Loss: 0.0333 Acc: 0.9987
val Loss: 0.0786 Acc: 0.9815
Training complete in 309m 42s

Epoch 71/79
----------
train Loss: 0.0337 Acc: 0.9984
val Loss: 0.0814 Acc: 0.9786
Training complete in 314m 18s

Epoch 72/79
----------
train Loss: 0.0344 Acc: 0.9985
val Loss: 0.0823 Acc: 0.9758
Training complete in 318m 54s

Epoch 73/79
----------
train Loss: 0.0329 Acc: 0.9984
val Loss: 0.0731 Acc: 0.9786
Training complete in 323m 10s

Epoch 74/79
----------
train Loss: 0.0327 Acc: 0.9984
val Loss: 0.0797 Acc: 0.9801
Training complete in 327m 21s

Epoch 75/79
----------
train Loss: 0.0340 Acc: 0.9984
val Loss: 0.0786 Acc: 0.9801
Training complete in 331m 20s

Epoch 76/79
----------
train Loss: 0.0343 Acc: 0.9984
val Loss: 0.0788 Acc: 0.9801
Training complete in 336m 0s

Epoch 77/79
----------
train Loss: 0.0338 Acc: 0.9983
val Loss: 0.0766 Acc: 0.9786
Training complete in 340m 33s

Epoch 78/79
----------
train Loss: 0.0348 Acc: 0.9980
val Loss: 0.0783 Acc: 0.9772
Training complete in 345m 9s

Epoch 79/79
----------
train Loss: 0.0335 Acc: 0.9984
val Loss: 0.0805 Acc: 0.9829
Training complete in 349m 44s

Training complete in 349m 44s
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7f06047d1518>]
5.578513145446777
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/79
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 0.8731 Acc: 0.1712
val Loss: 2.6284 Acc: 0.4416
Training complete in 7m 2s

Epoch 1/79
----------
train Loss: 0.7756 Acc: 0.5394
val Loss: 1.1049 Acc: 0.7222
Training complete in 13m 57s

Epoch 2/79
----------
train Loss: 0.8014 Acc: 0.6559
val Loss: 0.7683 Acc: 0.7863
Training complete in 20m 54s

Epoch 3/79
----------
train Loss: 0.8980 Acc: 0.7085
val Loss: 0.5943 Acc: 0.8462
Training complete in 27m 50s

Epoch 4/79
----------
train Loss: 0.9438 Acc: 0.7486
val Loss: 0.4827 Acc: 0.8632
Training complete in 34m 45s

Epoch 5/79
----------
train Loss: 0.8516 Acc: 0.7938
val Loss: 0.4532 Acc: 0.8818
Training complete in 41m 42s

Epoch 6/79
----------
train Loss: 0.6210 Acc: 0.8454
val Loss: 0.3061 Acc: 0.9302
Training complete in 48m 38s

Epoch 7/79
----------
train Loss: 0.4982 Acc: 0.8750
val Loss: 0.2579 Acc: 0.9316
Training complete in 55m 33s

Epoch 8/79
----------
train Loss: 0.4154 Acc: 0.8987
val Loss: 0.1943 Acc: 0.9430
Training complete in 62m 32s

Epoch 9/79
----------
train Loss: 0.3416 Acc: 0.9159
val Loss: 0.1984 Acc: 0.9444
Training complete in 69m 31s

Epoch 10/79
----------
train Loss: 0.3136 Acc: 0.9211
val Loss: 0.2010 Acc: 0.9373
Training complete in 76m 26s

Epoch 11/79
----------
train Loss: 0.2539 Acc: 0.9383
val Loss: 0.1395 Acc: 0.9516
Training complete in 83m 22s

Epoch 12/79
----------
train Loss: 0.2202 Acc: 0.9473
val Loss: 0.1234 Acc: 0.9615
Training complete in 90m 22s

Epoch 13/79
----------
train Loss: 0.1932 Acc: 0.9559
val Loss: 0.1478 Acc: 0.9587
Training complete in 97m 17s

Epoch 14/79
----------
train Loss: 0.1824 Acc: 0.9585
val Loss: 0.1437 Acc: 0.9544
Training complete in 104m 16s

Epoch 15/79
----------
train Loss: 0.1673 Acc: 0.9631
val Loss: 0.1568 Acc: 0.9644
Training complete in 111m 15s

Epoch 16/79
----------
train Loss: 0.1502 Acc: 0.9673
val Loss: 0.1115 Acc: 0.9701
Training complete in 118m 9s

Epoch 17/79
----------
train Loss: 0.1309 Acc: 0.9749
val Loss: 0.1065 Acc: 0.9744
Training complete in 125m 8s

Epoch 18/79
----------
train Loss: 0.1221 Acc: 0.9760
val Loss: 0.0890 Acc: 0.9744
Training complete in 132m 6s

Epoch 19/79
----------
train Loss: 0.1164 Acc: 0.9776
val Loss: 0.1100 Acc: 0.9687
Training complete in 139m 3s

Epoch 20/79
----------
train Loss: 0.1033 Acc: 0.9812
val Loss: 0.1085 Acc: 0.9672
Training complete in 146m 1s

Epoch 21/79
----------
train Loss: 0.1057 Acc: 0.9817
val Loss: 0.0976 Acc: 0.9744
Training complete in 152m 58s

Epoch 22/79
----------
train Loss: 0.0944 Acc: 0.9839
val Loss: 0.1009 Acc: 0.9729
Training complete in 159m 58s

Epoch 23/79
----------
train Loss: 0.0834 Acc: 0.9869
val Loss: 0.0897 Acc: 0.9772
Training complete in 166m 56s

Epoch 24/79
----------
train Loss: 0.0792 Acc: 0.9882
val Loss: 0.0793 Acc: 0.9758
Training complete in 173m 53s

Epoch 25/79
----------
train Loss: 0.0739 Acc: 0.9891
val Loss: 0.0863 Acc: 0.9758
Training complete in 180m 51s

Epoch 26/79
----------
train Loss: 0.0693 Acc: 0.9906
val Loss: 0.0896 Acc: 0.9715
Training complete in 187m 51s

Epoch 27/79
----------
train Loss: 0.0716 Acc: 0.9899
val Loss: 0.1017 Acc: 0.9729
Training complete in 194m 46s

Epoch 28/79
----------
train Loss: 0.0655 Acc: 0.9920
val Loss: 0.0895 Acc: 0.9772
Training complete in 201m 38s

Epoch 29/79
----------
train Loss: 0.0592 Acc: 0.9932
val Loss: 0.0955 Acc: 0.9729
Training complete in 206m 18s

Epoch 30/79
----------
train Loss: 0.0579 Acc: 0.9937
val Loss: 0.0928 Acc: 0.9772
Training complete in 210m 56s

Epoch 31/79
----------
train Loss: 0.0554 Acc: 0.9932
val Loss: 0.0976 Acc: 0.9744
Training complete in 215m 34s

Epoch 32/79
----------
train Loss: 0.0565 Acc: 0.9942
val Loss: 0.0990 Acc: 0.9772
Training complete in 220m 12s

Epoch 33/79
----------
train Loss: 0.0537 Acc: 0.9947
val Loss: 0.0916 Acc: 0.9772
Training complete in 224m 51s

Epoch 34/79
----------
train Loss: 0.0565 Acc: 0.9943
val Loss: 0.0949 Acc: 0.9772
Training complete in 229m 29s

Epoch 35/79
----------
train Loss: 0.0552 Acc: 0.9946
val Loss: 0.0920 Acc: 0.9772
Training complete in 234m 7s

Epoch 36/79
----------
train Loss: 0.0571 Acc: 0.9939
val Loss: 0.0919 Acc: 0.9772
Training complete in 238m 45s

Epoch 37/79
----------
train Loss: 0.0512 Acc: 0.9946
val Loss: 0.0795 Acc: 0.9772
Training complete in 243m 23s

Epoch 38/79
----------
train Loss: 0.0478 Acc: 0.9954
val Loss: 0.0796 Acc: 0.9786
Training complete in 248m 2s

Epoch 39/79
----------
train Loss: 0.0478 Acc: 0.9957
val Loss: 0.0933 Acc: 0.9758
Training complete in 252m 42s

Epoch 40/79
----------
train Loss: 0.0474 Acc: 0.9963
val Loss: 0.0906 Acc: 0.9715
Training complete in 257m 19s

Epoch 41/79
----------
train Loss: 0.0451 Acc: 0.9958
val Loss: 0.0850 Acc: 0.9758
Training complete in 261m 58s

Epoch 42/79
----------
train Loss: 0.0494 Acc: 0.9956
val Loss: 0.0914 Acc: 0.9758
Training complete in 266m 37s

Epoch 43/79
----------
train Loss: 0.0476 Acc: 0.9957
val Loss: 0.0949 Acc: 0.9758
Training complete in 271m 14s

Epoch 44/79
----------
train Loss: 0.0430 Acc: 0.9967
val Loss: 0.0946 Acc: 0.9772
Training complete in 275m 51s

Epoch 45/79
----------
train Loss: 0.0424 Acc: 0.9970
val Loss: 0.0919 Acc: 0.9801
Training complete in 280m 30s

Epoch 46/79
----------
train Loss: 0.0410 Acc: 0.9966
val Loss: 0.0931 Acc: 0.9758
Training complete in 285m 8s

Epoch 47/79
----------
train Loss: 0.0410 Acc: 0.9973
val Loss: 0.0885 Acc: 0.9758
Training complete in 289m 47s

Epoch 48/79
----------
train Loss: 0.0416 Acc: 0.9966
val Loss: 0.0976 Acc: 0.9758
Training complete in 294m 24s

Epoch 49/79
----------
train Loss: 0.0390 Acc: 0.9973
val Loss: 0.0853 Acc: 0.9758
Training complete in 299m 3s

Epoch 50/79
----------
train Loss: 0.0402 Acc: 0.9971
val Loss: 0.0912 Acc: 0.9729
Training complete in 303m 41s

Epoch 51/79
----------
train Loss: 0.0402 Acc: 0.9975
val Loss: 0.0823 Acc: 0.9786
Training complete in 308m 20s

Epoch 52/79
----------
train Loss: 0.0394 Acc: 0.9972
val Loss: 0.0829 Acc: 0.9758
Training complete in 312m 57s

Epoch 53/79
----------
train Loss: 0.0401 Acc: 0.9970
val Loss: 0.0834 Acc: 0.9758
Training complete in 317m 35s

Epoch 54/79
----------
train Loss: 0.0373 Acc: 0.9980
val Loss: 0.0849 Acc: 0.9758
Training complete in 322m 14s

Epoch 55/79
----------
train Loss: 0.0376 Acc: 0.9977
val Loss: 0.0838 Acc: 0.9772
Training complete in 326m 52s

Epoch 56/79
----------
train Loss: 0.0362 Acc: 0.9981
val Loss: 0.0973 Acc: 0.9758
Training complete in 331m 29s

Epoch 57/79
----------
train Loss: 0.0384 Acc: 0.9973
val Loss: 0.0770 Acc: 0.9786
Training complete in 336m 8s

Epoch 58/79
----------
train Loss: 0.0383 Acc: 0.9976
val Loss: 0.0886 Acc: 0.9786
Training complete in 340m 47s

Epoch 59/79
----------
train Loss: 0.0384 Acc: 0.9973
val Loss: 0.1002 Acc: 0.9758
Training complete in 344m 18s

Epoch 60/79
----------
train Loss: 0.0359 Acc: 0.9982
val Loss: 0.0945 Acc: 0.9758
Training complete in 347m 3s

Epoch 61/79
----------
train Loss: 0.0343 Acc: 0.9980
val Loss: 0.0837 Acc: 0.9772
Training complete in 350m 14s

Epoch 62/79
----------
train Loss: 0.0324 Acc: 0.9983
val Loss: 0.0853 Acc: 0.9786
Training complete in 352m 59s

Epoch 63/79
----------
train Loss: 0.0331 Acc: 0.9986
val Loss: 0.0823 Acc: 0.9786
Training complete in 355m 42s

Epoch 64/79
----------
train Loss: 0.0342 Acc: 0.9982
val Loss: 0.0924 Acc: 0.9758
Training complete in 358m 26s

Epoch 65/79
----------
train Loss: 0.0318 Acc: 0.9989
val Loss: 0.0918 Acc: 0.9758
Training complete in 361m 38s

Epoch 66/79
----------
train Loss: 0.0309 Acc: 0.9989
val Loss: 0.0888 Acc: 0.9772
Training complete in 364m 50s

Epoch 67/79
----------
train Loss: 0.0346 Acc: 0.9980
val Loss: 0.0874 Acc: 0.9758
Training complete in 367m 34s

Epoch 68/79
----------
train Loss: 0.0344 Acc: 0.9982
val Loss: 0.0996 Acc: 0.9772
Training complete in 370m 18s

Epoch 69/79
----------
train Loss: 0.0356 Acc: 0.9983
val Loss: 0.0895 Acc: 0.9801
Training complete in 373m 33s

Epoch 70/79
----------
train Loss: 0.0342 Acc: 0.9981
val Loss: 0.0966 Acc: 0.9772
Training complete in 376m 38s

Epoch 71/79
----------
train Loss: 0.0356 Acc: 0.9979
val Loss: 0.1002 Acc: 0.9758
Training complete in 379m 23s

Epoch 72/79
----------
train Loss: 0.0339 Acc: 0.9980
val Loss: 0.0933 Acc: 0.9758
Training complete in 382m 31s

Epoch 73/79
----------
train Loss: 0.0327 Acc: 0.9986
val Loss: 0.0996 Acc: 0.9758
Training complete in 385m 37s

Epoch 74/79
----------
train Loss: 0.0320 Acc: 0.9985
val Loss: 0.0989 Acc: 0.9729
Training complete in 388m 37s

Epoch 75/79
----------
train Loss: 0.0321 Acc: 0.9984
val Loss: 0.0948 Acc: 0.9744
Training complete in 391m 22s

Epoch 76/79
----------
train Loss: 0.0336 Acc: 0.9984
val Loss: 0.0972 Acc: 0.9744
Training complete in 394m 34s

Epoch 77/79
----------
train Loss: 0.0313 Acc: 0.9985
val Loss: 0.1079 Acc: 0.9744
Training complete in 397m 17s

Epoch 78/79
----------
train Loss: 0.0311 Acc: 0.9984
val Loss: 0.0996 Acc: 0.9772
Training complete in 400m 25s

Epoch 79/79
----------
train Loss: 0.0314 Acc: 0.9988
val Loss: 0.0889 Acc: 0.9772
Training complete in 403m 9s

Training complete in 403m 9s
Traceback (most recent call last):
  File "train.py", line 27, in <module>
    from has import HideAndSeed
ModuleNotFoundError: No module named 'has'
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7fe6a6e2e908>]
3.0519134998321533
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/79
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 0.8933 Acc: 0.1532
val Loss: 3.0388 Acc: 0.3632
Training complete in 8m 52s

Epoch 1/79
----------
train Loss: 0.9120 Acc: 0.4601
val Loss: 1.2024 Acc: 0.6952
Training complete in 17m 41s

Epoch 2/79
----------
train Loss: 0.9302 Acc: 0.6035
val Loss: 0.8462 Acc: 0.7835
Training complete in 26m 27s

Epoch 3/79
----------
train Loss: 1.0277 Acc: 0.6587
val Loss: 0.6945 Acc: 0.8162
Training complete in 35m 15s

Epoch 4/79
----------
train Loss: 1.1066 Acc: 0.7047
val Loss: 0.6047 Acc: 0.8348
Training complete in 44m 2s

Epoch 5/79
----------
train Loss: 1.0358 Acc: 0.7506
val Loss: 0.4197 Acc: 0.8875
Training complete in 52m 47s

Epoch 6/79
----------
train Loss: 0.7729 Acc: 0.8056
val Loss: 0.3083 Acc: 0.9117
Training complete in 61m 35s

Epoch 7/79
----------
train Loss: 0.6371 Acc: 0.8403
val Loss: 0.3036 Acc: 0.9117
Training complete in 70m 24s

Epoch 8/79
----------
train Loss: 0.5303 Acc: 0.8669
val Loss: 0.2232 Acc: 0.9373
Training complete in 79m 11s

Epoch 9/79
----------
train Loss: 0.4703 Acc: 0.8792
val Loss: 0.2433 Acc: 0.9288
Training complete in 87m 56s

Epoch 10/79
----------
train Loss: 0.4022 Acc: 0.8980
val Loss: 0.1978 Acc: 0.9373
Training complete in 96m 45s

Epoch 11/79
----------
train Loss: 0.3711 Acc: 0.9052
val Loss: 0.1744 Acc: 0.9516
Training complete in 105m 34s

Epoch 12/79
----------
train Loss: 0.3203 Acc: 0.9205
val Loss: 0.1349 Acc: 0.9544
Training complete in 114m 20s

Epoch 13/79
----------
train Loss: 0.2984 Acc: 0.9273
val Loss: 0.1505 Acc: 0.9530
Training complete in 123m 8s

Epoch 14/79
----------
train Loss: 0.2560 Acc: 0.9365
val Loss: 0.1257 Acc: 0.9587
Training complete in 131m 55s

Epoch 15/79
----------
train Loss: 0.2301 Acc: 0.9447
val Loss: 0.1302 Acc: 0.9630
Training complete in 140m 43s

Epoch 16/79
----------
train Loss: 0.2045 Acc: 0.9539
val Loss: 0.1318 Acc: 0.9672
Training complete in 149m 31s

Epoch 17/79
----------
train Loss: 0.1960 Acc: 0.9556
val Loss: 0.1184 Acc: 0.9658
Training complete in 158m 18s

Epoch 18/79
----------
train Loss: 0.1692 Acc: 0.9625
val Loss: 0.0982 Acc: 0.9701
Training complete in 167m 5s

Epoch 19/79
----------
train Loss: 0.1578 Acc: 0.9652
val Loss: 0.1129 Acc: 0.9729
Training complete in 175m 54s

Epoch 20/79
----------
train Loss: 0.1416 Acc: 0.9705
val Loss: 0.0964 Acc: 0.9701
Training complete in 184m 41s

Epoch 21/79
----------
train Loss: 0.1336 Acc: 0.9739
val Loss: 0.1096 Acc: 0.9687
Training complete in 193m 28s

Epoch 22/79
----------
train Loss: 0.1193 Acc: 0.9764
val Loss: 0.1024 Acc: 0.9672
Training complete in 202m 18s

Epoch 23/79
----------
train Loss: 0.1134 Acc: 0.9789
val Loss: 0.1232 Acc: 0.9658
Training complete in 211m 7s

Epoch 24/79
----------
train Loss: 0.0991 Acc: 0.9824
val Loss: 0.0955 Acc: 0.9701
Training complete in 219m 54s

Epoch 25/79
----------
train Loss: 0.0924 Acc: 0.9846
val Loss: 0.0954 Acc: 0.9687
Training complete in 228m 44s

Epoch 26/79
----------
train Loss: 0.0895 Acc: 0.9865
val Loss: 0.0901 Acc: 0.9729
Training complete in 234m 31s

Epoch 27/79
----------
train Loss: 0.0915 Acc: 0.9846
val Loss: 0.0878 Acc: 0.9744
Training complete in 239m 20s

Epoch 28/79
----------
train Loss: 0.0787 Acc: 0.9886
val Loss: 0.0756 Acc: 0.9786
Training complete in 244m 23s

Epoch 29/79
----------
train Loss: 0.0787 Acc: 0.9884
val Loss: 0.0804 Acc: 0.9729
Training complete in 249m 25s

Epoch 30/79
----------
train Loss: 0.0771 Acc: 0.9894
val Loss: 0.0742 Acc: 0.9715
Training complete in 254m 1s

Epoch 31/79
----------
train Loss: 0.0685 Acc: 0.9906
val Loss: 0.0735 Acc: 0.9772
Training complete in 259m 1s

Epoch 32/79
----------
train Loss: 0.0636 Acc: 0.9918
val Loss: 0.0754 Acc: 0.9758
Training complete in 264m 10s

Epoch 33/79
----------
train Loss: 0.0682 Acc: 0.9927
val Loss: 0.0768 Acc: 0.9715
Training complete in 269m 7s

Epoch 34/79
----------
train Loss: 0.0661 Acc: 0.9918
val Loss: 0.0774 Acc: 0.9715
Training complete in 274m 8s

Epoch 35/79
----------
train Loss: 0.0604 Acc: 0.9936
val Loss: 0.0808 Acc: 0.9729
Training complete in 279m 19s

Epoch 36/79
----------
train Loss: 0.0618 Acc: 0.9929
val Loss: 0.0823 Acc: 0.9772
Training complete in 284m 4s

Epoch 37/79
----------
train Loss: 0.0558 Acc: 0.9951
val Loss: 0.0889 Acc: 0.9772
Training complete in 289m 11s

Epoch 38/79
----------
train Loss: 0.0574 Acc: 0.9937
val Loss: 0.0854 Acc: 0.9801
Training complete in 294m 1s

Epoch 39/79
----------
train Loss: 0.0551 Acc: 0.9949
val Loss: 0.0797 Acc: 0.9801
Training complete in 298m 55s

Epoch 40/79
----------
train Loss: 0.0550 Acc: 0.9944
val Loss: 0.0770 Acc: 0.9786
Training complete in 304m 1s

Epoch 41/79
----------
train Loss: 0.0490 Acc: 0.9963
val Loss: 0.0808 Acc: 0.9772
Training complete in 309m 11s

Epoch 42/79
----------
train Loss: 0.0497 Acc: 0.9965
val Loss: 0.0788 Acc: 0.9758
Training complete in 314m 22s

Epoch 43/79
----------
train Loss: 0.0501 Acc: 0.9958
val Loss: 0.0811 Acc: 0.9786
Training complete in 319m 19s

Epoch 44/79
----------
train Loss: 0.0470 Acc: 0.9964
val Loss: 0.0753 Acc: 0.9801
Training complete in 324m 1s

Epoch 45/79
----------
train Loss: 0.0487 Acc: 0.9960
val Loss: 0.0762 Acc: 0.9801
Training complete in 329m 11s

Epoch 46/79
----------
train Loss: 0.0461 Acc: 0.9968
val Loss: 0.0699 Acc: 0.9815
Training complete in 334m 3s

Epoch 47/79
----------
train Loss: 0.0450 Acc: 0.9967
val Loss: 0.0673 Acc: 0.9801
Training complete in 338m 51s

Epoch 48/79
----------
train Loss: 0.0443 Acc: 0.9965
val Loss: 0.0701 Acc: 0.9815
Training complete in 344m 2s

Epoch 49/79
----------
train Loss: 0.0462 Acc: 0.9965
val Loss: 0.0673 Acc: 0.9801
Training complete in 348m 51s

Epoch 50/79
----------
train Loss: 0.0469 Acc: 0.9961
val Loss: 0.0723 Acc: 0.9801
Training complete in 353m 60s

Epoch 51/79
----------
train Loss: 0.0442 Acc: 0.9969
val Loss: 0.0738 Acc: 0.9801
Training complete in 359m 4s

Epoch 52/79
----------
train Loss: 0.0465 Acc: 0.9966
val Loss: 0.0846 Acc: 0.9772
Training complete in 363m 53s

Epoch 53/79
----------
train Loss: 0.0421 Acc: 0.9975
val Loss: 0.0859 Acc: 0.9744
Training complete in 368m 59s

Epoch 54/79
----------
train Loss: 0.0433 Acc: 0.9970
val Loss: 0.0906 Acc: 0.9758
Training complete in 374m 11s

Epoch 55/79
----------
train Loss: 0.0405 Acc: 0.9979
val Loss: 0.0815 Acc: 0.9786
Training complete in 379m 19s

Epoch 56/79
----------
train Loss: 0.0422 Acc: 0.9972
val Loss: 0.0741 Acc: 0.9786
Training complete in 384m 32s

Epoch 57/79
----------
train Loss: 0.0433 Acc: 0.9972
val Loss: 0.0856 Acc: 0.9786
Training complete in 389m 40s

Epoch 58/79
----------
train Loss: 0.0393 Acc: 0.9978
val Loss: 0.0796 Acc: 0.9772
Training complete in 394m 36s

Epoch 59/79
----------
train Loss: 0.0408 Acc: 0.9975
val Loss: 0.0815 Acc: 0.9772
Training complete in 399m 10s

Epoch 60/79
----------
train Loss: 0.0418 Acc: 0.9972
val Loss: 0.0790 Acc: 0.9786
Training complete in 404m 19s

Epoch 61/79
----------
train Loss: 0.0389 Acc: 0.9981
val Loss: 0.0778 Acc: 0.9815
Training complete in 409m 15s

Epoch 62/79
----------
train Loss: 0.0395 Acc: 0.9978
val Loss: 0.0866 Acc: 0.9801
Training complete in 414m 21s

Epoch 63/79
----------
train Loss: 0.0389 Acc: 0.9979
val Loss: 0.0780 Acc: 0.9786
Training complete in 418m 56s

Epoch 64/79
----------
train Loss: 0.0388 Acc: 0.9977
val Loss: 0.0759 Acc: 0.9801
Training complete in 423m 30s

Epoch 65/79
----------
train Loss: 0.0374 Acc: 0.9982
val Loss: 0.0786 Acc: 0.9801
Training complete in 428m 6s

Epoch 66/79
----------
train Loss: 0.0370 Acc: 0.9982
val Loss: 0.0735 Acc: 0.9772
Training complete in 433m 0s

Epoch 67/79
----------
train Loss: 0.0344 Acc: 0.9989
val Loss: 0.0838 Acc: 0.9786
Training complete in 437m 34s

Epoch 68/79
----------
train Loss: 0.0381 Acc: 0.9981
val Loss: 0.0794 Acc: 0.9758
Training complete in 442m 30s

Epoch 69/79
----------
train Loss: 0.0366 Acc: 0.9985
val Loss: 0.0769 Acc: 0.9758
Training complete in 447m 5s

Epoch 70/79
----------
train Loss: 0.0374 Acc: 0.9985
val Loss: 0.0765 Acc: 0.9786
Training complete in 451m 36s

Epoch 71/79
----------
train Loss: 0.0337 Acc: 0.9986
val Loss: 0.0677 Acc: 0.9772
Training complete in 456m 39s

Epoch 72/79
----------
train Loss: 0.0372 Acc: 0.9981
val Loss: 0.0748 Acc: 0.9829
Training complete in 461m 14s

Epoch 73/79
----------
train Loss: 0.0347 Acc: 0.9984
val Loss: 0.0801 Acc: 0.9786
Training complete in 466m 6s

Epoch 74/79
----------
train Loss: 0.0340 Acc: 0.9985
val Loss: 0.0772 Acc: 0.9786
Training complete in 471m 15s

Epoch 75/79
----------
train Loss: 0.0339 Acc: 0.9987
val Loss: 0.0780 Acc: 0.9786
Training complete in 476m 12s

Epoch 76/79
----------
train Loss: 0.0355 Acc: 0.9979
val Loss: 0.0691 Acc: 0.9758
Training complete in 481m 6s

Epoch 77/79
----------
train Loss: 0.0341 Acc: 0.9989
val Loss: 0.0750 Acc: 0.9801
Training complete in 485m 44s

Epoch 78/79
----------
train Loss: 0.0345 Acc: 0.9985
val Loss: 0.0824 Acc: 0.9801
Training complete in 490m 54s

Epoch 79/79
----------
train Loss: 0.0353 Acc: 0.9983
val Loss: 0.0740 Acc: 0.9758
Training complete in 495m 25s

Training complete in 495m 25s
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7f829a6c58d0>]
3.0518689155578613
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/79
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 0.8944 Acc: 0.1571
val Loss: 3.0314 Acc: 0.3575
Training complete in 9m 55s

Epoch 1/79
----------
train Loss: 0.8988 Acc: 0.4674
val Loss: 1.3658 Acc: 0.6467
Training complete in 18m 45s

Epoch 2/79
----------
train Loss: 0.9195 Acc: 0.6031
val Loss: 0.8495 Acc: 0.7650
Training complete in 27m 33s

Epoch 3/79
----------
train Loss: 1.0285 Acc: 0.6631
val Loss: 0.6883 Acc: 0.8219
Training complete in 36m 22s

Epoch 4/79
----------
train Loss: 1.0853 Acc: 0.7116
val Loss: 0.4962 Acc: 0.8561
Training complete in 45m 12s

Epoch 5/79
----------
train Loss: 1.0033 Acc: 0.7546
val Loss: 0.4498 Acc: 0.8775
Training complete in 54m 2s

Epoch 6/79
----------
train Loss: 0.7835 Acc: 0.8052
val Loss: 0.3281 Acc: 0.9131
Training complete in 62m 51s

Epoch 7/79
----------
train Loss: 0.6323 Acc: 0.8397
val Loss: 0.2511 Acc: 0.9231
Training complete in 71m 41s

Epoch 8/79
----------
train Loss: 0.5370 Acc: 0.8640
val Loss: 0.2771 Acc: 0.9302
Training complete in 80m 31s

Epoch 9/79
----------
train Loss: 0.4586 Acc: 0.8833
val Loss: 0.2180 Acc: 0.9444
Training complete in 89m 20s

Epoch 10/79
----------
train Loss: 0.4105 Acc: 0.8980
val Loss: 0.1955 Acc: 0.9402
Training complete in 98m 9s

Epoch 11/79
----------
train Loss: 0.3554 Acc: 0.9080
val Loss: 0.1976 Acc: 0.9444
Training complete in 106m 59s

Epoch 12/79
----------
train Loss: 0.3138 Acc: 0.9195
val Loss: 0.1602 Acc: 0.9516
Training complete in 115m 47s

Epoch 13/79
----------
train Loss: 0.2713 Acc: 0.9345
val Loss: 0.1384 Acc: 0.9587
Training complete in 124m 36s

Epoch 14/79
----------
train Loss: 0.2603 Acc: 0.9347
val Loss: 0.1404 Acc: 0.9558
Training complete in 133m 25s

Epoch 15/79
----------
train Loss: 0.2423 Acc: 0.9419
val Loss: 0.1585 Acc: 0.9530
Training complete in 142m 13s

Epoch 16/79
----------
train Loss: 0.1955 Acc: 0.9540
val Loss: 0.1284 Acc: 0.9544
Training complete in 151m 2s

Epoch 17/79
----------
train Loss: 0.1847 Acc: 0.9573
val Loss: 0.1258 Acc: 0.9658
Training complete in 156m 45s

Epoch 18/79
----------
train Loss: 0.1745 Acc: 0.9604
val Loss: 0.1205 Acc: 0.9558
Training complete in 161m 29s

Epoch 19/79
----------
train Loss: 0.1468 Acc: 0.9680
val Loss: 0.1108 Acc: 0.9601
Training complete in 166m 27s

Epoch 20/79
----------
train Loss: 0.1426 Acc: 0.9692
val Loss: 0.1002 Acc: 0.9644
Training complete in 171m 24s

Epoch 21/79
----------
train Loss: 0.1227 Acc: 0.9757
val Loss: 0.1121 Acc: 0.9601
Training complete in 175m 54s

Epoch 22/79
----------
train Loss: 0.1196 Acc: 0.9762
val Loss: 0.0978 Acc: 0.9701
Training complete in 180m 25s

Epoch 23/79
----------
train Loss: 0.1089 Acc: 0.9799
val Loss: 0.0902 Acc: 0.9744
Training complete in 185m 2s

Epoch 24/79
----------
train Loss: 0.0970 Acc: 0.9832
val Loss: 0.0887 Acc: 0.9672
Training complete in 189m 30s

Epoch 25/79
----------
train Loss: 0.0912 Acc: 0.9844
val Loss: 0.1002 Acc: 0.9715
Training complete in 194m 5s

Epoch 26/79
----------
train Loss: 0.0866 Acc: 0.9870
val Loss: 0.0723 Acc: 0.9744
Training complete in 198m 57s

Epoch 27/79
----------
train Loss: 0.0843 Acc: 0.9872
val Loss: 0.0900 Acc: 0.9715
Training complete in 203m 55s

Epoch 28/79
----------
train Loss: 0.0859 Acc: 0.9879
val Loss: 0.1074 Acc: 0.9715
Training complete in 208m 45s

Epoch 29/79
----------
train Loss: 0.0767 Acc: 0.9883
val Loss: 0.0876 Acc: 0.9687
Training complete in 213m 16s

Epoch 30/79
----------
train Loss: 0.0758 Acc: 0.9891
val Loss: 0.0806 Acc: 0.9758
Training complete in 218m 10s

Epoch 31/79
----------
train Loss: 0.0724 Acc: 0.9901
val Loss: 0.0817 Acc: 0.9729
Training complete in 223m 4s

Epoch 32/79
----------
train Loss: 0.0697 Acc: 0.9915
val Loss: 0.0835 Acc: 0.9729
Training complete in 228m 5s

Epoch 33/79
----------
train Loss: 0.0649 Acc: 0.9922
val Loss: 0.0848 Acc: 0.9729
Training complete in 233m 11s

Epoch 34/79
----------
train Loss: 0.0614 Acc: 0.9935
val Loss: 0.0927 Acc: 0.9687
Training complete in 237m 46s

Epoch 35/79
----------
train Loss: 0.0662 Acc: 0.9920
val Loss: 0.0833 Acc: 0.9729
Training complete in 242m 39s

Epoch 36/79
----------
train Loss: 0.0566 Acc: 0.9943
val Loss: 0.0834 Acc: 0.9729
Training complete in 247m 36s

Epoch 37/79
----------
train Loss: 0.0562 Acc: 0.9949
val Loss: 0.0878 Acc: 0.9729
Training complete in 252m 38s

Epoch 38/79
----------
train Loss: 0.0585 Acc: 0.9935
val Loss: 0.0901 Acc: 0.9729
Training complete in 257m 29s

Epoch 39/79
----------
train Loss: 0.0610 Acc: 0.9936
val Loss: 0.0904 Acc: 0.9687
Training complete in 262m 14s

Epoch 40/79
----------
train Loss: 0.0496 Acc: 0.9964
val Loss: 0.0850 Acc: 0.9744
Training complete in 266m 43s

Epoch 41/79
----------
train Loss: 0.0550 Acc: 0.9950
val Loss: 0.0866 Acc: 0.9744
Training complete in 271m 17s

Epoch 42/79
----------
train Loss: 0.0476 Acc: 0.9967
val Loss: 0.0855 Acc: 0.9744
Training complete in 276m 13s

Epoch 43/79
----------
train Loss: 0.0482 Acc: 0.9959
val Loss: 0.0852 Acc: 0.9715
Training complete in 281m 14s

Epoch 44/79
----------
train Loss: 0.0477 Acc: 0.9963
val Loss: 0.0773 Acc: 0.9744
Training complete in 286m 16s

Epoch 45/79
----------
train Loss: 0.0438 Acc: 0.9969
val Loss: 0.0780 Acc: 0.9744
Training complete in 291m 14s

Epoch 46/79
----------
train Loss: 0.0475 Acc: 0.9956
val Loss: 0.0833 Acc: 0.9744
Training complete in 296m 19s

Epoch 47/79
----------
train Loss: 0.0475 Acc: 0.9960
val Loss: 0.0800 Acc: 0.9744
Training complete in 301m 30s

Epoch 48/79
----------
train Loss: 0.0471 Acc: 0.9965
val Loss: 0.0844 Acc: 0.9744
Training complete in 306m 32s

Epoch 49/79
----------
train Loss: 0.0447 Acc: 0.9968
val Loss: 0.0785 Acc: 0.9772
Training complete in 311m 36s

Epoch 50/79
----------
train Loss: 0.0445 Acc: 0.9972
val Loss: 0.0794 Acc: 0.9758
Training complete in 317m 13s

Epoch 51/79
----------
train Loss: 0.0432 Acc: 0.9975
val Loss: 0.0777 Acc: 0.9772
Training complete in 322m 55s

Epoch 52/79
----------
train Loss: 0.0402 Acc: 0.9973
val Loss: 0.0738 Acc: 0.9772
Training complete in 329m 37s

Epoch 53/79
----------
train Loss: 0.0418 Acc: 0.9976
val Loss: 0.0768 Acc: 0.9772
Training complete in 335m 42s

Epoch 54/79
----------
train Loss: 0.0452 Acc: 0.9966
val Loss: 0.0824 Acc: 0.9772
Training complete in 343m 58s

Epoch 55/79
----------
train Loss: 0.0411 Acc: 0.9978
val Loss: 0.0791 Acc: 0.9801
Training complete in 352m 48s

Epoch 56/79
----------
train Loss: 0.0403 Acc: 0.9978
val Loss: 0.0724 Acc: 0.9772
Training complete in 361m 38s

Epoch 57/79
----------
train Loss: 0.0443 Acc: 0.9966
val Loss: 0.0769 Acc: 0.9786
Training complete in 370m 29s

Epoch 58/79
----------
train Loss: 0.0402 Acc: 0.9981
val Loss: 0.0832 Acc: 0.9786
Training complete in 379m 17s

Epoch 59/79
----------
train Loss: 0.0381 Acc: 0.9986
val Loss: 0.0778 Acc: 0.9786
Training complete in 388m 9s

Epoch 60/79
----------
train Loss: 0.0413 Acc: 0.9970
val Loss: 0.0833 Acc: 0.9758
Training complete in 396m 58s

Epoch 61/79
----------
train Loss: 0.0372 Acc: 0.9986
val Loss: 0.0797 Acc: 0.9758
Training complete in 405m 48s

Epoch 62/79
----------
train Loss: 0.0365 Acc: 0.9984
val Loss: 0.0843 Acc: 0.9758
Training complete in 414m 39s

Epoch 63/79
----------
train Loss: 0.0388 Acc: 0.9980
val Loss: 0.0775 Acc: 0.9801
Training complete in 423m 29s

Epoch 64/79
----------
train Loss: 0.0370 Acc: 0.9982
val Loss: 0.0748 Acc: 0.9744
Training complete in 432m 20s

Epoch 65/79
----------
train Loss: 0.0409 Acc: 0.9976
val Loss: 0.0681 Acc: 0.9786
Training complete in 441m 10s

Epoch 66/79
----------
train Loss: 0.0398 Acc: 0.9978
val Loss: 0.0728 Acc: 0.9786
Training complete in 450m 0s

Epoch 67/79
----------
train Loss: 0.0364 Acc: 0.9989
val Loss: 0.0783 Acc: 0.9786
Training complete in 458m 50s

Epoch 68/79
----------
train Loss: 0.0372 Acc: 0.9981
val Loss: 0.0748 Acc: 0.9786
Training complete in 467m 41s

Epoch 69/79
----------
train Loss: 0.0367 Acc: 0.9982
val Loss: 0.0744 Acc: 0.9786
Training complete in 476m 31s

Epoch 70/79
----------
train Loss: 0.0365 Acc: 0.9980
val Loss: 0.0776 Acc: 0.9786
Training complete in 485m 21s

Epoch 71/79
----------
train Loss: 0.0366 Acc: 0.9983
val Loss: 0.0767 Acc: 0.9772
Training complete in 494m 11s

Epoch 72/79
----------
train Loss: 0.0349 Acc: 0.9989
val Loss: 0.0814 Acc: 0.9772
Training complete in 503m 1s

Epoch 73/79
----------
train Loss: 0.0364 Acc: 0.9984
val Loss: 0.0769 Acc: 0.9772
Training complete in 511m 51s

Epoch 74/79
----------
train Loss: 0.0356 Acc: 0.9982
val Loss: 0.0740 Acc: 0.9801
Training complete in 520m 42s

Epoch 75/79
----------
train Loss: 0.0345 Acc: 0.9988
val Loss: 0.0797 Acc: 0.9772
Training complete in 529m 32s

Epoch 76/79
----------
train Loss: 0.0340 Acc: 0.9989
val Loss: 0.0743 Acc: 0.9801
Training complete in 538m 23s

Epoch 77/79
----------
train Loss: 0.0335 Acc: 0.9988
val Loss: 0.0762 Acc: 0.9801
Training complete in 547m 13s

Epoch 78/79
----------
train Loss: 0.0345 Acc: 0.9985
val Loss: 0.0740 Acc: 0.9772
Training complete in 556m 2s

Epoch 79/79
----------
train Loss: 0.0355 Acc: 0.9982
val Loss: 0.0791 Acc: 0.9772
Training complete in 564m 53s

Training complete in 564m 53s
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7f8a59e63828>]
2.488097667694092
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/79
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 0.9032 Acc: 0.1465
val Loss: 3.0873 Acc: 0.3661
Training complete in 5m 18s

Epoch 1/79
----------
train Loss: 0.9238 Acc: 0.4515
val Loss: 1.2652 Acc: 0.6581
Training complete in 10m 48s

Epoch 2/79
----------
train Loss: 0.9536 Acc: 0.5899
val Loss: 0.8328 Acc: 0.7593
Training complete in 19m 31s

Epoch 3/79
----------
train Loss: 1.0412 Acc: 0.6536
val Loss: 0.6455 Acc: 0.8219
Training complete in 28m 14s

Epoch 4/79
----------
train Loss: 1.1320 Acc: 0.6972
val Loss: 0.5886 Acc: 0.8319
Training complete in 36m 57s

Epoch 5/79
----------
train Loss: 1.0067 Acc: 0.7530
val Loss: 0.3925 Acc: 0.8875
Training complete in 45m 41s

Epoch 6/79
----------
train Loss: 0.7698 Acc: 0.8090
val Loss: 0.3275 Acc: 0.9074
Training complete in 54m 24s

Epoch 7/79
----------
train Loss: 0.6514 Acc: 0.8329
val Loss: 0.2734 Acc: 0.9231
Training complete in 63m 6s

Epoch 8/79
----------
train Loss: 0.5498 Acc: 0.8629
val Loss: 0.2397 Acc: 0.9359
Training complete in 71m 49s

Epoch 9/79
----------
train Loss: 0.4858 Acc: 0.8765
val Loss: 0.2370 Acc: 0.9416
Training complete in 80m 33s

Epoch 10/79
----------
train Loss: 0.4217 Acc: 0.8917
val Loss: 0.1840 Acc: 0.9459
Training complete in 89m 15s

Epoch 11/79
----------
train Loss: 0.3544 Acc: 0.9099
val Loss: 0.1913 Acc: 0.9430
Training complete in 97m 58s

Epoch 12/79
----------
train Loss: 0.3272 Acc: 0.9163
val Loss: 0.1572 Acc: 0.9558
Training complete in 106m 42s

Epoch 13/79
----------
train Loss: 0.3011 Acc: 0.9260
val Loss: 0.1705 Acc: 0.9516
Training complete in 115m 25s

Epoch 14/79
----------
train Loss: 0.2540 Acc: 0.9360
val Loss: 0.1765 Acc: 0.9516
Training complete in 124m 8s

Epoch 15/79
----------
train Loss: 0.2300 Acc: 0.9463
val Loss: 0.1638 Acc: 0.9587
Training complete in 132m 50s

Epoch 16/79
----------
train Loss: 0.2074 Acc: 0.9515
val Loss: 0.1315 Acc: 0.9530
Training complete in 141m 33s

Epoch 17/79
----------
train Loss: 0.1991 Acc: 0.9525
val Loss: 0.1209 Acc: 0.9587
Training complete in 150m 16s

Epoch 18/79
----------
train Loss: 0.1789 Acc: 0.9576
val Loss: 0.1342 Acc: 0.9630
Training complete in 158m 60s

Epoch 19/79
----------
train Loss: 0.1661 Acc: 0.9630
val Loss: 0.1255 Acc: 0.9587
Training complete in 167m 42s

Epoch 20/79
----------
train Loss: 0.1312 Acc: 0.9745
val Loss: 0.1251 Acc: 0.9587
Training complete in 176m 25s

Epoch 21/79
----------
train Loss: 0.1246 Acc: 0.9756
val Loss: 0.1374 Acc: 0.9573
Training complete in 185m 7s

Epoch 22/79
----------
train Loss: 0.1171 Acc: 0.9772
val Loss: 0.0872 Acc: 0.9701
Training complete in 193m 50s

Epoch 23/79
----------
train Loss: 0.1042 Acc: 0.9813
val Loss: 0.1090 Acc: 0.9658
Training complete in 202m 31s

Epoch 24/79
----------
train Loss: 0.0994 Acc: 0.9834
val Loss: 0.1014 Acc: 0.9687
Training complete in 211m 14s

Epoch 25/79
----------
train Loss: 0.0930 Acc: 0.9851
val Loss: 0.0840 Acc: 0.9772
Training complete in 219m 56s

Epoch 26/79
----------
train Loss: 0.0944 Acc: 0.9830
val Loss: 0.0963 Acc: 0.9715
Training complete in 228m 39s

Epoch 27/79
----------
train Loss: 0.0879 Acc: 0.9872
val Loss: 0.1022 Acc: 0.9630
Training complete in 237m 20s

Epoch 28/79
----------
train Loss: 0.0846 Acc: 0.9876
val Loss: 0.0933 Acc: 0.9715
Training complete in 246m 2s

Epoch 29/79
----------
train Loss: 0.0791 Acc: 0.9891
val Loss: 0.0864 Acc: 0.9744
Training complete in 254m 46s

Epoch 30/79
----------
train Loss: 0.0758 Acc: 0.9897
val Loss: 0.0807 Acc: 0.9772
Training complete in 263m 28s

Epoch 31/79
----------
train Loss: 0.0727 Acc: 0.9908
val Loss: 0.0853 Acc: 0.9772
Training complete in 272m 10s

Epoch 32/79
----------
train Loss: 0.0688 Acc: 0.9911
val Loss: 0.0769 Acc: 0.9758
Training complete in 280m 52s

Epoch 33/79
----------
train Loss: 0.0690 Acc: 0.9908
val Loss: 0.0838 Acc: 0.9729
Training complete in 289m 35s

Epoch 34/79
----------
train Loss: 0.0642 Acc: 0.9930
val Loss: 0.0676 Acc: 0.9772
Training complete in 298m 18s

Epoch 35/79
----------
train Loss: 0.0625 Acc: 0.9937
val Loss: 0.0823 Acc: 0.9772
Training complete in 306m 59s

Epoch 36/79
----------
train Loss: 0.0585 Acc: 0.9937
val Loss: 0.0775 Acc: 0.9801
Training complete in 315m 41s

Epoch 37/79
----------
train Loss: 0.0625 Acc: 0.9939
val Loss: 0.0832 Acc: 0.9772
Training complete in 321m 39s

Epoch 38/79
----------
train Loss: 0.0561 Acc: 0.9944
val Loss: 0.0966 Acc: 0.9729
Training complete in 326m 55s

Epoch 39/79
----------
train Loss: 0.0576 Acc: 0.9938
val Loss: 0.0843 Acc: 0.9772
Training complete in 331m 35s

Epoch 40/79
----------
train Loss: 0.0578 Acc: 0.9949
val Loss: 0.0913 Acc: 0.9729
Training complete in 336m 47s

Epoch 41/79
----------
train Loss: 0.0529 Acc: 0.9960
val Loss: 0.0812 Acc: 0.9758
Training complete in 341m 32s

Epoch 42/79
----------
train Loss: 0.0498 Acc: 0.9962
val Loss: 0.0762 Acc: 0.9786
Training complete in 346m 47s

Epoch 43/79
----------
train Loss: 0.0473 Acc: 0.9966
val Loss: 0.0811 Acc: 0.9758
Training complete in 352m 27s

Epoch 44/79
----------
train Loss: 0.0496 Acc: 0.9964
val Loss: 0.0818 Acc: 0.9786
Training complete in 357m 13s

Epoch 45/79
----------
train Loss: 0.0487 Acc: 0.9966
val Loss: 0.0807 Acc: 0.9801
Training complete in 362m 19s

Epoch 46/79
----------
train Loss: 0.0487 Acc: 0.9964
val Loss: 0.0832 Acc: 0.9772
Training complete in 367m 33s

Epoch 47/79
----------
train Loss: 0.0465 Acc: 0.9968
val Loss: 0.0786 Acc: 0.9772
Training complete in 372m 46s

Epoch 48/79
----------
train Loss: 0.0457 Acc: 0.9968
val Loss: 0.0779 Acc: 0.9786
Training complete in 377m 57s

Epoch 49/79
----------
train Loss: 0.0467 Acc: 0.9968
val Loss: 0.0722 Acc: 0.9772
Training complete in 382m 49s

Epoch 50/79
----------
train Loss: 0.0466 Acc: 0.9971
val Loss: 0.0717 Acc: 0.9786
Training complete in 387m 55s

Epoch 51/79
----------
train Loss: 0.0444 Acc: 0.9968
val Loss: 0.0801 Acc: 0.9772
Training complete in 392m 47s

Epoch 52/79
----------
train Loss: 0.0413 Acc: 0.9977
val Loss: 0.0759 Acc: 0.9772
Training complete in 397m 24s

Epoch 53/79
----------
train Loss: 0.0425 Acc: 0.9975
val Loss: 0.0790 Acc: 0.9758
Training complete in 402m 1s

Epoch 54/79
----------
train Loss: 0.0440 Acc: 0.9969
val Loss: 0.0768 Acc: 0.9729
Training complete in 407m 6s

Epoch 55/79
----------
train Loss: 0.0419 Acc: 0.9977
val Loss: 0.0658 Acc: 0.9744
Training complete in 411m 59s

Epoch 56/79
----------
train Loss: 0.0423 Acc: 0.9976
val Loss: 0.0772 Acc: 0.9786
Training complete in 416m 53s

Epoch 57/79
----------
train Loss: 0.0407 Acc: 0.9973
val Loss: 0.0771 Acc: 0.9758
Training complete in 421m 29s

Epoch 58/79
----------
train Loss: 0.0415 Acc: 0.9972
val Loss: 0.0784 Acc: 0.9801
Training complete in 426m 34s

Epoch 59/79
----------
train Loss: 0.0420 Acc: 0.9971
val Loss: 0.0771 Acc: 0.9801
Training complete in 431m 43s

Epoch 60/79
----------
train Loss: 0.0395 Acc: 0.9981
val Loss: 0.0753 Acc: 0.9758
Training complete in 436m 50s

Epoch 61/79
----------
train Loss: 0.0387 Acc: 0.9977
val Loss: 0.0791 Acc: 0.9744
Training complete in 442m 16s

Epoch 62/79
----------
train Loss: 0.0377 Acc: 0.9982
val Loss: 0.0862 Acc: 0.9744
Training complete in 447m 37s

Epoch 63/79
----------
train Loss: 0.0391 Acc: 0.9979
val Loss: 0.0774 Acc: 0.9758
Training complete in 452m 35s

Epoch 64/79
----------
train Loss: 0.0401 Acc: 0.9975
val Loss: 0.0881 Acc: 0.9786
Training complete in 457m 38s

Epoch 65/79
----------
train Loss: 0.0392 Acc: 0.9978
val Loss: 0.0792 Acc: 0.9772
Training complete in 462m 42s

Epoch 66/79
----------
train Loss: 0.0370 Acc: 0.9982
val Loss: 0.0762 Acc: 0.9786
Training complete in 467m 27s

Epoch 67/79
----------
train Loss: 0.0363 Acc: 0.9986
val Loss: 0.0821 Acc: 0.9758
Training complete in 472m 38s

Epoch 68/79
----------
train Loss: 0.0360 Acc: 0.9986
val Loss: 0.0834 Acc: 0.9758
Training complete in 477m 52s

Epoch 69/79
----------
train Loss: 0.0374 Acc: 0.9983
val Loss: 0.0762 Acc: 0.9786
Training complete in 483m 16s

Epoch 70/79
----------
train Loss: 0.0339 Acc: 0.9983
val Loss: 0.0822 Acc: 0.9786
Training complete in 488m 40s

Epoch 71/79
----------
train Loss: 0.0373 Acc: 0.9979
val Loss: 0.0808 Acc: 0.9786
Training complete in 493m 43s

Epoch 72/79
----------
train Loss: 0.0364 Acc: 0.9985
val Loss: 0.0790 Acc: 0.9786
Training complete in 498m 54s

Epoch 73/79
----------
train Loss: 0.0355 Acc: 0.9985
val Loss: 0.0869 Acc: 0.9772
Training complete in 504m 4s

Epoch 74/79
----------
train Loss: 0.0364 Acc: 0.9984
val Loss: 0.0876 Acc: 0.9744
Training complete in 509m 16s

Epoch 75/79
----------
train Loss: 0.0375 Acc: 0.9978
val Loss: 0.0871 Acc: 0.9758
Training complete in 514m 2s

Epoch 76/79
----------
train Loss: 0.0368 Acc: 0.9980
val Loss: 0.0769 Acc: 0.9786
Training complete in 519m 11s

Epoch 77/79
----------
train Loss: 0.0347 Acc: 0.9978
val Loss: 0.0914 Acc: 0.9758
Training complete in 523m 59s

Epoch 78/79
----------
train Loss: 0.0378 Acc: 0.9983
val Loss: 0.0862 Acc: 0.9772
Training complete in 528m 37s

Epoch 79/79
----------
train Loss: 0.0357 Acc: 0.9984
val Loss: 0.0868 Acc: 0.9772
Training complete in 533m 51s

Training complete in 533m 51s
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7ff95102e668>]
2.5027854442596436
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/79
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: nan Acc: 0.0078
val Loss: nan Acc: 0.0014
Training complete in 5m 32s

Epoch 1/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 11m 1s

Epoch 2/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 19m 59s

Epoch 3/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 29m 26s

Epoch 4/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 38m 59s

Epoch 5/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 48m 26s

Epoch 6/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 57m 57s

Epoch 7/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 67m 29s

Epoch 8/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 77m 0s

Epoch 9/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 86m 30s

Epoch 10/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 95m 59s

Epoch 11/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 105m 31s

Epoch 12/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 115m 3s

Epoch 13/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 124m 35s

Epoch 14/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 134m 5s

Epoch 15/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 143m 36s

Epoch 16/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 153m 8s

Epoch 17/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 162m 39s

Epoch 18/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 172m 10s

Epoch 19/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 181m 42s

Epoch 20/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 191m 13s

Epoch 21/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 200m 44s

Epoch 22/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 210m 13s

Epoch 23/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 219m 43s

Epoch 24/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 229m 15s

Epoch 25/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 238m 45s

Epoch 26/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 248m 17s

Epoch 27/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 257m 48s

Epoch 28/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 267m 16s

Epoch 29/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 276m 47s

Epoch 30/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 286m 18s

Epoch 31/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 295m 49s

Epoch 32/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 304m 31s

Epoch 33/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 310m 5s

Epoch 34/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 315m 44s

Epoch 35/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 321m 40s

Epoch 36/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 327m 26s

Epoch 37/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 334m 11s

Epoch 38/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 341m 24s

Epoch 39/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 350m 56s

Epoch 40/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 360m 27s

Epoch 41/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 369m 58s

Epoch 42/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 379m 29s

Epoch 43/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 389m 0s

Epoch 44/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 398m 31s

Epoch 45/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 408m 5s

Epoch 46/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 417m 35s

Epoch 47/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 427m 6s

Epoch 48/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 436m 38s

Epoch 49/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 446m 10s

Epoch 50/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 455m 40s

Epoch 51/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 465m 11s

Epoch 52/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 474m 42s

Epoch 53/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 484m 12s

Epoch 54/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 493m 45s

Epoch 55/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 503m 18s

Epoch 56/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 512m 49s

Epoch 57/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 522m 22s

Epoch 58/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 531m 52s

Epoch 59/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 541m 23s

Epoch 60/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 550m 56s

Epoch 61/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 560m 26s

Epoch 62/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 569m 57s

Epoch 63/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 579m 28s

Epoch 64/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 588m 57s

Epoch 65/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 598m 30s

Epoch 66/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 608m 1s

Epoch 67/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 617m 34s

Epoch 68/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 627m 4s

Epoch 69/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 634m 48s

Epoch 70/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 640m 16s

Epoch 71/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 645m 36s

Epoch 72/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 651m 3s

Epoch 73/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 656m 29s

Epoch 74/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 661m 43s

Epoch 75/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 667m 9s

Epoch 76/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 672m 40s

Epoch 77/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 678m 9s

Epoch 78/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 683m 39s

Epoch 79/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 689m 14s

Training complete in 689m 14s
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7fad2663b828>]
2.495758056640625
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (Sum_ReLU_2): Sum_ReLU_2()
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/79
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: nan Acc: 0.0071
val Loss: nan Acc: 0.0014
Training complete in 7m 9s

Epoch 1/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 17m 11s

Epoch 2/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 27m 16s

Epoch 3/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 37m 19s

Epoch 4/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 47m 21s

Epoch 5/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 57m 24s

Epoch 6/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 67m 28s

Epoch 7/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 77m 31s

Epoch 8/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 87m 32s

Epoch 9/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 97m 36s

Epoch 10/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 107m 40s

Epoch 11/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 117m 42s

Epoch 12/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 127m 46s

Epoch 13/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 137m 50s

Epoch 14/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 147m 52s

Epoch 15/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 157m 53s

Epoch 16/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 167m 54s

Epoch 17/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 177m 57s

Epoch 18/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 187m 59s

Epoch 19/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 198m 1s

Epoch 20/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 208m 3s

Epoch 21/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 218m 4s

Epoch 22/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 228m 5s

Epoch 23/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 238m 9s

Epoch 24/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 248m 12s

Epoch 25/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 258m 14s

Epoch 26/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 268m 19s

Epoch 27/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 278m 24s

Epoch 28/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 288m 26s

Epoch 29/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 298m 30s

Epoch 30/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 308m 33s

Epoch 31/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 318m 36s

Epoch 32/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 328m 38s

Epoch 33/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 338m 41s

Epoch 34/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 348m 42s

Epoch 35/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 358m 46s

Epoch 36/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 368m 50s

Epoch 37/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 378m 53s

Epoch 38/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 386m 30s

Epoch 39/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 392m 34s

Epoch 40/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 399m 3s

Epoch 41/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 405m 26s

Epoch 42/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 411m 21s

Epoch 43/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 417m 14s

Epoch 44/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 423m 14s

Epoch 45/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 429m 58s

Epoch 46/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 436m 24s

Epoch 47/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 442m 2s

Epoch 48/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 448m 7s

Epoch 49/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 454m 3s

Epoch 50/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 460m 3s

Epoch 51/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 465m 58s

Epoch 52/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 471m 54s

Epoch 53/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 477m 53s

Epoch 54/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 484m 0s

Epoch 55/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 490m 34s

Epoch 56/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 496m 31s

Epoch 57/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 502m 9s

Epoch 58/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 508m 43s

Epoch 59/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 515m 3s

Epoch 60/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 520m 56s

Epoch 61/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 526m 54s

Epoch 62/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 533m 3s

Epoch 63/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 538m 49s

Epoch 64/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 544m 45s

Epoch 65/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 551m 1s

Epoch 66/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 556m 60s

Epoch 67/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 562m 56s

Epoch 68/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 568m 55s

Epoch 69/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 574m 55s

Epoch 70/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 580m 53s

Epoch 71/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0000
Training complete in 586m 55s

Epoch 72/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 593m 2s

Epoch 73/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 598m 58s

Epoch 74/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 604m 57s

Epoch 75/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 610m 59s

Epoch 76/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 616m 54s

Epoch 77/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 622m 50s

Epoch 78/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 628m 49s

Epoch 79/79
----------
train Loss: nan Acc: 0.0013
val Loss: nan Acc: 0.0014
Training complete in 634m 50s

Training complete in 634m 50s
Traceback (most recent call last):
  File "train.py", line 27, in <module>
    from has import HideAndSeed
ModuleNotFoundError: No module named 'has'
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7feab5909898>]
2.491417169570923
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (Sum_ReLU_2): Sum_ReLU_2()
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/99
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 0.8928 Acc: 0.1513
val Loss: 3.0818 Acc: 0.3533
Training complete in 4m 53s

Epoch 1/99
----------
train Loss: 0.9092 Acc: 0.4576
val Loss: 1.2923 Acc: 0.6838
Training complete in 13m 31s

Epoch 2/99
----------
train Loss: 0.9278 Acc: 0.6044
val Loss: 0.8698 Acc: 0.7735
Training complete in 22m 49s

Epoch 3/99
----------
train Loss: 1.0267 Acc: 0.6648
val Loss: 0.7233 Acc: 0.7977
Training complete in 32m 8s

Epoch 4/99
----------
train Loss: 1.1075 Acc: 0.7028
val Loss: 0.5567 Acc: 0.8533
Training complete in 41m 28s

Epoch 5/99
----------
train Loss: 1.0090 Acc: 0.7544
val Loss: 0.4000 Acc: 0.8846
Training complete in 50m 45s

Epoch 6/99
----------
train Loss: 0.7706 Acc: 0.8097
val Loss: 0.3305 Acc: 0.9003
Training complete in 60m 3s

Epoch 7/99
----------
train Loss: 0.6510 Acc: 0.8338
val Loss: 0.2719 Acc: 0.9274
Training complete in 69m 21s

Epoch 8/99
----------
train Loss: 0.5093 Acc: 0.8678
val Loss: 0.2760 Acc: 0.9288
Training complete in 78m 42s

Epoch 9/99
----------
train Loss: 0.4825 Acc: 0.8761
val Loss: 0.2029 Acc: 0.9402
Training complete in 88m 0s

Epoch 10/99
----------
train Loss: 0.4310 Acc: 0.8928
val Loss: 0.2262 Acc: 0.9345
Training complete in 97m 19s

Epoch 11/99
----------
train Loss: 0.3635 Acc: 0.9070
val Loss: 0.1629 Acc: 0.9501
Training complete in 106m 37s

Epoch 12/99
----------
train Loss: 0.2958 Acc: 0.9254
val Loss: 0.1391 Acc: 0.9530
Training complete in 115m 57s

Epoch 13/99
----------
train Loss: 0.2834 Acc: 0.9276
val Loss: 0.1533 Acc: 0.9487
Training complete in 125m 16s

Epoch 14/99
----------
train Loss: 0.2591 Acc: 0.9373
val Loss: 0.1396 Acc: 0.9573
Training complete in 134m 36s

Epoch 15/99
----------
train Loss: 0.2309 Acc: 0.9429
val Loss: 0.1377 Acc: 0.9601
Training complete in 143m 54s

Epoch 16/99
----------
train Loss: 0.1993 Acc: 0.9501
val Loss: 0.1364 Acc: 0.9558
Training complete in 153m 12s

Epoch 17/99
----------
train Loss: 0.1962 Acc: 0.9534
val Loss: 0.1370 Acc: 0.9601
Training complete in 162m 31s

Epoch 18/99
----------
train Loss: 0.1802 Acc: 0.9592
val Loss: 0.1392 Acc: 0.9658
Training complete in 171m 51s

Epoch 19/99
----------
train Loss: 0.1666 Acc: 0.9621
val Loss: 0.1164 Acc: 0.9672
Training complete in 181m 10s

Epoch 20/99
----------
train Loss: 0.1523 Acc: 0.9681
val Loss: 0.1212 Acc: 0.9644
Training complete in 190m 27s

Epoch 21/99
----------
train Loss: 0.1204 Acc: 0.9755
val Loss: 0.0900 Acc: 0.9715
Training complete in 199m 45s

Epoch 22/99
----------
train Loss: 0.1112 Acc: 0.9796
val Loss: 0.0913 Acc: 0.9715
Training complete in 209m 5s

Epoch 23/99
----------
train Loss: 0.1093 Acc: 0.9804
val Loss: 0.0867 Acc: 0.9758
Training complete in 218m 24s

Epoch 24/99
----------
train Loss: 0.1004 Acc: 0.9817
val Loss: 0.0876 Acc: 0.9744
Training complete in 227m 42s

Epoch 25/99
----------
train Loss: 0.0951 Acc: 0.9839
val Loss: 0.0934 Acc: 0.9729
Training complete in 237m 1s

Epoch 26/99
----------
train Loss: 0.0917 Acc: 0.9850
val Loss: 0.0947 Acc: 0.9672
Training complete in 246m 21s

Epoch 27/99
----------
train Loss: 0.0838 Acc: 0.9867
val Loss: 0.0989 Acc: 0.9715
Training complete in 255m 41s

Epoch 28/99
----------
train Loss: 0.0831 Acc: 0.9876
val Loss: 0.0904 Acc: 0.9715
Training complete in 264m 58s

Epoch 29/99
----------
train Loss: 0.0713 Acc: 0.9906
val Loss: 0.0836 Acc: 0.9758
Training complete in 274m 17s

Epoch 30/99
----------
train Loss: 0.0710 Acc: 0.9918
val Loss: 0.0822 Acc: 0.9758
Training complete in 283m 35s

Epoch 31/99
----------
train Loss: 0.0692 Acc: 0.9913
val Loss: 0.0741 Acc: 0.9744
Training complete in 292m 55s

Epoch 32/99
----------
train Loss: 0.0678 Acc: 0.9915
val Loss: 0.0870 Acc: 0.9729
Training complete in 302m 12s

Epoch 33/99
----------
train Loss: 0.0663 Acc: 0.9920
val Loss: 0.0792 Acc: 0.9758
Training complete in 311m 31s

Epoch 34/99
----------
train Loss: 0.0635 Acc: 0.9929
val Loss: 0.0701 Acc: 0.9801
Training complete in 320m 47s

Epoch 35/99
----------
train Loss: 0.0679 Acc: 0.9917
val Loss: 0.0829 Acc: 0.9744
Training complete in 330m 6s

Epoch 36/99
----------
train Loss: 0.0603 Acc: 0.9931
val Loss: 0.0888 Acc: 0.9729
Training complete in 339m 24s

Epoch 37/99
----------
train Loss: 0.0578 Acc: 0.9942
val Loss: 0.0833 Acc: 0.9772
Training complete in 348m 43s

Epoch 38/99
----------
train Loss: 0.0542 Acc: 0.9945
val Loss: 0.0925 Acc: 0.9744
Training complete in 357m 59s

Epoch 39/99
----------
train Loss: 0.0533 Acc: 0.9956
val Loss: 0.0872 Acc: 0.9758
Training complete in 367m 19s

Epoch 40/99
----------
train Loss: 0.0533 Acc: 0.9949
val Loss: 0.0826 Acc: 0.9772
Training complete in 376m 2s

Epoch 41/99
----------
train Loss: 0.0509 Acc: 0.9957
val Loss: 0.0807 Acc: 0.9801
Training complete in 380m 58s

Epoch 42/99
----------
train Loss: 0.0510 Acc: 0.9953
val Loss: 0.0831 Acc: 0.9758
Training complete in 386m 16s

Epoch 43/99
----------
train Loss: 0.0503 Acc: 0.9959
val Loss: 0.0786 Acc: 0.9786
Training complete in 391m 31s

Epoch 44/99
----------
train Loss: 0.0483 Acc: 0.9960
val Loss: 0.0743 Acc: 0.9786
Training complete in 396m 30s

Epoch 45/99
----------
train Loss: 0.0502 Acc: 0.9961
val Loss: 0.0780 Acc: 0.9758
Training complete in 401m 39s

Epoch 46/99
----------
train Loss: 0.0462 Acc: 0.9961
val Loss: 0.0822 Acc: 0.9772
Training complete in 406m 25s

Epoch 47/99
----------
train Loss: 0.0471 Acc: 0.9960
val Loss: 0.0819 Acc: 0.9758
Training complete in 411m 30s

Epoch 48/99
----------
train Loss: 0.0484 Acc: 0.9961
val Loss: 0.0863 Acc: 0.9786
Training complete in 416m 8s

Epoch 49/99
----------
train Loss: 0.0448 Acc: 0.9972
val Loss: 0.0891 Acc: 0.9758
Training complete in 421m 21s

Epoch 50/99
----------
train Loss: 0.0442 Acc: 0.9972
val Loss: 0.0838 Acc: 0.9758
Training complete in 426m 34s

Epoch 51/99
----------
train Loss: 0.0436 Acc: 0.9972
val Loss: 0.0875 Acc: 0.9758
Training complete in 431m 36s

Epoch 52/99
----------
train Loss: 0.0452 Acc: 0.9970
val Loss: 0.0869 Acc: 0.9786
Training complete in 436m 44s

Epoch 53/99
----------
train Loss: 0.0434 Acc: 0.9970
val Loss: 0.0787 Acc: 0.9801
Training complete in 441m 50s

Epoch 54/99
----------
train Loss: 0.0409 Acc: 0.9977
val Loss: 0.0853 Acc: 0.9772
Training complete in 446m 34s

Epoch 55/99
----------
train Loss: 0.0424 Acc: 0.9973
val Loss: 0.0847 Acc: 0.9801
Training complete in 451m 50s

Epoch 56/99
----------
train Loss: 0.0406 Acc: 0.9979
val Loss: 0.0764 Acc: 0.9786
Training complete in 456m 36s

Epoch 57/99
----------
train Loss: 0.0406 Acc: 0.9975
val Loss: 0.0823 Acc: 0.9801
Training complete in 461m 54s

Epoch 58/99
----------
train Loss: 0.0408 Acc: 0.9976
val Loss: 0.0831 Acc: 0.9772
Training complete in 466m 52s

Epoch 59/99
----------
train Loss: 0.0411 Acc: 0.9975
val Loss: 0.0831 Acc: 0.9801
Training complete in 471m 60s

Epoch 60/99
----------
train Loss: 0.0411 Acc: 0.9979
val Loss: 0.0721 Acc: 0.9815
Training complete in 477m 45s

Epoch 61/99
----------
train Loss: 0.0396 Acc: 0.9979
val Loss: 0.0750 Acc: 0.9758
Training complete in 483m 2s

Epoch 62/99
----------
train Loss: 0.0387 Acc: 0.9978
val Loss: 0.0782 Acc: 0.9772
Training complete in 488m 9s

Epoch 63/99
----------
train Loss: 0.0383 Acc: 0.9981
val Loss: 0.0780 Acc: 0.9786
Training complete in 493m 12s

Epoch 64/99
----------
train Loss: 0.0370 Acc: 0.9979
val Loss: 0.0754 Acc: 0.9786
Training complete in 498m 12s

Epoch 65/99
----------
train Loss: 0.0371 Acc: 0.9982
val Loss: 0.0794 Acc: 0.9815
Training complete in 503m 28s

Epoch 66/99
----------
train Loss: 0.0368 Acc: 0.9984
val Loss: 0.0814 Acc: 0.9801
Training complete in 508m 45s

Epoch 67/99
----------
train Loss: 0.0363 Acc: 0.9984
val Loss: 0.0762 Acc: 0.9801
Training complete in 513m 58s

Epoch 68/99
----------
train Loss: 0.0378 Acc: 0.9973
val Loss: 0.0783 Acc: 0.9772
Training complete in 518m 43s

Epoch 69/99
----------
train Loss: 0.0397 Acc: 0.9977
val Loss: 0.0835 Acc: 0.9786
Training complete in 523m 40s

Epoch 70/99
----------
train Loss: 0.0362 Acc: 0.9980
val Loss: 0.0733 Acc: 0.9801
Training complete in 528m 55s

Epoch 71/99
----------
train Loss: 0.0371 Acc: 0.9983
val Loss: 0.0827 Acc: 0.9801
Training complete in 534m 9s

Epoch 72/99
----------
train Loss: 0.0361 Acc: 0.9985
val Loss: 0.0834 Acc: 0.9801
Training complete in 538m 60s

Epoch 73/99
----------
train Loss: 0.0363 Acc: 0.9984
val Loss: 0.0860 Acc: 0.9786
Training complete in 543m 45s

Epoch 74/99
----------
train Loss: 0.0372 Acc: 0.9982
val Loss: 0.0807 Acc: 0.9815
Training complete in 549m 0s

Epoch 75/99
----------
train Loss: 0.0359 Acc: 0.9984
val Loss: 0.0873 Acc: 0.9801
Training complete in 553m 47s

Epoch 76/99
----------
train Loss: 0.0369 Acc: 0.9980
val Loss: 0.0854 Acc: 0.9772
Training complete in 559m 7s

Epoch 77/99
----------
train Loss: 0.0367 Acc: 0.9982
val Loss: 0.0889 Acc: 0.9801
Training complete in 564m 13s

Epoch 78/99
----------
train Loss: 0.0355 Acc: 0.9983
val Loss: 0.0826 Acc: 0.9801
Training complete in 569m 20s

Epoch 79/99
----------
train Loss: 0.0355 Acc: 0.9982
val Loss: 0.0840 Acc: 0.9786
Training complete in 574m 15s

Epoch 80/99
----------
train Loss: 0.0340 Acc: 0.9983
val Loss: 0.0883 Acc: 0.9801
Training complete in 579m 29s

Epoch 81/99
----------
train Loss: 0.0337 Acc: 0.9987
val Loss: 0.0847 Acc: 0.9786
Training complete in 584m 42s

Epoch 82/99
----------
train Loss: 0.0355 Acc: 0.9987
val Loss: 0.0807 Acc: 0.9786
Training complete in 589m 59s

Epoch 83/99
----------
train Loss: 0.0345 Acc: 0.9984
val Loss: 0.0901 Acc: 0.9801
Training complete in 594m 60s

Epoch 84/99
----------
train Loss: 0.0331 Acc: 0.9991
val Loss: 0.0828 Acc: 0.9772
Training complete in 599m 41s

Epoch 85/99
----------
train Loss: 0.0335 Acc: 0.9984
val Loss: 0.0835 Acc: 0.9772
Training complete in 605m 9s

Epoch 86/99
----------
train Loss: 0.0320 Acc: 0.9989
val Loss: 0.0915 Acc: 0.9786
Training complete in 610m 27s

Epoch 87/99
----------
train Loss: 0.0340 Acc: 0.9983
val Loss: 0.0869 Acc: 0.9772
Training complete in 615m 41s

Epoch 88/99
----------
train Loss: 0.0326 Acc: 0.9987
val Loss: 0.0880 Acc: 0.9801
Training complete in 620m 45s

Epoch 89/99
----------
train Loss: 0.0327 Acc: 0.9992
val Loss: 0.0805 Acc: 0.9786
Training complete in 625m 56s

Epoch 90/99
----------
train Loss: 0.0339 Acc: 0.9987
val Loss: 0.0884 Acc: 0.9772
Training complete in 630m 40s

Epoch 91/99
----------
train Loss: 0.0322 Acc: 0.9989
val Loss: 0.0942 Acc: 0.9772
Training complete in 635m 23s

Epoch 92/99
----------
train Loss: 0.0336 Acc: 0.9985
val Loss: 0.0846 Acc: 0.9758
Training complete in 640m 40s

Epoch 93/99
----------
train Loss: 0.0327 Acc: 0.9988
val Loss: 0.0854 Acc: 0.9801
Training complete in 645m 54s

Epoch 94/99
----------
train Loss: 0.0314 Acc: 0.9987
val Loss: 0.0859 Acc: 0.9801
Training complete in 651m 27s

Epoch 95/99
----------
train Loss: 0.0333 Acc: 0.9984
val Loss: 0.0853 Acc: 0.9801
Training complete in 656m 39s

Epoch 96/99
----------
train Loss: 0.0330 Acc: 0.9987
val Loss: 0.0898 Acc: 0.9772
Training complete in 661m 55s

Epoch 97/99
----------
train Loss: 0.0316 Acc: 0.9989
val Loss: 0.0855 Acc: 0.9786
Training complete in 667m 8s

Epoch 98/99
----------
train Loss: 0.0330 Acc: 0.9987
val Loss: 0.0814 Acc: 0.9815
Training complete in 672m 18s

Epoch 99/99
----------
train Loss: 0.0312 Acc: 0.9985
val Loss: 0.0861 Acc: 0.9786
Training complete in 677m 40s

Training complete in 677m 40s
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7f9ca9926a20>]
2.4779696464538574
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (nsrelu): NSReLU()
  (Sum_ReLU_2): Sum_ReLU_2()
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/99
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 0.8877 Acc: 0.1578
val Loss: 2.9080 Acc: 0.3946
Training complete in 8m 26s

Epoch 1/99
----------
train Loss: 0.8991 Acc: 0.4647
val Loss: 1.1877 Acc: 0.7009
Training complete in 13m 26s

Epoch 2/99
----------
train Loss: 0.9204 Acc: 0.6090
val Loss: 0.8565 Acc: 0.7692
Training complete in 18m 21s

Epoch 3/99
----------
train Loss: 1.0353 Acc: 0.6607
val Loss: 0.6979 Acc: 0.8134
Training complete in 23m 1s

Epoch 4/99
----------
train Loss: 1.0887 Acc: 0.7101
val Loss: 0.5626 Acc: 0.8476
Training complete in 27m 57s

Epoch 5/99
----------
train Loss: 0.9966 Acc: 0.7532
val Loss: 0.4127 Acc: 0.8903
Training complete in 32m 54s

Epoch 6/99
----------
train Loss: 0.7815 Acc: 0.8038
val Loss: 0.2671 Acc: 0.9202
Training complete in 38m 2s

Epoch 7/99
----------
train Loss: 0.6388 Acc: 0.8420
val Loss: 0.3010 Acc: 0.9174
Training complete in 43m 7s

Epoch 8/99
----------
train Loss: 0.5518 Acc: 0.8566
val Loss: 0.2322 Acc: 0.9330
Training complete in 48m 13s

Epoch 9/99
----------
train Loss: 0.4838 Acc: 0.8762
val Loss: 0.2016 Acc: 0.9444
Training complete in 53m 18s

Epoch 10/99
----------
train Loss: 0.4075 Acc: 0.8951
val Loss: 0.1888 Acc: 0.9416
Training complete in 58m 24s

Epoch 11/99
----------
train Loss: 0.3537 Acc: 0.9107
val Loss: 0.1671 Acc: 0.9530
Training complete in 63m 25s

Epoch 12/99
----------
train Loss: 0.3079 Acc: 0.9201
val Loss: 0.1764 Acc: 0.9416
Training complete in 68m 34s

Epoch 13/99
----------
train Loss: 0.3098 Acc: 0.9220
val Loss: 0.1346 Acc: 0.9587
Training complete in 73m 34s

Epoch 14/99
----------
train Loss: 0.2650 Acc: 0.9320
val Loss: 0.1286 Acc: 0.9558
Training complete in 78m 37s

Epoch 15/99
----------
train Loss: 0.2220 Acc: 0.9461
val Loss: 0.1322 Acc: 0.9630
Training complete in 83m 41s

Epoch 16/99
----------
train Loss: 0.1964 Acc: 0.9539
val Loss: 0.1133 Acc: 0.9630
Training complete in 88m 48s

Epoch 17/99
----------
train Loss: 0.1802 Acc: 0.9572
val Loss: 0.1329 Acc: 0.9615
Training complete in 93m 51s

Epoch 18/99
----------
train Loss: 0.1773 Acc: 0.9603
val Loss: 0.1179 Acc: 0.9601
Training complete in 98m 57s

Epoch 19/99
----------
train Loss: 0.1637 Acc: 0.9652
val Loss: 0.1120 Acc: 0.9672
Training complete in 104m 5s

Epoch 20/99
----------
train Loss: 0.1501 Acc: 0.9688
val Loss: 0.1176 Acc: 0.9687
Training complete in 109m 10s

Epoch 21/99
----------
train Loss: 0.1326 Acc: 0.9721
val Loss: 0.1079 Acc: 0.9687
Training complete in 114m 16s

Epoch 22/99
----------
train Loss: 0.1276 Acc: 0.9743
val Loss: 0.0989 Acc: 0.9658
Training complete in 119m 25s

Epoch 23/99
----------
train Loss: 0.1158 Acc: 0.9769
val Loss: 0.1033 Acc: 0.9672
Training complete in 124m 26s

Epoch 24/99
----------
train Loss: 0.1044 Acc: 0.9817
val Loss: 0.0721 Acc: 0.9758
Training complete in 129m 34s

Epoch 25/99
----------
train Loss: 0.1053 Acc: 0.9826
val Loss: 0.0809 Acc: 0.9715
Training complete in 134m 39s

Epoch 26/99
----------
train Loss: 0.0925 Acc: 0.9839
val Loss: 0.0900 Acc: 0.9672
Training complete in 139m 40s

Epoch 27/99
----------
train Loss: 0.0824 Acc: 0.9867
val Loss: 0.1048 Acc: 0.9687
Training complete in 144m 44s

Epoch 28/99
----------
train Loss: 0.0843 Acc: 0.9875
val Loss: 0.0981 Acc: 0.9658
Training complete in 149m 49s

Epoch 29/99
----------
train Loss: 0.0784 Acc: 0.9895
val Loss: 0.0863 Acc: 0.9715
Training complete in 154m 59s

Epoch 30/99
----------
train Loss: 0.0705 Acc: 0.9910
val Loss: 0.0851 Acc: 0.9744
Training complete in 160m 9s

Epoch 31/99
----------
train Loss: 0.0700 Acc: 0.9915
val Loss: 0.0715 Acc: 0.9729
Training complete in 165m 11s

Epoch 32/99
----------
train Loss: 0.0669 Acc: 0.9918
val Loss: 0.0823 Acc: 0.9687
Training complete in 170m 20s

Epoch 33/99
----------
train Loss: 0.0717 Acc: 0.9905
val Loss: 0.0847 Acc: 0.9715
Training complete in 175m 27s

Epoch 34/99
----------
train Loss: 0.0597 Acc: 0.9937
val Loss: 0.0793 Acc: 0.9701
Training complete in 180m 26s

Epoch 35/99
----------
train Loss: 0.0605 Acc: 0.9933
val Loss: 0.0991 Acc: 0.9687
Training complete in 185m 31s

Epoch 36/99
----------
train Loss: 0.0639 Acc: 0.9922
val Loss: 0.0865 Acc: 0.9672
Training complete in 190m 36s

Epoch 37/99
----------
train Loss: 0.0621 Acc: 0.9940
val Loss: 0.0872 Acc: 0.9672
Training complete in 195m 43s

Epoch 38/99
----------
train Loss: 0.0564 Acc: 0.9939
val Loss: 0.0841 Acc: 0.9744
Training complete in 200m 42s

Epoch 39/99
----------
train Loss: 0.0556 Acc: 0.9948
val Loss: 0.0912 Acc: 0.9715
Training complete in 205m 42s

Epoch 40/99
----------
train Loss: 0.0494 Acc: 0.9962
val Loss: 0.0803 Acc: 0.9758
Training complete in 210m 39s

Epoch 41/99
----------
train Loss: 0.0520 Acc: 0.9954
val Loss: 0.0833 Acc: 0.9744
Training complete in 215m 41s

Epoch 42/99
----------
train Loss: 0.0473 Acc: 0.9968
val Loss: 0.0749 Acc: 0.9744
Training complete in 220m 42s

Epoch 43/99
----------
train Loss: 0.0494 Acc: 0.9958
val Loss: 0.0738 Acc: 0.9729
Training complete in 225m 47s

Epoch 44/99
----------
train Loss: 0.0480 Acc: 0.9965
val Loss: 0.0731 Acc: 0.9758
Training complete in 230m 43s

Epoch 45/99
----------
train Loss: 0.0471 Acc: 0.9964
val Loss: 0.0662 Acc: 0.9786
Training complete in 235m 41s

Epoch 46/99
----------
train Loss: 0.0455 Acc: 0.9969
val Loss: 0.0770 Acc: 0.9729
Training complete in 240m 51s

Epoch 47/99
----------
train Loss: 0.0485 Acc: 0.9958
val Loss: 0.0741 Acc: 0.9758
Training complete in 245m 58s

Epoch 48/99
----------
train Loss: 0.0477 Acc: 0.9961
val Loss: 0.0778 Acc: 0.9744
Training complete in 250m 56s

Epoch 49/99
----------
train Loss: 0.0446 Acc: 0.9975
val Loss: 0.0794 Acc: 0.9758
Training complete in 255m 56s

Epoch 50/99
----------
train Loss: 0.0424 Acc: 0.9977
val Loss: 0.0810 Acc: 0.9744
Training complete in 260m 56s

Epoch 51/99
----------
train Loss: 0.0439 Acc: 0.9967
val Loss: 0.0868 Acc: 0.9729
Training complete in 266m 5s

Epoch 52/99
----------
train Loss: 0.0440 Acc: 0.9971
val Loss: 0.0821 Acc: 0.9758
Training complete in 271m 21s

Epoch 53/99
----------
train Loss: 0.0410 Acc: 0.9975
val Loss: 0.0780 Acc: 0.9729
Training complete in 276m 31s

Epoch 54/99
----------
train Loss: 0.0425 Acc: 0.9972
val Loss: 0.0830 Acc: 0.9772
Training complete in 281m 44s

Epoch 55/99
----------
train Loss: 0.0423 Acc: 0.9970
val Loss: 0.0765 Acc: 0.9772
Training complete in 286m 54s

Epoch 56/99
----------
train Loss: 0.0456 Acc: 0.9971
val Loss: 0.0783 Acc: 0.9744
Training complete in 292m 6s

Epoch 57/99
----------
train Loss: 0.0422 Acc: 0.9974
val Loss: 0.0760 Acc: 0.9801
Training complete in 297m 17s

Epoch 58/99
----------
train Loss: 0.0404 Acc: 0.9977
val Loss: 0.0747 Acc: 0.9786
Training complete in 302m 29s

Epoch 59/99
----------
train Loss: 0.0386 Acc: 0.9978
val Loss: 0.0706 Acc: 0.9801
Training complete in 307m 47s

Epoch 60/99
----------
train Loss: 0.0393 Acc: 0.9976
val Loss: 0.0719 Acc: 0.9801
Training complete in 313m 3s

Epoch 61/99
----------
train Loss: 0.0386 Acc: 0.9980
val Loss: 0.0683 Acc: 0.9772
Training complete in 318m 16s

Epoch 62/99
----------
train Loss: 0.0401 Acc: 0.9977
val Loss: 0.0774 Acc: 0.9772
Training complete in 323m 27s

Epoch 63/99
----------
train Loss: 0.0382 Acc: 0.9974
val Loss: 0.0766 Acc: 0.9801
Training complete in 328m 35s

Epoch 64/99
----------
train Loss: 0.0381 Acc: 0.9981
val Loss: 0.0801 Acc: 0.9772
Training complete in 333m 40s

Epoch 65/99
----------
train Loss: 0.0387 Acc: 0.9977
val Loss: 0.0761 Acc: 0.9772
Training complete in 338m 45s

Epoch 66/99
----------
train Loss: 0.0380 Acc: 0.9977
val Loss: 0.0741 Acc: 0.9758
Training complete in 343m 49s

Epoch 67/99
----------
train Loss: 0.0368 Acc: 0.9985
val Loss: 0.0721 Acc: 0.9815
Training complete in 348m 56s

Epoch 68/99
----------
train Loss: 0.0380 Acc: 0.9976
val Loss: 0.0750 Acc: 0.9786
Training complete in 354m 4s

Epoch 69/99
----------
train Loss: 0.0370 Acc: 0.9985
val Loss: 0.0807 Acc: 0.9758
Training complete in 359m 12s

Epoch 70/99
----------
train Loss: 0.0355 Acc: 0.9986
val Loss: 0.0798 Acc: 0.9786
Training complete in 364m 23s

Epoch 71/99
----------
train Loss: 0.0365 Acc: 0.9982
val Loss: 0.0780 Acc: 0.9772
Training complete in 369m 29s

Epoch 72/99
----------
train Loss: 0.0364 Acc: 0.9985
val Loss: 0.0899 Acc: 0.9729
Training complete in 374m 38s

Epoch 73/99
----------
train Loss: 0.0362 Acc: 0.9980
val Loss: 0.0803 Acc: 0.9758
Training complete in 379m 50s

Epoch 74/99
----------
train Loss: 0.0357 Acc: 0.9984
val Loss: 0.0757 Acc: 0.9772
Training complete in 385m 2s

Epoch 75/99
----------
train Loss: 0.0363 Acc: 0.9984
val Loss: 0.0773 Acc: 0.9772
Training complete in 390m 17s

Epoch 76/99
----------
train Loss: 0.0358 Acc: 0.9982
val Loss: 0.0728 Acc: 0.9758
Training complete in 395m 30s

Epoch 77/99
----------
train Loss: 0.0365 Acc: 0.9981
val Loss: 0.0701 Acc: 0.9758
Training complete in 400m 38s

Epoch 78/99
----------
train Loss: 0.0352 Acc: 0.9986
val Loss: 0.0725 Acc: 0.9772
Training complete in 405m 50s

Epoch 79/99
----------
train Loss: 0.0359 Acc: 0.9985
val Loss: 0.0802 Acc: 0.9758
Training complete in 411m 3s

Epoch 80/99
----------
train Loss: 0.0336 Acc: 0.9985
val Loss: 0.0746 Acc: 0.9786
Training complete in 416m 14s

Epoch 81/99
----------
train Loss: 0.0348 Acc: 0.9984
val Loss: 0.0767 Acc: 0.9772
Training complete in 421m 20s

Epoch 82/99
----------
train Loss: 0.0340 Acc: 0.9985
val Loss: 0.0713 Acc: 0.9786
Training complete in 426m 27s

Epoch 83/99
----------
train Loss: 0.0343 Acc: 0.9984
val Loss: 0.0773 Acc: 0.9758
Training complete in 431m 36s

Epoch 84/99
----------
train Loss: 0.0334 Acc: 0.9985
val Loss: 0.0757 Acc: 0.9744
Training complete in 436m 36s

Epoch 85/99
----------
train Loss: 0.0351 Acc: 0.9987
val Loss: 0.0738 Acc: 0.9758
Training complete in 441m 39s

Epoch 86/99
----------
train Loss: 0.0330 Acc: 0.9987
val Loss: 0.0755 Acc: 0.9758
Training complete in 446m 40s

Epoch 87/99
----------
train Loss: 0.0357 Acc: 0.9979
val Loss: 0.0710 Acc: 0.9815
Training complete in 451m 10s

Epoch 88/99
----------
train Loss: 0.0353 Acc: 0.9985
val Loss: 0.0708 Acc: 0.9772
Training complete in 455m 59s

Epoch 89/99
----------
train Loss: 0.0323 Acc: 0.9990
val Loss: 0.0763 Acc: 0.9772
Training complete in 460m 53s

Epoch 90/99
----------
train Loss: 0.0324 Acc: 0.9988
val Loss: 0.0699 Acc: 0.9758
Training complete in 465m 19s

Epoch 91/99
----------
train Loss: 0.0331 Acc: 0.9982
val Loss: 0.0742 Acc: 0.9772
Training complete in 470m 24s

Epoch 92/99
----------
train Loss: 0.0325 Acc: 0.9989
val Loss: 0.0742 Acc: 0.9758
Training complete in 475m 22s

Epoch 93/99
----------
train Loss: 0.0322 Acc: 0.9988
val Loss: 0.0746 Acc: 0.9786
Training complete in 480m 21s

Epoch 94/99
----------
train Loss: 0.0336 Acc: 0.9985
val Loss: 0.0797 Acc: 0.9772
Training complete in 485m 19s

Epoch 95/99
----------
train Loss: 0.0320 Acc: 0.9989
val Loss: 0.0768 Acc: 0.9758
Training complete in 490m 14s

Epoch 96/99
----------
train Loss: 0.0324 Acc: 0.9989
val Loss: 0.0728 Acc: 0.9772
Training complete in 495m 7s

Epoch 97/99
----------
train Loss: 0.0327 Acc: 0.9987
val Loss: 0.0771 Acc: 0.9758
Training complete in 499m 39s

Epoch 98/99
----------
train Loss: 0.0309 Acc: 0.9989
val Loss: 0.0778 Acc: 0.9786
Training complete in 505m 0s

Epoch 99/99
----------
train Loss: 0.0325 Acc: 0.9987
val Loss: 0.0743 Acc: 0.9772
Training complete in 509m 49s

Training complete in 509m 49s
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7fec1debe978>]
2.502086877822876
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (nsrelu): NSReLU()
  (Sum_ReLU_2): Sum_ReLU_2()
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/99
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 0.9044 Acc: 0.1492
val Loss: 3.2124 Acc: 0.3291
Training complete in 3m 39s

Epoch 1/99
----------
train Loss: 0.9210 Acc: 0.4507
val Loss: 1.3105 Acc: 0.6624
Training complete in 7m 11s

Epoch 2/99
----------
train Loss: 0.9470 Acc: 0.5934
val Loss: 0.8276 Acc: 0.7749
Training complete in 10m 28s

Epoch 3/99
----------
train Loss: 1.0275 Acc: 0.6587
val Loss: 0.6769 Acc: 0.7963
Training complete in 13m 49s

Epoch 4/99
----------
train Loss: 1.0947 Acc: 0.7064
val Loss: 0.5157 Acc: 0.8689
Training complete in 16m 59s

Epoch 5/99
----------
train Loss: 1.0154 Acc: 0.7489
val Loss: 0.3917 Acc: 0.8960
Training complete in 19m 46s

Epoch 6/99
----------
train Loss: 0.7789 Acc: 0.8049
val Loss: 0.3371 Acc: 0.8989
Training complete in 23m 22s

Epoch 7/99
----------
train Loss: 0.6316 Acc: 0.8386
val Loss: 0.2556 Acc: 0.9145
Training complete in 26m 24s

Epoch 8/99
----------
train Loss: 0.5317 Acc: 0.8660
val Loss: 0.2092 Acc: 0.9416
Training complete in 30m 2s

Epoch 9/99
----------
train Loss: 0.4758 Acc: 0.8777
val Loss: 0.1867 Acc: 0.9473
Training complete in 33m 6s

Epoch 10/99
----------
train Loss: 0.4051 Acc: 0.8948
val Loss: 0.2100 Acc: 0.9402
Training complete in 36m 40s

Epoch 11/99
----------
train Loss: 0.3533 Acc: 0.9104
val Loss: 0.1659 Acc: 0.9516
Training complete in 39m 60s

Epoch 12/99
----------
train Loss: 0.3208 Acc: 0.9163
val Loss: 0.1787 Acc: 0.9516
Training complete in 43m 2s

Epoch 13/99
----------
train Loss: 0.2816 Acc: 0.9316
val Loss: 0.1330 Acc: 0.9544
Training complete in 46m 22s

Epoch 14/99
----------
train Loss: 0.2472 Acc: 0.9410
val Loss: 0.1484 Acc: 0.9544
Training complete in 49m 38s

Epoch 15/99
----------
train Loss: 0.2192 Acc: 0.9490
val Loss: 0.1385 Acc: 0.9601
Training complete in 52m 29s

Epoch 16/99
----------
train Loss: 0.1960 Acc: 0.9536
val Loss: 0.1114 Acc: 0.9630
Training complete in 56m 1s

Epoch 17/99
----------
train Loss: 0.1787 Acc: 0.9604
val Loss: 0.1133 Acc: 0.9687
Training complete in 59m 27s

Epoch 18/99
----------
train Loss: 0.1824 Acc: 0.9585
val Loss: 0.1223 Acc: 0.9672
Training complete in 63m 3s

Epoch 19/99
----------
train Loss: 0.1608 Acc: 0.9648
val Loss: 0.1132 Acc: 0.9658
Training complete in 66m 31s

Epoch 20/99
----------
train Loss: 0.1443 Acc: 0.9702
val Loss: 0.1145 Acc: 0.9644
Training complete in 70m 5s

Epoch 21/99
----------
train Loss: 0.1231 Acc: 0.9761
val Loss: 0.0965 Acc: 0.9729
Training complete in 73m 29s

Epoch 22/99
----------
train Loss: 0.1169 Acc: 0.9784
val Loss: 0.0867 Acc: 0.9715
Training complete in 76m 31s

Epoch 23/99
----------
train Loss: 0.1192 Acc: 0.9771
val Loss: 0.1096 Acc: 0.9630
Training complete in 79m 47s

Epoch 24/99
----------
train Loss: 0.1110 Acc: 0.9803
val Loss: 0.0887 Acc: 0.9772
Training complete in 83m 15s

Epoch 25/99
----------
train Loss: 0.1034 Acc: 0.9814
val Loss: 0.0982 Acc: 0.9701
Training complete in 86m 18s

Epoch 26/99
----------
train Loss: 0.0891 Acc: 0.9857
val Loss: 0.0955 Acc: 0.9715
Training complete in 89m 26s

Epoch 27/99
----------
train Loss: 0.0786 Acc: 0.9884
val Loss: 0.0806 Acc: 0.9758
Training complete in 92m 30s

Epoch 28/99
----------
train Loss: 0.0734 Acc: 0.9905
val Loss: 0.0748 Acc: 0.9744
Training complete in 96m 4s

Epoch 29/99
----------
train Loss: 0.0797 Acc: 0.9894
val Loss: 0.0635 Acc: 0.9786
Training complete in 99m 49s

Epoch 30/99
----------
train Loss: 0.0729 Acc: 0.9894
val Loss: 0.0732 Acc: 0.9687
Training complete in 103m 24s

Epoch 31/99
----------
train Loss: 0.0677 Acc: 0.9917
val Loss: 0.0694 Acc: 0.9758
Training complete in 106m 22s

Epoch 32/99
----------
train Loss: 0.0713 Acc: 0.9910
val Loss: 0.0870 Acc: 0.9715
Training complete in 109m 50s

Epoch 33/99
----------
train Loss: 0.0681 Acc: 0.9920
val Loss: 0.0839 Acc: 0.9758
Training complete in 113m 2s

Epoch 34/99
----------
train Loss: 0.0599 Acc: 0.9925
val Loss: 0.0893 Acc: 0.9687
Training complete in 116m 33s

Epoch 35/99
----------
train Loss: 0.0623 Acc: 0.9934
val Loss: 0.0946 Acc: 0.9715
Training complete in 119m 39s

Epoch 36/99
----------
train Loss: 0.0640 Acc: 0.9928
val Loss: 0.0838 Acc: 0.9715
Training complete in 122m 43s

Epoch 37/99
----------
train Loss: 0.0549 Acc: 0.9941
val Loss: 0.0893 Acc: 0.9758
Training complete in 126m 18s

Epoch 38/99
----------
train Loss: 0.0556 Acc: 0.9945
val Loss: 0.0683 Acc: 0.9772
Training complete in 129m 21s

Epoch 39/99
----------
train Loss: 0.0518 Acc: 0.9963
val Loss: 0.0712 Acc: 0.9829
Training complete in 132m 24s

Epoch 40/99
----------
train Loss: 0.0507 Acc: 0.9957
val Loss: 0.0872 Acc: 0.9786
Training complete in 135m 59s

Epoch 41/99
----------
train Loss: 0.0551 Acc: 0.9949
val Loss: 0.0845 Acc: 0.9715
Training complete in 139m 30s

Epoch 42/99
----------
train Loss: 0.0504 Acc: 0.9960
val Loss: 0.0893 Acc: 0.9715
Training complete in 142m 32s

Epoch 43/99
----------
train Loss: 0.0502 Acc: 0.9954
val Loss: 0.0737 Acc: 0.9801
Training complete in 145m 34s

Epoch 44/99
----------
train Loss: 0.0515 Acc: 0.9959
val Loss: 0.0747 Acc: 0.9758
Training complete in 148m 37s

Epoch 45/99
----------
train Loss: 0.0474 Acc: 0.9965
val Loss: 0.0703 Acc: 0.9744
Training complete in 152m 11s

Epoch 46/99
----------
train Loss: 0.0471 Acc: 0.9967
val Loss: 0.0843 Acc: 0.9786
Training complete in 155m 45s

Epoch 47/99
----------
train Loss: 0.0487 Acc: 0.9961
val Loss: 0.0761 Acc: 0.9772
Training complete in 159m 5s

Epoch 48/99
----------
train Loss: 0.0495 Acc: 0.9962
val Loss: 0.0815 Acc: 0.9786
Training complete in 162m 30s

Epoch 49/99
----------
train Loss: 0.0470 Acc: 0.9964
val Loss: 0.0790 Acc: 0.9744
Training complete in 166m 5s

Epoch 50/99
----------
train Loss: 0.0459 Acc: 0.9967
val Loss: 0.0811 Acc: 0.9758
Training complete in 169m 26s

Epoch 51/99
----------
train Loss: 0.0449 Acc: 0.9968
val Loss: 0.0789 Acc: 0.9744
Training complete in 172m 29s

Epoch 52/99
----------
train Loss: 0.0429 Acc: 0.9973
val Loss: 0.0850 Acc: 0.9772
Training complete in 175m 32s

Epoch 53/99
----------
train Loss: 0.0435 Acc: 0.9969
val Loss: 0.0838 Acc: 0.9758
Training complete in 178m 59s

Epoch 54/99
----------
train Loss: 0.0457 Acc: 0.9968
val Loss: 0.0783 Acc: 0.9758
Training complete in 182m 32s

Epoch 55/99
----------
train Loss: 0.0414 Acc: 0.9975
val Loss: 0.0831 Acc: 0.9729
Training complete in 186m 5s

Epoch 56/99
----------
train Loss: 0.0413 Acc: 0.9974
val Loss: 0.0800 Acc: 0.9786
Training complete in 189m 31s

Epoch 57/99
----------
train Loss: 0.0443 Acc: 0.9968
val Loss: 0.0814 Acc: 0.9744
Training complete in 192m 32s

Epoch 58/99
----------
train Loss: 0.0380 Acc: 0.9980
val Loss: 0.0887 Acc: 0.9729
Training complete in 196m 2s

Epoch 59/99
----------
train Loss: 0.0395 Acc: 0.9977
val Loss: 0.0783 Acc: 0.9758
Training complete in 199m 35s

Epoch 60/99
----------
train Loss: 0.0371 Acc: 0.9982
val Loss: 0.0932 Acc: 0.9758
Training complete in 202m 31s

Epoch 61/99
----------
train Loss: 0.0378 Acc: 0.9979
val Loss: 0.0866 Acc: 0.9758
Training complete in 206m 7s

Epoch 62/99
----------
train Loss: 0.0370 Acc: 0.9984
val Loss: 0.0756 Acc: 0.9772
Training complete in 209m 42s

Epoch 63/99
----------
train Loss: 0.0384 Acc: 0.9980
val Loss: 0.0884 Acc: 0.9744
Training complete in 212m 49s

Epoch 64/99
----------
train Loss: 0.0399 Acc: 0.9977
val Loss: 0.0820 Acc: 0.9758
Training complete in 216m 26s

Epoch 65/99
----------
train Loss: 0.0404 Acc: 0.9975
val Loss: 0.0824 Acc: 0.9772
Training complete in 219m 30s

Epoch 66/99
----------
train Loss: 0.0403 Acc: 0.9979
val Loss: 0.0809 Acc: 0.9758
Training complete in 222m 49s

Epoch 67/99
----------
train Loss: 0.0387 Acc: 0.9981
val Loss: 0.0773 Acc: 0.9772
Training complete in 226m 12s

Epoch 68/99
----------
train Loss: 0.0365 Acc: 0.9988
val Loss: 0.0860 Acc: 0.9758
Training complete in 229m 55s

Epoch 69/99
----------
train Loss: 0.0370 Acc: 0.9985
val Loss: 0.0780 Acc: 0.9758
Training complete in 232m 58s

Epoch 70/99
----------
train Loss: 0.0378 Acc: 0.9976
val Loss: 0.0890 Acc: 0.9729
Training complete in 236m 20s

Epoch 71/99
----------
train Loss: 0.0355 Acc: 0.9981
val Loss: 0.0867 Acc: 0.9744
Training complete in 239m 7s

Epoch 72/99
----------
train Loss: 0.0360 Acc: 0.9986
val Loss: 0.0813 Acc: 0.9729
Training complete in 242m 29s

Epoch 73/99
----------
train Loss: 0.0342 Acc: 0.9985
val Loss: 0.0934 Acc: 0.9744
Training complete in 245m 44s

Epoch 74/99
----------
train Loss: 0.0363 Acc: 0.9982
val Loss: 0.0815 Acc: 0.9772
Training complete in 248m 59s

Epoch 75/99
----------
train Loss: 0.0363 Acc: 0.9984
val Loss: 0.0841 Acc: 0.9772
Training complete in 252m 31s

Epoch 76/99
----------
train Loss: 0.0368 Acc: 0.9980
val Loss: 0.0810 Acc: 0.9801
Training complete in 256m 5s

Epoch 77/99
----------
train Loss: 0.0346 Acc: 0.9984
val Loss: 0.0871 Acc: 0.9772
Training complete in 259m 30s

Epoch 78/99
----------
train Loss: 0.0343 Acc: 0.9988
val Loss: 0.0836 Acc: 0.9772
Training complete in 262m 47s

Epoch 79/99
----------
train Loss: 0.0351 Acc: 0.9983
val Loss: 0.0793 Acc: 0.9758
Training complete in 266m 26s

Epoch 80/99
----------
train Loss: 0.0351 Acc: 0.9989
val Loss: 0.0786 Acc: 0.9744
Training complete in 270m 1s

Epoch 81/99
----------
train Loss: 0.0352 Acc: 0.9980
val Loss: 0.0848 Acc: 0.9744
Training complete in 273m 36s

Epoch 82/99
----------
train Loss: 0.0342 Acc: 0.9985
val Loss: 0.0850 Acc: 0.9744
Training complete in 276m 39s

Epoch 83/99
----------
train Loss: 0.0341 Acc: 0.9984
val Loss: 0.0881 Acc: 0.9744
Training complete in 279m 43s

Epoch 84/99
----------
train Loss: 0.0335 Acc: 0.9991
val Loss: 0.0936 Acc: 0.9758
Training complete in 282m 46s

Epoch 85/99
----------
train Loss: 0.0337 Acc: 0.9987
val Loss: 0.0868 Acc: 0.9772
Training complete in 286m 7s

Epoch 86/99
----------
train Loss: 0.0341 Acc: 0.9988
val Loss: 0.0873 Acc: 0.9744
Training complete in 289m 10s

Epoch 87/99
----------
train Loss: 0.0334 Acc: 0.9985
val Loss: 0.0878 Acc: 0.9729
Training complete in 292m 42s

Epoch 88/99
----------
train Loss: 0.0342 Acc: 0.9988
val Loss: 0.0848 Acc: 0.9729
Training complete in 295m 37s

Epoch 89/99
----------
train Loss: 0.0334 Acc: 0.9989
val Loss: 0.0830 Acc: 0.9758
Training complete in 299m 13s

Epoch 90/99
----------
train Loss: 0.0329 Acc: 0.9991
val Loss: 0.0859 Acc: 0.9758
Training complete in 302m 46s

Epoch 91/99
----------
train Loss: 0.0331 Acc: 0.9987
val Loss: 0.0957 Acc: 0.9729
Training complete in 306m 23s

Epoch 92/99
----------
train Loss: 0.0329 Acc: 0.9989
val Loss: 0.0845 Acc: 0.9729
Training complete in 309m 40s

Epoch 93/99
----------
train Loss: 0.0332 Acc: 0.9987
val Loss: 0.0861 Acc: 0.9744
Training complete in 313m 2s

Epoch 94/99
----------
train Loss: 0.0332 Acc: 0.9986
val Loss: 0.0946 Acc: 0.9758
Training complete in 316m 13s

Epoch 95/99
----------
train Loss: 0.0341 Acc: 0.9985
val Loss: 0.0862 Acc: 0.9758
Training complete in 319m 56s

Epoch 96/99
----------
train Loss: 0.0318 Acc: 0.9989
val Loss: 0.0844 Acc: 0.9786
Training complete in 323m 1s

Epoch 97/99
----------
We use the scale: 1
-------test-----------
Traceback (most recent call last):
  File "test.py", line 253, in <module>
    model = model.cuda()
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/module.py", line 305, in cuda
    return self._apply(lambda t: t.cuda(device))
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/module.py", line 202, in _apply
    module._apply(fn)
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/module.py", line 202, in _apply
    module._apply(fn)
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/module.py", line 202, in _apply
    module._apply(fn)
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/module.py", line 224, in _apply
    param_applied = fn(param)
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/module.py", line 305, in <lambda>
    return self._apply(lambda t: t.cuda(device))
KeyboardInterrupt
train Loss: 0.0334 Acc: 0.9984
val Loss: 0.0876 Acc: 0.9744
Training complete in 327m 50s

Epoch 98/99
----------
We use the scale: 1
-------test-----------
8
16
24
32
40
48
56
64
72
80
88
96
104
112
120
128
136
144
152
160
168
176
184
192
200
208
216
224
232
240
248
256
264
272
280
288
296
304
312
320
328
336
344
352
360
368
376
384
392
400
408
416
424
432
440
448
456
464
472
480
488
496
504
512
520
528
536
544
552
560
568
576
584
592
600
608
616
624
632
640
648
656
664
672
680
688
696
704
712
720
728
736
744
752
760
768
776
784
792
800
808
816
824
832
840
848
856
864
872
880
888
896
904
912
920
928
936
944
952
960
968
976
984
992
1000
1008
1016
1024
1032
1040
1048
1056
1064
1072
1080
1088
1096
1104
1112
1120
1128
1136
1144
1152
1160
1168
1176
1184
1192
1200
1208
1216
1224
1232
1240
1248
1256
1264
1272
1280
1288
1296
1304
1312
1320
1328
1336
1344
1352
1360
1368
1376
1384
1392
1400
1408
1416
1424
1432
1440
1448
1456
1464
1472
1480
1488
1496
1504
1512
1520
1528
1536
1544
1552
1560
1568
1576
1584
1592
1600
1608
1616
1624
1632
1640
1648
1656
1664
1672
1680
1688
1696
1704
1712
1720
1728
1736
1744
1752
1760
1768
1776
1784
1792
1800
1808
1816
1824
1832
1840
1848
1856
1864
1872
1880
1888
1896
1904
1912
1920
1928
1936
1944
1952
1960
1968
1976
1984
1992
2000
2008
2016
2024
2032
2040
2048
2056
2064
2072
2080
2088
2096
2104
2112
2120
2128
2136
2144
2152
2160
2168
2176
2184
2192
2200
2208
2216
2224
2232
2240
2248
2256
2264
2272
2280
2288
2296
2304
2312
2320
Traceback (most recent call last):
  File "test.py", line 257, in <module>
    gallery_feature = extract_feature(model,dataloaders['gallery'])
  File "test.py", line 184, in extract_feature
    features = torch.cat((features,ff.data.cpu()), 0)
KeyboardInterrupt
train Loss: 0.0329 Acc: 0.9989
val Loss: 0.0846 Acc: 0.9744
Training complete in 332m 39s

Epoch 99/99
----------
train Loss: 0.0332 Acc: 0.9984
val Loss: 0.0839 Acc: 0.9772
Training complete in 336m 14s

Training complete in 336m 14s
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7f3acbb6da58>]
2.623077392578125
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (nsrelu): NSReLU()
  (Sum_ReLU_2): Sum_ReLU_2()
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/99
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 0.9002 Acc: 0.1527
val Loss: 2.9744 Acc: 0.3632
Training complete in 4m 50s

Epoch 1/99
----------
train Loss: 0.8591 Acc: 0.4936
val Loss: 1.1919 Acc: 0.6866
Training complete in 9m 16s

Epoch 2/99
----------
train Loss: 0.9092 Acc: 0.6088
val Loss: 0.7989 Acc: 0.7920
Training complete in 13m 43s

Epoch 3/99
----------
train Loss: 0.9911 Acc: 0.6695
val Loss: 0.6891 Acc: 0.8291
Training complete in 18m 8s

Epoch 4/99
----------
train Loss: 1.0859 Acc: 0.7108
val Loss: 0.5271 Acc: 0.8604
Training complete in 22m 39s

Epoch 5/99
----------
train Loss: 0.9747 Acc: 0.7599
val Loss: 0.3952 Acc: 0.8818
Training complete in 27m 3s

Epoch 6/99
----------
train Loss: 0.7450 Acc: 0.8161
val Loss: 0.2596 Acc: 0.9302
Training complete in 31m 33s

Epoch 7/99
----------
train Loss: 0.5992 Acc: 0.8485
val Loss: 0.2385 Acc: 0.9316
Training complete in 36m 9s

Epoch 8/99
----------
train Loss: 0.5018 Acc: 0.8727
val Loss: 0.2371 Acc: 0.9288
Training complete in 40m 35s

Epoch 9/99
----------
train Loss: 0.4541 Acc: 0.8840
val Loss: 0.2121 Acc: 0.9459
Training complete in 45m 12s

Epoch 10/99
----------
train Loss: 0.3721 Acc: 0.9073
val Loss: 0.1889 Acc: 0.9459
Training complete in 49m 44s

Epoch 11/99
----------
train Loss: 0.3460 Acc: 0.9113
val Loss: 0.1684 Acc: 0.9430
Training complete in 54m 17s

Epoch 12/99
----------
train Loss: 0.3001 Acc: 0.9212
val Loss: 0.1546 Acc: 0.9516
Training complete in 58m 52s

Epoch 13/99
----------
train Loss: 0.2786 Acc: 0.9310
val Loss: 0.1193 Acc: 0.9615
Training complete in 63m 22s

Epoch 14/99
----------
train Loss: 0.2468 Acc: 0.9394
val Loss: 0.1308 Acc: 0.9630
Training complete in 67m 50s

Epoch 15/99
----------
train Loss: 0.2109 Acc: 0.9493
val Loss: 0.1368 Acc: 0.9601
Training complete in 72m 24s

Epoch 16/99
----------
train Loss: 0.2001 Acc: 0.9528
val Loss: 0.1129 Acc: 0.9658
Training complete in 76m 56s

Epoch 17/99
----------
train Loss: 0.1904 Acc: 0.9566
val Loss: 0.1323 Acc: 0.9630
Training complete in 81m 25s

Epoch 18/99
----------
train Loss: 0.1549 Acc: 0.9636
val Loss: 0.1192 Acc: 0.9644
Training complete in 85m 54s

Epoch 19/99
----------
train Loss: 0.1497 Acc: 0.9676
val Loss: 0.1171 Acc: 0.9672
Training complete in 90m 25s

Epoch 20/99
----------
train Loss: 0.1383 Acc: 0.9705
val Loss: 0.0975 Acc: 0.9630
Training complete in 94m 60s

Epoch 21/99
----------
train Loss: 0.1275 Acc: 0.9753
val Loss: 0.0864 Acc: 0.9701
Training complete in 99m 29s

Epoch 22/99
----------
train Loss: 0.1082 Acc: 0.9795
val Loss: 0.1036 Acc: 0.9687
Training complete in 104m 7s

Epoch 23/99
----------
train Loss: 0.1092 Acc: 0.9811
val Loss: 0.0811 Acc: 0.9729
Training complete in 108m 37s

Epoch 24/99
----------
train Loss: 0.0988 Acc: 0.9827
val Loss: 0.1005 Acc: 0.9729
Training complete in 113m 9s

Epoch 25/99
----------
train Loss: 0.0946 Acc: 0.9841
val Loss: 0.0912 Acc: 0.9715
Training complete in 117m 38s

Epoch 26/99
----------
train Loss: 0.0867 Acc: 0.9849
val Loss: 0.0892 Acc: 0.9715
Training complete in 122m 8s

Epoch 27/99
----------
train Loss: 0.0785 Acc: 0.9881
val Loss: 0.0936 Acc: 0.9715
Training complete in 126m 41s

Epoch 28/99
----------
train Loss: 0.0767 Acc: 0.9886
val Loss: 0.0904 Acc: 0.9772
Training complete in 131m 14s

Epoch 29/99
----------
train Loss: 0.0739 Acc: 0.9906
val Loss: 0.0865 Acc: 0.9772
Training complete in 135m 42s

Epoch 30/99
----------
train Loss: 0.0695 Acc: 0.9905
val Loss: 0.0718 Acc: 0.9801
Training complete in 140m 9s

Epoch 31/99
----------
train Loss: 0.0665 Acc: 0.9915
val Loss: 0.0899 Acc: 0.9729
Training complete in 145m 8s

Epoch 32/99
----------
train Loss: 0.0688 Acc: 0.9906
val Loss: 0.0806 Acc: 0.9786
Training complete in 153m 44s

Epoch 33/99
----------
train Loss: 0.0673 Acc: 0.9918
val Loss: 0.0766 Acc: 0.9758
Training complete in 162m 25s

Epoch 34/99
----------
train Loss: 0.0634 Acc: 0.9925
val Loss: 0.0834 Acc: 0.9744
Training complete in 171m 8s

Epoch 35/99
----------
train Loss: 0.0583 Acc: 0.9930
val Loss: 0.0788 Acc: 0.9758
Training complete in 179m 49s

Epoch 36/99
----------
train Loss: 0.0604 Acc: 0.9933
val Loss: 0.0842 Acc: 0.9744
Training complete in 188m 31s

Epoch 37/99
----------
train Loss: 0.0568 Acc: 0.9938
val Loss: 0.0927 Acc: 0.9715
Training complete in 197m 12s

Epoch 38/99
----------
train Loss: 0.0547 Acc: 0.9950
val Loss: 0.0779 Acc: 0.9801
Training complete in 205m 54s

Epoch 39/99
----------
train Loss: 0.0530 Acc: 0.9944
val Loss: 0.0990 Acc: 0.9715
Training complete in 214m 36s

Epoch 40/99
----------
train Loss: 0.0528 Acc: 0.9955
val Loss: 0.0672 Acc: 0.9744
Training complete in 223m 18s

Epoch 41/99
----------
train Loss: 0.0521 Acc: 0.9955
val Loss: 0.0913 Acc: 0.9715
Training complete in 232m 2s

Epoch 42/99
----------
train Loss: 0.0501 Acc: 0.9954
val Loss: 0.0798 Acc: 0.9729
Training complete in 240m 44s

Epoch 43/99
----------
train Loss: 0.0481 Acc: 0.9958
val Loss: 0.0888 Acc: 0.9729
Training complete in 249m 27s

Epoch 44/99
----------
train Loss: 0.0479 Acc: 0.9962
val Loss: 0.0779 Acc: 0.9758
Training complete in 258m 10s

Epoch 45/99
----------
train Loss: 0.0472 Acc: 0.9960
val Loss: 0.0856 Acc: 0.9729
Training complete in 266m 54s

Epoch 46/99
----------
train Loss: 0.0485 Acc: 0.9960
val Loss: 0.0826 Acc: 0.9715
Training complete in 275m 36s

Epoch 47/99
----------
train Loss: 0.0482 Acc: 0.9954
val Loss: 0.0856 Acc: 0.9729
Training complete in 284m 19s

Epoch 48/99
----------
train Loss: 0.0420 Acc: 0.9973
val Loss: 0.0840 Acc: 0.9715
Training complete in 293m 1s

Epoch 49/99
----------
train Loss: 0.0496 Acc: 0.9956
val Loss: 0.0832 Acc: 0.9758
Training complete in 301m 46s

Epoch 50/99
----------
train Loss: 0.0430 Acc: 0.9973
val Loss: 0.0688 Acc: 0.9801
Training complete in 310m 28s

Epoch 51/99
----------
train Loss: 0.0409 Acc: 0.9975
val Loss: 0.0758 Acc: 0.9801
Training complete in 319m 10s

Epoch 52/99
----------
train Loss: 0.0381 Acc: 0.9976
val Loss: 0.0867 Acc: 0.9786
Training complete in 327m 51s

Epoch 53/99
----------
train Loss: 0.0408 Acc: 0.9972
val Loss: 0.0916 Acc: 0.9729
Training complete in 336m 32s

Epoch 54/99
----------
train Loss: 0.0399 Acc: 0.9978
val Loss: 0.0773 Acc: 0.9758
Training complete in 345m 14s

Epoch 55/99
----------
train Loss: 0.0396 Acc: 0.9975
val Loss: 0.0760 Acc: 0.9815
Training complete in 353m 53s

Epoch 56/99
----------
train Loss: 0.0426 Acc: 0.9970
val Loss: 0.0799 Acc: 0.9801
Training complete in 362m 34s

Epoch 57/99
----------
train Loss: 0.0380 Acc: 0.9980
val Loss: 0.0689 Acc: 0.9758
Training complete in 371m 13s

Epoch 58/99
----------
train Loss: 0.0401 Acc: 0.9973
val Loss: 0.0826 Acc: 0.9786
Training complete in 379m 54s

Epoch 59/99
----------
train Loss: 0.0394 Acc: 0.9973
val Loss: 0.0835 Acc: 0.9744
Training complete in 388m 33s

Epoch 60/99
----------
train Loss: 0.0389 Acc: 0.9975
val Loss: 0.0845 Acc: 0.9786
Training complete in 397m 13s

Epoch 61/99
----------
train Loss: 0.0384 Acc: 0.9978
val Loss: 0.0698 Acc: 0.9829
Training complete in 405m 52s

Epoch 62/99
----------
train Loss: 0.0375 Acc: 0.9979
val Loss: 0.0814 Acc: 0.9772
Training complete in 414m 33s

Epoch 63/99
----------
train Loss: 0.0379 Acc: 0.9982
val Loss: 0.0766 Acc: 0.9786
Training complete in 423m 13s

Epoch 64/99
----------
train Loss: 0.0377 Acc: 0.9976
val Loss: 0.0825 Acc: 0.9758
Training complete in 431m 58s

Epoch 65/99
----------
train Loss: 0.0359 Acc: 0.9980
val Loss: 0.0860 Acc: 0.9729
Training complete in 440m 39s

Epoch 66/99
----------
train Loss: 0.0361 Acc: 0.9980
val Loss: 0.0802 Acc: 0.9786
Training complete in 449m 21s

Epoch 67/99
----------
train Loss: 0.0340 Acc: 0.9984
val Loss: 0.0720 Acc: 0.9801
Training complete in 457m 60s

Epoch 68/99
----------
train Loss: 0.0358 Acc: 0.9979
val Loss: 0.0818 Acc: 0.9786
Training complete in 466m 41s

Epoch 69/99
----------
train Loss: 0.0356 Acc: 0.9985
val Loss: 0.0862 Acc: 0.9801
Training complete in 475m 21s

Epoch 70/99
----------
train Loss: 0.0348 Acc: 0.9981
val Loss: 0.0832 Acc: 0.9786
Training complete in 483m 60s

Epoch 71/99
----------
train Loss: 0.0355 Acc: 0.9983
val Loss: 0.0809 Acc: 0.9758
Training complete in 490m 11s

Epoch 72/99
----------
train Loss: 0.0347 Acc: 0.9982
val Loss: 0.0799 Acc: 0.9786
Training complete in 494m 48s

Epoch 73/99
----------
train Loss: 0.0351 Acc: 0.9984
val Loss: 0.0858 Acc: 0.9786
Training complete in 499m 23s

Epoch 74/99
----------
train Loss: 0.0339 Acc: 0.9985
val Loss: 0.0786 Acc: 0.9829
Training complete in 503m 53s

Epoch 75/99
----------
train Loss: 0.0361 Acc: 0.9984
val Loss: 0.0816 Acc: 0.9801
Training complete in 508m 33s

Epoch 76/99
----------
train Loss: 0.0338 Acc: 0.9984
val Loss: 0.0825 Acc: 0.9801
Training complete in 513m 5s

Epoch 77/99
----------
train Loss: 0.0335 Acc: 0.9983
val Loss: 0.0782 Acc: 0.9786
Training complete in 517m 32s

Epoch 78/99
----------
train Loss: 0.0350 Acc: 0.9980
val Loss: 0.0769 Acc: 0.9801
Training complete in 522m 2s

Epoch 79/99
----------
train Loss: 0.0338 Acc: 0.9980
val Loss: 0.0799 Acc: 0.9772
Training complete in 526m 33s

Epoch 80/99
----------
train Loss: 0.0334 Acc: 0.9985
val Loss: 0.0767 Acc: 0.9786
Training complete in 531m 4s

Epoch 81/99
----------
train Loss: 0.0339 Acc: 0.9986
val Loss: 0.0768 Acc: 0.9815
Training complete in 535m 39s

Epoch 82/99
----------
train Loss: 0.0331 Acc: 0.9982
val Loss: 0.0819 Acc: 0.9772
Training complete in 540m 15s

Epoch 83/99
----------
train Loss: 0.0347 Acc: 0.9979
val Loss: 0.0823 Acc: 0.9758
Training complete in 544m 50s

Epoch 84/99
----------
train Loss: 0.0331 Acc: 0.9987
val Loss: 0.0868 Acc: 0.9772
Training complete in 549m 31s

Epoch 85/99
----------
train Loss: 0.0328 Acc: 0.9985
val Loss: 0.0901 Acc: 0.9772
Training complete in 554m 1s

Epoch 86/99
----------
train Loss: 0.0325 Acc: 0.9987
val Loss: 0.0838 Acc: 0.9801
Training complete in 558m 32s

Epoch 87/99
----------
train Loss: 0.0338 Acc: 0.9980
val Loss: 0.0737 Acc: 0.9772
Training complete in 563m 9s

Epoch 88/99
----------
train Loss: 0.0304 Acc: 0.9989
val Loss: 0.0820 Acc: 0.9815
Training complete in 567m 48s

Epoch 89/99
----------
train Loss: 0.0323 Acc: 0.9985
val Loss: 0.0810 Acc: 0.9786
Training complete in 572m 16s

Epoch 90/99
----------
train Loss: 0.0318 Acc: 0.9984
val Loss: 0.0846 Acc: 0.9801
Training complete in 576m 52s

Epoch 91/99
----------
train Loss: 0.0338 Acc: 0.9982
val Loss: 0.0851 Acc: 0.9801
Training complete in 581m 21s

Epoch 92/99
----------
train Loss: 0.0327 Acc: 0.9988
val Loss: 0.0891 Acc: 0.9801
Training complete in 585m 58s

Epoch 93/99
----------
train Loss: 0.0310 Acc: 0.9989
val Loss: 0.0900 Acc: 0.9786
Training complete in 590m 44s

Epoch 94/99
----------
train Loss: 0.0313 Acc: 0.9987
val Loss: 0.0874 Acc: 0.9786
Training complete in 595m 29s

Epoch 95/99
----------
train Loss: 0.0312 Acc: 0.9990
val Loss: 0.0745 Acc: 0.9801
Training complete in 600m 12s

Epoch 96/99
----------
train Loss: 0.0317 Acc: 0.9986
val Loss: 0.0793 Acc: 0.9786
Training complete in 604m 52s

Epoch 97/99
----------
train Loss: 0.0310 Acc: 0.9987
val Loss: 0.0892 Acc: 0.9758
Training complete in 609m 44s

Epoch 98/99
----------
train Loss: 0.0323 Acc: 0.9985
val Loss: 0.0797 Acc: 0.9786
Training complete in 614m 23s

Epoch 99/99
----------
train Loss: 0.0298 Acc: 0.9989
val Loss: 0.0805 Acc: 0.9786
Training complete in 619m 2s

Training complete in 619m 2s
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7fa3df907940>]
3.002105712890625
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (nsrelu): NSReLU()
  (Sum_ReLU_2): Sum_ReLU_2()
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/99
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 0.9077 Acc: 0.1494
val Loss: 2.9484 Acc: 0.3575
Training complete in 4m 32s

Epoch 1/99
----------
train Loss: 0.8717 Acc: 0.4800
val Loss: 1.2030 Acc: 0.6866
Training complete in 8m 56s

Epoch 2/99
----------
train Loss: 0.9119 Acc: 0.6094
val Loss: 0.9144 Acc: 0.7521
Training complete in 13m 11s

Epoch 3/99
----------
train Loss: 1.0140 Acc: 0.6649
val Loss: 0.6913 Acc: 0.8091
Training complete in 17m 34s

Epoch 4/99
----------
train Loss: 1.0754 Acc: 0.7101
val Loss: 0.5691 Acc: 0.8419
Training complete in 22m 3s

Epoch 5/99
----------
train Loss: 1.0003 Acc: 0.7560
val Loss: 0.3692 Acc: 0.8960
Training complete in 26m 23s

Epoch 6/99
----------
train Loss: 0.7329 Acc: 0.8148
val Loss: 0.3319 Acc: 0.9160
Training complete in 30m 41s

Epoch 7/99
----------
train Loss: 0.6146 Acc: 0.8469
val Loss: 0.2550 Acc: 0.9202
Training complete in 35m 6s

Epoch 8/99
----------
train Loss: 0.5080 Acc: 0.8691
val Loss: 0.2304 Acc: 0.9302
Training complete in 39m 30s

Epoch 9/99
----------
train Loss: 0.4533 Acc: 0.8839
val Loss: 0.2235 Acc: 0.9330
Training complete in 43m 57s

Epoch 10/99
----------
train Loss: 0.3844 Acc: 0.9027
val Loss: 0.2317 Acc: 0.9359
Training complete in 48m 22s

Epoch 11/99
----------
train Loss: 0.3347 Acc: 0.9154
val Loss: 0.1471 Acc: 0.9516
Training complete in 52m 40s

Epoch 12/99
----------
train Loss: 0.2864 Acc: 0.9276
val Loss: 0.1437 Acc: 0.9558
Training complete in 57m 6s

Epoch 13/99
----------
train Loss: 0.2610 Acc: 0.9350
val Loss: 0.1436 Acc: 0.9516
Training complete in 61m 31s

Epoch 14/99
----------
train Loss: 0.2532 Acc: 0.9387
val Loss: 0.1233 Acc: 0.9601
Training complete in 65m 49s

Epoch 15/99
----------
train Loss: 0.2147 Acc: 0.9475
val Loss: 0.1423 Acc: 0.9544
Training complete in 70m 22s

Epoch 16/99
----------
train Loss: 0.1872 Acc: 0.9580
val Loss: 0.1089 Acc: 0.9672
Training complete in 74m 52s

Epoch 17/99
----------
train Loss: 0.1685 Acc: 0.9621
val Loss: 0.1137 Acc: 0.9573
Training complete in 79m 16s

Epoch 18/99
----------
train Loss: 0.1672 Acc: 0.9628
val Loss: 0.1067 Acc: 0.9729
Training complete in 83m 42s

Epoch 19/99
----------
train Loss: 0.1357 Acc: 0.9718
val Loss: 0.1186 Acc: 0.9644
Training complete in 87m 55s

Epoch 20/99
----------
train Loss: 0.1352 Acc: 0.9714
val Loss: 0.1180 Acc: 0.9630
Training complete in 92m 22s

Epoch 21/99
----------
train Loss: 0.1265 Acc: 0.9762
val Loss: 0.1096 Acc: 0.9658
Training complete in 96m 47s

Epoch 22/99
----------
train Loss: 0.1154 Acc: 0.9791
val Loss: 0.1282 Acc: 0.9587
Training complete in 101m 18s

Epoch 23/99
----------
train Loss: 0.1038 Acc: 0.9810
val Loss: 0.1055 Acc: 0.9729
Training complete in 105m 46s

Epoch 24/99
----------
train Loss: 0.1022 Acc: 0.9822
val Loss: 0.1228 Acc: 0.9658
Training complete in 110m 17s

Epoch 25/99
----------
train Loss: 0.0959 Acc: 0.9830
val Loss: 0.1071 Acc: 0.9701
Training complete in 114m 39s

Epoch 26/99
----------
train Loss: 0.0833 Acc: 0.9875
val Loss: 0.0989 Acc: 0.9687
Training complete in 119m 3s

Epoch 27/99
----------
train Loss: 0.0851 Acc: 0.9869
val Loss: 0.0823 Acc: 0.9744
Training complete in 123m 31s

Epoch 28/99
----------
train Loss: 0.0794 Acc: 0.9887
val Loss: 0.0850 Acc: 0.9729
Training complete in 128m 6s

Epoch 29/99
----------
train Loss: 0.0712 Acc: 0.9895
val Loss: 0.0815 Acc: 0.9744
Training complete in 132m 29s

Epoch 30/99
----------
train Loss: 0.0711 Acc: 0.9908
val Loss: 0.0883 Acc: 0.9758
Training complete in 137m 1s

Epoch 31/99
----------
train Loss: 0.0664 Acc: 0.9912
val Loss: 0.0881 Acc: 0.9744
Training complete in 141m 22s

Epoch 32/99
----------
train Loss: 0.0638 Acc: 0.9932
val Loss: 0.0942 Acc: 0.9715
Training complete in 145m 53s

Epoch 33/99
----------
train Loss: 0.0618 Acc: 0.9927
val Loss: 0.0854 Acc: 0.9758
Training complete in 150m 27s

Epoch 34/99
----------
train Loss: 0.0654 Acc: 0.9920
val Loss: 0.0987 Acc: 0.9715
Training complete in 154m 54s

Epoch 35/99
----------
train Loss: 0.0563 Acc: 0.9942
val Loss: 0.0798 Acc: 0.9815
Training complete in 159m 15s

Epoch 36/99
----------
train Loss: 0.0556 Acc: 0.9944
val Loss: 0.0822 Acc: 0.9729
Training complete in 163m 42s

Epoch 37/99
----------
train Loss: 0.0523 Acc: 0.9949
val Loss: 0.0815 Acc: 0.9772
Training complete in 168m 14s

Epoch 38/99
----------
train Loss: 0.0509 Acc: 0.9951
val Loss: 0.0905 Acc: 0.9758
Training complete in 172m 47s

Epoch 39/99
----------
train Loss: 0.0538 Acc: 0.9951
val Loss: 0.0982 Acc: 0.9758
Training complete in 177m 14s

Epoch 40/99
----------
train Loss: 0.0528 Acc: 0.9954
val Loss: 0.0806 Acc: 0.9772
Training complete in 181m 45s

Epoch 41/99
----------
train Loss: 0.0518 Acc: 0.9952
val Loss: 0.0878 Acc: 0.9729
Training complete in 186m 18s

Epoch 42/99
----------
train Loss: 0.0497 Acc: 0.9954
val Loss: 0.0894 Acc: 0.9772
Training complete in 190m 50s

Epoch 43/99
----------
train Loss: 0.0513 Acc: 0.9951
val Loss: 0.0864 Acc: 0.9786
Training complete in 195m 36s

Epoch 44/99
----------
train Loss: 0.0485 Acc: 0.9956
val Loss: 0.0866 Acc: 0.9786
Training complete in 200m 17s

Epoch 45/99
----------
train Loss: 0.0477 Acc: 0.9966
val Loss: 0.0901 Acc: 0.9758
Training complete in 204m 51s

Epoch 46/99
----------
train Loss: 0.0468 Acc: 0.9961
val Loss: 0.0759 Acc: 0.9801
Training complete in 209m 8s

Epoch 47/99
----------
train Loss: 0.0470 Acc: 0.9962
val Loss: 0.0870 Acc: 0.9758
Training complete in 213m 33s

Epoch 48/99
----------
train Loss: 0.0443 Acc: 0.9972
val Loss: 0.0808 Acc: 0.9801
Training complete in 218m 5s

Epoch 49/99
----------
train Loss: 0.0465 Acc: 0.9961
val Loss: 0.0918 Acc: 0.9729
Training complete in 222m 34s

Epoch 50/99
----------
train Loss: 0.0433 Acc: 0.9970
val Loss: 0.0824 Acc: 0.9786
Training complete in 227m 4s

Epoch 51/99
----------
train Loss: 0.0411 Acc: 0.9970
val Loss: 0.0909 Acc: 0.9772
Training complete in 231m 31s

Epoch 52/99
----------
train Loss: 0.0395 Acc: 0.9977
val Loss: 0.0949 Acc: 0.9744
Training complete in 236m 6s

Epoch 53/99
----------
train Loss: 0.0399 Acc: 0.9969
val Loss: 0.0929 Acc: 0.9786
Training complete in 240m 33s

Epoch 54/99
----------
train Loss: 0.0399 Acc: 0.9976
val Loss: 0.0832 Acc: 0.9772
Training complete in 245m 11s

Epoch 55/99
----------
train Loss: 0.0404 Acc: 0.9973
val Loss: 0.0925 Acc: 0.9758
Training complete in 249m 40s

Epoch 56/99
----------
train Loss: 0.0391 Acc: 0.9979
val Loss: 0.0686 Acc: 0.9786
Training complete in 253m 55s

Epoch 57/99
----------
train Loss: 0.0387 Acc: 0.9977
val Loss: 0.0911 Acc: 0.9758
Training complete in 258m 20s

Epoch 58/99
----------
train Loss: 0.0398 Acc: 0.9972
val Loss: 0.0901 Acc: 0.9729
Training complete in 262m 40s

Epoch 59/99
----------
train Loss: 0.0403 Acc: 0.9973
val Loss: 0.0869 Acc: 0.9772
Training complete in 267m 9s

Epoch 60/99
----------
train Loss: 0.0383 Acc: 0.9983
val Loss: 0.0856 Acc: 0.9744
Training complete in 271m 30s

Epoch 61/99
----------
train Loss: 0.0382 Acc: 0.9976
val Loss: 0.0843 Acc: 0.9801
Training complete in 275m 53s

Epoch 62/99
----------
train Loss: 0.0382 Acc: 0.9980
val Loss: 0.0848 Acc: 0.9772
Training complete in 280m 19s

Epoch 63/99
----------
train Loss: 0.0359 Acc: 0.9985
val Loss: 0.0870 Acc: 0.9772
Training complete in 284m 45s

Epoch 64/99
----------
train Loss: 0.0390 Acc: 0.9975
val Loss: 0.0832 Acc: 0.9801
Training complete in 289m 6s

Epoch 65/99
----------
train Loss: 0.0363 Acc: 0.9980
val Loss: 0.0934 Acc: 0.9758
Training complete in 293m 36s

Epoch 66/99
----------
train Loss: 0.0379 Acc: 0.9978
val Loss: 0.0854 Acc: 0.9772
Training complete in 298m 6s

Epoch 67/99
----------
train Loss: 0.0369 Acc: 0.9979
val Loss: 0.0887 Acc: 0.9772
Training complete in 302m 40s

Epoch 68/99
----------
train Loss: 0.0357 Acc: 0.9981
val Loss: 0.0877 Acc: 0.9772
Training complete in 307m 12s

Epoch 69/99
----------
train Loss: 0.0353 Acc: 0.9982
val Loss: 0.0966 Acc: 0.9772
Training complete in 311m 29s

Epoch 70/99
----------
train Loss: 0.0344 Acc: 0.9983
val Loss: 0.0913 Acc: 0.9758
Training complete in 315m 57s

Epoch 71/99
----------
train Loss: 0.0347 Acc: 0.9982
val Loss: 0.0870 Acc: 0.9772
Training complete in 320m 26s

Epoch 72/99
----------
train Loss: 0.0355 Acc: 0.9984
val Loss: 0.0864 Acc: 0.9772
Training complete in 324m 51s

Epoch 73/99
----------
train Loss: 0.0343 Acc: 0.9981
val Loss: 0.0952 Acc: 0.9715
Training complete in 329m 14s

Epoch 74/99
----------
train Loss: 0.0370 Acc: 0.9977
val Loss: 0.0887 Acc: 0.9786
Training complete in 333m 39s

Epoch 75/99
----------
train Loss: 0.0352 Acc: 0.9984
val Loss: 0.0860 Acc: 0.9772
Training complete in 338m 16s

Epoch 76/99
----------
train Loss: 0.0344 Acc: 0.9982
val Loss: 0.0953 Acc: 0.9744
Training complete in 342m 59s

Epoch 77/99
----------
train Loss: 0.0348 Acc: 0.9984
val Loss: 0.0803 Acc: 0.9786
Training complete in 347m 37s

Epoch 78/99
----------
train Loss: 0.0338 Acc: 0.9982
val Loss: 0.0820 Acc: 0.9758
Training complete in 352m 5s

Epoch 79/99
----------
train Loss: 0.0346 Acc: 0.9979
val Loss: 0.0856 Acc: 0.9772
Training complete in 356m 13s

Epoch 80/99
----------
train Loss: 0.0331 Acc: 0.9986
val Loss: 0.0889 Acc: 0.9758
Training complete in 360m 44s

Epoch 81/99
----------
train Loss: 0.0323 Acc: 0.9985
val Loss: 0.0914 Acc: 0.9758
Training complete in 364m 55s

Epoch 82/99
----------
train Loss: 0.0328 Acc: 0.9987
val Loss: 0.0905 Acc: 0.9772
Training complete in 369m 26s

Epoch 83/99
----------
train Loss: 0.0311 Acc: 0.9989
val Loss: 0.0880 Acc: 0.9801
Training complete in 373m 56s

Epoch 84/99
----------
train Loss: 0.0304 Acc: 0.9987
val Loss: 0.0833 Acc: 0.9772
Training complete in 378m 14s

Epoch 85/99
----------
train Loss: 0.0326 Acc: 0.9984
val Loss: 0.0766 Acc: 0.9758
Training complete in 383m 1s

Epoch 86/99
----------
train Loss: 0.0322 Acc: 0.9986
val Loss: 0.0868 Acc: 0.9758
Training complete in 387m 32s

Epoch 87/99
----------
train Loss: 0.0321 Acc: 0.9986
val Loss: 0.0861 Acc: 0.9772
Training complete in 392m 1s

Epoch 88/99
----------
train Loss: 0.0326 Acc: 0.9988
val Loss: 0.0914 Acc: 0.9758
Training complete in 396m 23s

Epoch 89/99
----------
train Loss: 0.0311 Acc: 0.9991
val Loss: 0.0864 Acc: 0.9772
Training complete in 401m 3s

Epoch 90/99
----------
train Loss: 0.0326 Acc: 0.9984
val Loss: 0.0876 Acc: 0.9772
Training complete in 405m 23s

Epoch 91/99
----------
train Loss: 0.0324 Acc: 0.9989
val Loss: 0.0893 Acc: 0.9758
Training complete in 409m 36s

Epoch 92/99
----------
train Loss: 0.0323 Acc: 0.9986
val Loss: 0.0928 Acc: 0.9744
Training complete in 414m 1s

Epoch 93/99
----------
train Loss: 0.0310 Acc: 0.9986
val Loss: 0.0892 Acc: 0.9772
Training complete in 418m 31s

Epoch 94/99
----------
train Loss: 0.0310 Acc: 0.9990
val Loss: 0.0875 Acc: 0.9786
Training complete in 422m 50s

Epoch 95/99
----------
train Loss: 0.0311 Acc: 0.9989
val Loss: 0.0869 Acc: 0.9786
Training complete in 427m 27s

Epoch 96/99
----------
train Loss: 0.0308 Acc: 0.9989
val Loss: 0.0818 Acc: 0.9772
Training complete in 431m 43s

Epoch 97/99
----------
train Loss: 0.0318 Acc: 0.9984
val Loss: 0.0869 Acc: 0.9786
Training complete in 436m 8s

Epoch 98/99
----------
train Loss: 0.0308 Acc: 0.9986
val Loss: 0.0906 Acc: 0.9772
Training complete in 440m 16s

Epoch 99/99
----------
train Loss: 0.0300 Acc: 0.9992
val Loss: 0.0872 Acc: 0.9772
Training complete in 444m 45s

Training complete in 444m 45s
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7fa5ee886860>]
2.7151737213134766
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (nsrelu): NSReLU()
  (Sum_ReLU_2): Sum_ReLU_2()
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/99
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 0.8892 Acc: 0.1594
val Loss: 2.9896 Acc: 0.3632
Training complete in 5m 26s

Epoch 1/99
----------
train Loss: 0.8934 Acc: 0.4651
val Loss: 1.1393 Acc: 0.7023
Training complete in 10m 51s

Epoch 2/99
----------
train Loss: 0.9372 Acc: 0.6026
val Loss: 0.8219 Acc: 0.7835
Training complete in 16m 12s

Epoch 3/99
----------
train Loss: 1.0301 Acc: 0.6601
val Loss: 0.7844 Acc: 0.7963
Training complete in 21m 26s

Epoch 4/99
----------
train Loss: 1.1082 Acc: 0.7039
val Loss: 0.5730 Acc: 0.8305
Training complete in 26m 36s

Epoch 5/99
----------
train Loss: 1.0023 Acc: 0.7524
val Loss: 0.3851 Acc: 0.9003
Training complete in 31m 55s

Epoch 6/99
----------
train Loss: 0.7518 Acc: 0.8104
val Loss: 0.3000 Acc: 0.9145
Training complete in 37m 19s

Epoch 7/99
----------
train Loss: 0.6607 Acc: 0.8363
val Loss: 0.2952 Acc: 0.9217
Training complete in 42m 40s

Epoch 8/99
----------
train Loss: 0.5343 Acc: 0.8628
val Loss: 0.2437 Acc: 0.9274
Training complete in 48m 5s

Epoch 9/99
----------
train Loss: 0.4600 Acc: 0.8843
val Loss: 0.2112 Acc: 0.9387
Training complete in 53m 28s

Epoch 10/99
----------
train Loss: 0.3986 Acc: 0.8988
val Loss: 0.1884 Acc: 0.9459
Training complete in 58m 50s

Epoch 11/99
----------
train Loss: 0.3821 Acc: 0.9049
val Loss: 0.1654 Acc: 0.9473
Training complete in 64m 11s

Epoch 12/99
----------
train Loss: 0.3315 Acc: 0.9161
val Loss: 0.2025 Acc: 0.9459
Training complete in 69m 33s

Epoch 13/99
----------
train Loss: 0.2794 Acc: 0.9313
val Loss: 0.1390 Acc: 0.9615
Training complete in 74m 43s

Epoch 14/99
----------
train Loss: 0.2564 Acc: 0.9369
val Loss: 0.1864 Acc: 0.9430
Training complete in 79m 59s

Epoch 15/99
----------
train Loss: 0.2279 Acc: 0.9438
val Loss: 0.1257 Acc: 0.9658
Training complete in 85m 28s

Epoch 16/99
----------
train Loss: 0.2107 Acc: 0.9491
val Loss: 0.1514 Acc: 0.9558
Training complete in 90m 52s

Epoch 17/99
----------
train Loss: 0.1885 Acc: 0.9571
val Loss: 0.1310 Acc: 0.9644
Training complete in 96m 7s

Epoch 18/99
----------
train Loss: 0.1641 Acc: 0.9635
val Loss: 0.1290 Acc: 0.9658
Training complete in 101m 42s

Epoch 19/99
----------
train Loss: 0.1474 Acc: 0.9683
val Loss: 0.1113 Acc: 0.9644
Training complete in 107m 9s

Epoch 20/99
----------
train Loss: 0.1449 Acc: 0.9686
val Loss: 0.1109 Acc: 0.9630
Training complete in 112m 32s

Epoch 21/99
----------
train Loss: 0.1277 Acc: 0.9748
val Loss: 0.1201 Acc: 0.9672
Training complete in 117m 60s

Epoch 22/99
----------
train Loss: 0.1227 Acc: 0.9769
val Loss: 0.1382 Acc: 0.9672
Training complete in 123m 25s

Epoch 23/99
----------
train Loss: 0.1167 Acc: 0.9782
val Loss: 0.0872 Acc: 0.9729
Training complete in 128m 50s

Epoch 24/99
----------
train Loss: 0.1099 Acc: 0.9809
val Loss: 0.0946 Acc: 0.9687
Training complete in 134m 17s

Epoch 25/99
----------
train Loss: 0.0942 Acc: 0.9832
val Loss: 0.1057 Acc: 0.9672
Training complete in 139m 40s

Epoch 26/99
----------
train Loss: 0.0938 Acc: 0.9849
val Loss: 0.1138 Acc: 0.9658
Training complete in 145m 10s

Epoch 27/99
----------
train Loss: 0.0862 Acc: 0.9867
val Loss: 0.0829 Acc: 0.9701
Training complete in 150m 41s

Epoch 28/99
----------
train Loss: 0.0810 Acc: 0.9877
val Loss: 0.0967 Acc: 0.9687
Training complete in 155m 57s

Epoch 29/99
----------
train Loss: 0.0797 Acc: 0.9875
val Loss: 0.0977 Acc: 0.9772
Training complete in 161m 22s

Epoch 30/99
----------
train Loss: 0.0736 Acc: 0.9898
val Loss: 0.1049 Acc: 0.9701
Training complete in 166m 50s

Epoch 31/99
----------
train Loss: 0.0678 Acc: 0.9919
val Loss: 0.0864 Acc: 0.9729
Training complete in 172m 12s

Epoch 32/99
----------
train Loss: 0.0678 Acc: 0.9910
val Loss: 0.0914 Acc: 0.9729
Training complete in 177m 37s

Epoch 33/99
----------
train Loss: 0.0640 Acc: 0.9932
val Loss: 0.0893 Acc: 0.9701
Training complete in 183m 2s

Epoch 34/99
----------
train Loss: 0.0626 Acc: 0.9929
val Loss: 0.0946 Acc: 0.9744
Training complete in 188m 15s

Epoch 35/99
----------
train Loss: 0.0643 Acc: 0.9930
val Loss: 0.1061 Acc: 0.9701
Training complete in 193m 25s

Epoch 36/99
----------
train Loss: 0.0585 Acc: 0.9942
val Loss: 0.0906 Acc: 0.9672
Training complete in 198m 53s

Epoch 37/99
----------
train Loss: 0.0554 Acc: 0.9953
val Loss: 0.0945 Acc: 0.9701
Training complete in 204m 14s

Epoch 38/99
----------
train Loss: 0.0561 Acc: 0.9948
val Loss: 0.0850 Acc: 0.9744
Training complete in 209m 57s

Epoch 39/99
----------
train Loss: 0.0563 Acc: 0.9948
val Loss: 0.0841 Acc: 0.9715
Training complete in 215m 36s

Epoch 40/99
----------
train Loss: 0.0523 Acc: 0.9956
val Loss: 0.0970 Acc: 0.9758
Training complete in 221m 13s

Epoch 41/99
----------
train Loss: 0.0542 Acc: 0.9948
val Loss: 0.0912 Acc: 0.9758
Training complete in 226m 43s

Epoch 42/99
----------
train Loss: 0.0514 Acc: 0.9957
val Loss: 0.0884 Acc: 0.9758
Training complete in 232m 23s

Epoch 43/99
----------
train Loss: 0.0476 Acc: 0.9965
val Loss: 0.0847 Acc: 0.9758
Training complete in 238m 0s

Epoch 44/99
----------
train Loss: 0.0482 Acc: 0.9965
val Loss: 0.0851 Acc: 0.9744
Training complete in 243m 31s

Epoch 45/99
----------
train Loss: 0.0504 Acc: 0.9954
val Loss: 0.0922 Acc: 0.9687
Training complete in 248m 59s

Epoch 46/99
----------
train Loss: 0.0453 Acc: 0.9965
val Loss: 0.0883 Acc: 0.9715
Training complete in 254m 30s

Epoch 47/99
----------
train Loss: 0.0452 Acc: 0.9968
val Loss: 0.0894 Acc: 0.9744
Training complete in 260m 4s

Epoch 48/99
----------
train Loss: 0.0451 Acc: 0.9968
val Loss: 0.0899 Acc: 0.9729
Training complete in 265m 40s

Epoch 49/99
----------
train Loss: 0.0483 Acc: 0.9957
val Loss: 0.0933 Acc: 0.9758
Training complete in 271m 13s

Epoch 50/99
----------
train Loss: 0.0472 Acc: 0.9962
val Loss: 0.0936 Acc: 0.9758
Training complete in 276m 52s

Epoch 51/99
----------
train Loss: 0.0450 Acc: 0.9968
val Loss: 0.0873 Acc: 0.9744
Training complete in 282m 27s

Epoch 52/99
----------
train Loss: 0.0428 Acc: 0.9974
val Loss: 0.0888 Acc: 0.9758
Training complete in 287m 56s

Epoch 53/99
----------
train Loss: 0.0400 Acc: 0.9979
val Loss: 0.0885 Acc: 0.9729
Training complete in 293m 27s

Epoch 54/99
----------
train Loss: 0.0432 Acc: 0.9967
val Loss: 0.0870 Acc: 0.9729
Training complete in 299m 2s

Epoch 55/99
----------
train Loss: 0.0423 Acc: 0.9980
val Loss: 0.0872 Acc: 0.9744
Training complete in 304m 32s

Epoch 56/99
----------
train Loss: 0.0439 Acc: 0.9965
val Loss: 0.0788 Acc: 0.9758
Training complete in 310m 0s

Epoch 57/99
----------
train Loss: 0.0412 Acc: 0.9973
val Loss: 0.0863 Acc: 0.9729
Training complete in 315m 35s

Epoch 58/99
----------
train Loss: 0.0404 Acc: 0.9979
val Loss: 0.0847 Acc: 0.9729
Training complete in 321m 7s

Epoch 59/99
----------
train Loss: 0.0416 Acc: 0.9972
val Loss: 0.0801 Acc: 0.9772
Training complete in 326m 42s

Epoch 60/99
----------
train Loss: 0.0394 Acc: 0.9979
val Loss: 0.0850 Acc: 0.9758
Training complete in 332m 17s

Epoch 61/99
----------
train Loss: 0.0407 Acc: 0.9975
val Loss: 0.0755 Acc: 0.9758
Training complete in 337m 54s

Epoch 62/99
----------
train Loss: 0.0388 Acc: 0.9984
val Loss: 0.0790 Acc: 0.9729
Training complete in 343m 25s

Epoch 63/99
----------
train Loss: 0.0390 Acc: 0.9977
val Loss: 0.0883 Acc: 0.9715
Training complete in 349m 4s

Epoch 64/99
----------
train Loss: 0.0391 Acc: 0.9976
val Loss: 0.0867 Acc: 0.9729
Training complete in 354m 41s

Epoch 65/99
----------
train Loss: 0.0352 Acc: 0.9980
val Loss: 0.0854 Acc: 0.9715
Training complete in 360m 20s

Epoch 66/99
----------
train Loss: 0.0371 Acc: 0.9982
val Loss: 0.0791 Acc: 0.9744
Training complete in 365m 53s

Epoch 67/99
----------
train Loss: 0.0389 Acc: 0.9977
val Loss: 0.0879 Acc: 0.9744
Training complete in 371m 29s

Epoch 68/99
----------
train Loss: 0.0374 Acc: 0.9979
val Loss: 0.0865 Acc: 0.9744
Training complete in 377m 0s

Epoch 69/99
----------
train Loss: 0.0383 Acc: 0.9975
val Loss: 0.0813 Acc: 0.9758
Training complete in 382m 38s

Epoch 70/99
----------
train Loss: 0.0369 Acc: 0.9981
val Loss: 0.0832 Acc: 0.9744
Training complete in 388m 14s

Epoch 71/99
----------
train Loss: 0.0364 Acc: 0.9980
val Loss: 0.0942 Acc: 0.9729
Training complete in 393m 49s

Epoch 72/99
----------
train Loss: 0.0368 Acc: 0.9981
val Loss: 0.0903 Acc: 0.9715
Training complete in 399m 20s

Epoch 73/99
----------
train Loss: 0.0368 Acc: 0.9982
val Loss: 0.0886 Acc: 0.9715
Training complete in 405m 8s

Epoch 74/99
----------
train Loss: 0.0341 Acc: 0.9987
val Loss: 0.0846 Acc: 0.9758
Training complete in 410m 52s

Epoch 75/99
----------
train Loss: 0.0374 Acc: 0.9982
val Loss: 0.0838 Acc: 0.9758
Training complete in 416m 49s

Epoch 76/99
----------
train Loss: 0.0351 Acc: 0.9984
val Loss: 0.0768 Acc: 0.9786
Training complete in 422m 22s

Epoch 77/99
----------
train Loss: 0.0330 Acc: 0.9986
val Loss: 0.0936 Acc: 0.9758
Training complete in 427m 51s

Epoch 78/99
----------
train Loss: 0.0345 Acc: 0.9985
val Loss: 0.0879 Acc: 0.9744
Training complete in 433m 22s

Epoch 79/99
----------
train Loss: 0.0334 Acc: 0.9985
val Loss: 0.0872 Acc: 0.9758
Training complete in 439m 1s

Epoch 80/99
----------
train Loss: 0.0340 Acc: 0.9982
val Loss: 0.0863 Acc: 0.9729
Training complete in 444m 30s

Epoch 81/99
----------
train Loss: 0.0360 Acc: 0.9982
val Loss: 0.0976 Acc: 0.9744
Training complete in 449m 54s

Epoch 82/99
----------
train Loss: 0.0340 Acc: 0.9984
val Loss: 0.0870 Acc: 0.9744
Training complete in 454m 45s

Epoch 83/99
----------
train Loss: 0.0345 Acc: 0.9985
val Loss: 0.0919 Acc: 0.9758
Training complete in 459m 51s

Epoch 84/99
----------
train Loss: 0.0346 Acc: 0.9981
val Loss: 0.0883 Acc: 0.9758
Training complete in 464m 42s

Epoch 85/99
----------
train Loss: 0.0349 Acc: 0.9988
val Loss: 0.0879 Acc: 0.9744
Training complete in 469m 53s

Epoch 86/99
----------
train Loss: 0.0327 Acc: 0.9987
val Loss: 0.0834 Acc: 0.9772
Training complete in 475m 50s

Epoch 87/99
----------
train Loss: 0.0352 Acc: 0.9984
val Loss: 0.0883 Acc: 0.9758
Training complete in 481m 35s

Epoch 88/99
----------
train Loss: 0.0341 Acc: 0.9985
val Loss: 0.0895 Acc: 0.9744
Training complete in 486m 46s

Epoch 89/99
----------
train Loss: 0.0331 Acc: 0.9986
val Loss: 0.0854 Acc: 0.9744
Training complete in 492m 20s

Epoch 90/99
----------
train Loss: 0.0349 Acc: 0.9984
val Loss: 0.0848 Acc: 0.9758
Training complete in 497m 26s

Epoch 91/99
----------
train Loss: 0.0320 Acc: 0.9989
val Loss: 0.0907 Acc: 0.9729
Training complete in 502m 50s

Epoch 92/99
----------
train Loss: 0.0334 Acc: 0.9985
val Loss: 0.0896 Acc: 0.9744
Training complete in 508m 7s

Epoch 93/99
----------
train Loss: 0.0330 Acc: 0.9986
val Loss: 0.0819 Acc: 0.9758
Training complete in 513m 10s

Epoch 94/99
----------
train Loss: 0.0322 Acc: 0.9989
val Loss: 0.0915 Acc: 0.9758
Training complete in 518m 33s

Epoch 95/99
----------
train Loss: 0.0331 Acc: 0.9984
val Loss: 0.0944 Acc: 0.9729
Training complete in 523m 58s

Epoch 96/99
----------
train Loss: 0.0312 Acc: 0.9989
val Loss: 0.0910 Acc: 0.9744
Training complete in 529m 27s

Epoch 97/99
----------
train Loss: 0.0320 Acc: 0.9989
val Loss: 0.0897 Acc: 0.9729
Training complete in 534m 36s

Epoch 98/99
----------
train Loss: 0.0299 Acc: 0.9992
val Loss: 0.0861 Acc: 0.9758
Training complete in 540m 20s

Epoch 99/99
----------
train Loss: 0.0305 Acc: 0.9990
val Loss: 0.0857 Acc: 0.9758
Training complete in 545m 35s

Training complete in 545m 35s
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7f99972e99e8>]
2.733738422393799
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (nsrelu): NSReLU()
  (Sum_ReLU_2): Sum_ReLU_2()
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/99
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 0.9067 Acc: 0.1479
val Loss: 3.1099 Acc: 0.3618
Training complete in 5m 19s

Epoch 1/99
----------
train Loss: 0.9328 Acc: 0.4451
val Loss: 1.2779 Acc: 0.6823
Training complete in 10m 37s

Epoch 2/99
----------
train Loss: 0.9491 Acc: 0.5932
val Loss: 0.8990 Acc: 0.7450
Training complete in 15m 48s

Epoch 3/99
----------
train Loss: 1.0312 Acc: 0.6630
val Loss: 0.7201 Acc: 0.8077
Training complete in 21m 0s

Epoch 4/99
----------
train Loss: 1.0980 Acc: 0.7066
val Loss: 0.5626 Acc: 0.8362
Training complete in 26m 27s

Epoch 5/99
----------
train Loss: 0.9999 Acc: 0.7535
val Loss: 0.4086 Acc: 0.8932
Training complete in 31m 37s

Epoch 6/99
----------
train Loss: 0.7706 Acc: 0.8044
val Loss: 0.3566 Acc: 0.9003
Training complete in 36m 56s

Epoch 7/99
----------
train Loss: 0.6386 Acc: 0.8366
val Loss: 0.2346 Acc: 0.9259
Training complete in 42m 14s

Epoch 8/99
----------
train Loss: 0.5275 Acc: 0.8655
val Loss: 0.2223 Acc: 0.9330
Training complete in 47m 26s

Epoch 9/99
----------
train Loss: 0.4464 Acc: 0.8885
val Loss: 0.1887 Acc: 0.9430
Training complete in 52m 34s

Epoch 10/99
----------
train Loss: 0.4083 Acc: 0.8934
val Loss: 0.2186 Acc: 0.9373
Training complete in 57m 50s

Epoch 11/99
----------
train Loss: 0.3535 Acc: 0.9116
val Loss: 0.1892 Acc: 0.9444
Training complete in 63m 6s

Epoch 12/99
----------
train Loss: 0.3157 Acc: 0.9207
val Loss: 0.1426 Acc: 0.9530
Training complete in 68m 25s

Epoch 13/99
----------
train Loss: 0.2920 Acc: 0.9286
val Loss: 0.1502 Acc: 0.9544
Training complete in 73m 44s

Epoch 14/99
----------
train Loss: 0.2504 Acc: 0.9396
val Loss: 0.1661 Acc: 0.9544
Training complete in 78m 56s

Epoch 15/99
----------
train Loss: 0.2074 Acc: 0.9511
val Loss: 0.1651 Acc: 0.9516
Training complete in 84m 13s

Epoch 16/99
----------
train Loss: 0.2067 Acc: 0.9530
val Loss: 0.1393 Acc: 0.9587
Training complete in 89m 31s

Epoch 17/99
----------
train Loss: 0.1855 Acc: 0.9572
val Loss: 0.1310 Acc: 0.9601
Training complete in 94m 50s

Epoch 18/99
----------
train Loss: 0.1845 Acc: 0.9588
val Loss: 0.1196 Acc: 0.9601
Training complete in 100m 9s

Epoch 19/99
----------
train Loss: 0.1469 Acc: 0.9686
val Loss: 0.1267 Acc: 0.9615
Training complete in 105m 26s

Epoch 20/99
----------
train Loss: 0.1437 Acc: 0.9696
val Loss: 0.1209 Acc: 0.9644
Training complete in 110m 45s

Epoch 21/99
----------
train Loss: 0.1248 Acc: 0.9743
val Loss: 0.1130 Acc: 0.9644
Training complete in 116m 2s

Epoch 22/99
----------
train Loss: 0.1164 Acc: 0.9773
val Loss: 0.1140 Acc: 0.9658
Training complete in 121m 13s

Epoch 23/99
----------
train Loss: 0.1075 Acc: 0.9801
val Loss: 0.0935 Acc: 0.9758
Training complete in 126m 33s

Epoch 24/99
----------
train Loss: 0.1006 Acc: 0.9807
val Loss: 0.1138 Acc: 0.9729
Training complete in 131m 56s

Epoch 25/99
----------
train Loss: 0.1032 Acc: 0.9816
val Loss: 0.0920 Acc: 0.9744
Training complete in 137m 8s

Epoch 26/99
----------
train Loss: 0.0834 Acc: 0.9870
val Loss: 0.1151 Acc: 0.9658
Training complete in 142m 26s

Epoch 27/99
----------
train Loss: 0.0848 Acc: 0.9874
val Loss: 0.0931 Acc: 0.9715
Training complete in 147m 37s

Epoch 28/99
----------
train Loss: 0.0787 Acc: 0.9898
val Loss: 0.0695 Acc: 0.9701
Training complete in 152m 52s

Epoch 29/99
----------
train Loss: 0.0781 Acc: 0.9885
val Loss: 0.0941 Acc: 0.9758
Training complete in 158m 11s

Epoch 30/99
----------
train Loss: 0.0742 Acc: 0.9910
val Loss: 0.0922 Acc: 0.9715
Training complete in 163m 25s

Epoch 31/99
----------
train Loss: 0.0710 Acc: 0.9903
val Loss: 0.0876 Acc: 0.9744
Training complete in 168m 33s

Epoch 32/99
----------
train Loss: 0.0672 Acc: 0.9915
val Loss: 0.0801 Acc: 0.9772
Training complete in 173m 45s

Epoch 33/99
----------
train Loss: 0.0666 Acc: 0.9927
val Loss: 0.0834 Acc: 0.9744
Training complete in 178m 56s

Epoch 34/99
----------
train Loss: 0.0650 Acc: 0.9920
val Loss: 0.0878 Acc: 0.9744
Training complete in 184m 9s

Epoch 35/99
----------
train Loss: 0.0625 Acc: 0.9934
val Loss: 0.0851 Acc: 0.9744
Training complete in 189m 29s

Epoch 36/99
----------
train Loss: 0.0582 Acc: 0.9942
val Loss: 0.0826 Acc: 0.9758
Training complete in 194m 39s

Epoch 37/99
----------
train Loss: 0.0627 Acc: 0.9934
val Loss: 0.0807 Acc: 0.9786
Training complete in 199m 50s

Epoch 38/99
----------
train Loss: 0.0583 Acc: 0.9944
val Loss: 0.0841 Acc: 0.9758
Training complete in 205m 1s

Epoch 39/99
----------
train Loss: 0.0515 Acc: 0.9958
val Loss: 0.0826 Acc: 0.9772
Training complete in 210m 20s

Epoch 40/99
----------
train Loss: 0.0510 Acc: 0.9954
val Loss: 0.0904 Acc: 0.9786
Training complete in 215m 33s

Epoch 41/99
----------
train Loss: 0.0515 Acc: 0.9957
val Loss: 0.0876 Acc: 0.9772
Training complete in 220m 49s

Epoch 42/99
----------
train Loss: 0.0459 Acc: 0.9967
val Loss: 0.0863 Acc: 0.9729
Training complete in 226m 1s

Epoch 43/99
----------
train Loss: 0.0487 Acc: 0.9964
val Loss: 0.0915 Acc: 0.9772
Training complete in 231m 19s

Epoch 44/99
----------
train Loss: 0.0504 Acc: 0.9956
val Loss: 0.0944 Acc: 0.9672
Training complete in 236m 31s

Epoch 45/99
----------
train Loss: 0.0490 Acc: 0.9967
val Loss: 0.0869 Acc: 0.9758
Training complete in 241m 43s

Epoch 46/99
----------
train Loss: 0.0474 Acc: 0.9962
val Loss: 0.0894 Acc: 0.9729
Training complete in 246m 56s

Epoch 47/99
----------
train Loss: 0.0468 Acc: 0.9963
val Loss: 0.0913 Acc: 0.9701
Training complete in 252m 5s

Epoch 48/99
----------
train Loss: 0.0460 Acc: 0.9961
val Loss: 0.0914 Acc: 0.9744
Training complete in 257m 24s

Epoch 49/99
----------
train Loss: 0.0445 Acc: 0.9974
val Loss: 0.1015 Acc: 0.9744
Training complete in 262m 41s

Epoch 50/99
----------
train Loss: 0.0452 Acc: 0.9968
val Loss: 0.0952 Acc: 0.9744
Training complete in 267m 58s

Epoch 51/99
----------
train Loss: 0.0467 Acc: 0.9965
val Loss: 0.0890 Acc: 0.9758
Training complete in 273m 8s

Epoch 52/99
----------
train Loss: 0.0431 Acc: 0.9971
val Loss: 0.0856 Acc: 0.9687
Training complete in 278m 27s

Epoch 53/99
----------
train Loss: 0.0431 Acc: 0.9972
val Loss: 0.0901 Acc: 0.9715
Training complete in 283m 43s

Epoch 54/99
----------
train Loss: 0.0428 Acc: 0.9979
val Loss: 0.0937 Acc: 0.9744
Training complete in 288m 57s

Epoch 55/99
----------
train Loss: 0.0424 Acc: 0.9973
val Loss: 0.0899 Acc: 0.9758
Training complete in 293m 59s

Epoch 56/99
----------
train Loss: 0.0391 Acc: 0.9976
val Loss: 0.0808 Acc: 0.9786
Training complete in 298m 48s

Epoch 57/99
----------
train Loss: 0.0417 Acc: 0.9972
val Loss: 0.0882 Acc: 0.9729
Training complete in 303m 51s

Epoch 58/99
----------
train Loss: 0.0409 Acc: 0.9977
val Loss: 0.0816 Acc: 0.9758
Training complete in 308m 54s

Epoch 59/99
----------
train Loss: 0.0396 Acc: 0.9976
val Loss: 0.0961 Acc: 0.9715
Training complete in 313m 52s

Epoch 60/99
----------
train Loss: 0.0392 Acc: 0.9979
val Loss: 0.0961 Acc: 0.9729
Training complete in 318m 35s

Epoch 61/99
----------
train Loss: 0.0406 Acc: 0.9977
val Loss: 0.0877 Acc: 0.9729
Training complete in 323m 33s

Epoch 62/99
----------
train Loss: 0.0382 Acc: 0.9982
val Loss: 0.0894 Acc: 0.9772
Training complete in 328m 31s

Epoch 63/99
----------
train Loss: 0.0394 Acc: 0.9979
val Loss: 0.0877 Acc: 0.9772
Training complete in 333m 32s

Epoch 64/99
----------
train Loss: 0.0383 Acc: 0.9979
val Loss: 0.0867 Acc: 0.9758
Training complete in 338m 25s

Epoch 65/99
----------
train Loss: 0.0354 Acc: 0.9981
val Loss: 0.0846 Acc: 0.9744
Training complete in 343m 23s

Epoch 66/99
----------
train Loss: 0.0383 Acc: 0.9977
val Loss: 0.0852 Acc: 0.9729
Training complete in 348m 11s

Epoch 67/99
----------
train Loss: 0.0370 Acc: 0.9984
val Loss: 0.0907 Acc: 0.9701
Training complete in 353m 10s

Epoch 68/99
----------
train Loss: 0.0372 Acc: 0.9978
val Loss: 0.0906 Acc: 0.9758
Training complete in 357m 50s

Epoch 69/99
----------
train Loss: 0.0363 Acc: 0.9985
val Loss: 0.0896 Acc: 0.9715
Training complete in 362m 50s

Epoch 70/99
----------
train Loss: 0.0357 Acc: 0.9983
val Loss: 0.0859 Acc: 0.9744
Training complete in 367m 34s

Epoch 71/99
----------
train Loss: 0.0354 Acc: 0.9984
val Loss: 0.0883 Acc: 0.9715
Training complete in 372m 36s

Epoch 72/99
----------
train Loss: 0.0359 Acc: 0.9982
val Loss: 0.0993 Acc: 0.9715
Training complete in 377m 34s

Epoch 73/99
----------
train Loss: 0.0365 Acc: 0.9981
val Loss: 0.0904 Acc: 0.9786
Training complete in 382m 37s

Epoch 74/99
----------
train Loss: 0.0370 Acc: 0.9982
val Loss: 0.0890 Acc: 0.9744
Training complete in 387m 33s

Epoch 75/99
----------
train Loss: 0.0372 Acc: 0.9981
val Loss: 0.0966 Acc: 0.9758
Training complete in 392m 37s

Epoch 76/99
----------
train Loss: 0.0349 Acc: 0.9987
val Loss: 0.0858 Acc: 0.9801
Training complete in 397m 36s

Epoch 77/99
----------
train Loss: 0.0345 Acc: 0.9982
val Loss: 0.0896 Acc: 0.9744
Training complete in 402m 16s

Epoch 78/99
----------
train Loss: 0.0331 Acc: 0.9987
val Loss: 0.0833 Acc: 0.9715
Training complete in 407m 18s

Epoch 79/99
----------
train Loss: 0.0339 Acc: 0.9986
val Loss: 0.0927 Acc: 0.9744
Training complete in 412m 9s

Epoch 80/99
----------
train Loss: 0.0334 Acc: 0.9987
val Loss: 0.0904 Acc: 0.9772
Training complete in 417m 7s

Epoch 81/99
----------
train Loss: 0.0332 Acc: 0.9983
val Loss: 0.0839 Acc: 0.9758
Training complete in 422m 6s

Epoch 82/99
----------
train Loss: 0.0349 Acc: 0.9984
val Loss: 0.0882 Acc: 0.9744
Training complete in 427m 7s

Epoch 83/99
----------
train Loss: 0.0334 Acc: 0.9986
val Loss: 0.0960 Acc: 0.9729
Training complete in 431m 45s

Epoch 84/99
----------
train Loss: 0.0344 Acc: 0.9979
val Loss: 0.0798 Acc: 0.9786
Training complete in 436m 26s

Epoch 85/99
----------
train Loss: 0.0354 Acc: 0.9984
val Loss: 0.0915 Acc: 0.9758
Training complete in 441m 4s

Epoch 86/99
----------
train Loss: 0.0338 Acc: 0.9989
val Loss: 0.0914 Acc: 0.9772
Training complete in 445m 40s

Epoch 87/99
----------
train Loss: 0.0334 Acc: 0.9984
val Loss: 0.0857 Acc: 0.9786
Training complete in 450m 21s

Epoch 88/99
----------
train Loss: 0.0324 Acc: 0.9988
val Loss: 0.0918 Acc: 0.9772
Training complete in 455m 25s

Epoch 89/99
----------
train Loss: 0.0341 Acc: 0.9983
val Loss: 0.0821 Acc: 0.9758
Training complete in 460m 29s

Epoch 90/99
----------
train Loss: 0.0350 Acc: 0.9985
val Loss: 0.0839 Acc: 0.9815
Training complete in 465m 20s

Epoch 91/99
----------
train Loss: 0.0322 Acc: 0.9990
val Loss: 0.0854 Acc: 0.9744
Training complete in 469m 57s

Epoch 92/99
----------
train Loss: 0.0323 Acc: 0.9988
val Loss: 0.0905 Acc: 0.9744
Training complete in 474m 59s

Epoch 93/99
----------
train Loss: 0.0325 Acc: 0.9991
val Loss: 0.0896 Acc: 0.9758
Training complete in 479m 54s

Epoch 94/99
----------
train Loss: 0.0314 Acc: 0.9989
val Loss: 0.0886 Acc: 0.9744
Training complete in 484m 34s

Epoch 95/99
----------
train Loss: 0.0309 Acc: 0.9992
val Loss: 0.0917 Acc: 0.9758
Training complete in 489m 11s

Epoch 96/99
----------
train Loss: 0.0330 Acc: 0.9986
val Loss: 0.0850 Acc: 0.9786
Training complete in 494m 3s

Epoch 97/99
----------
train Loss: 0.0326 Acc: 0.9991
val Loss: 0.0870 Acc: 0.9758
Training complete in 499m 5s

Epoch 98/99
----------
train Loss: 0.0318 Acc: 0.9987
val Loss: 0.0848 Acc: 0.9744
Training complete in 503m 45s

Epoch 99/99
----------
train Loss: 0.0327 Acc: 0.9987
val Loss: 0.0853 Acc: 0.9744
Training complete in 508m 42s

Training complete in 508m 42s
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7ffaaa5e19e8>]
2.6763293743133545
ft_resnest50(
  (model): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (nsrelu): NSReLU()
  (Sum_ReLU_2): Sum_ReLU_2()
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/99
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 0.9664 Acc: 0.1233
val Loss: 3.7368 Acc: 0.2721
Training complete in 3m 43s

Epoch 1/99
----------
train Loss: 0.9383 Acc: 0.4775
val Loss: 0.8815 Acc: 0.8020
Training complete in 9m 39s

Epoch 2/99
----------
train Loss: 0.6688 Acc: 0.7259
val Loss: 0.3634 Acc: 0.9003
Training complete in 17m 40s

Epoch 3/99
----------
train Loss: 0.6626 Acc: 0.7824
val Loss: 0.2938 Acc: 0.9003
Training complete in 25m 43s

Epoch 4/99
----------
train Loss: 0.7261 Acc: 0.7998
val Loss: 0.2296 Acc: 0.9202
Training complete in 33m 46s

Epoch 5/99
----------
train Loss: 0.6611 Acc: 0.8325
val Loss: 0.0893 Acc: 0.9573
Training complete in 41m 49s

Epoch 6/99
----------
train Loss: 0.4635 Acc: 0.8831
val Loss: 0.0366 Acc: 0.9701
Training complete in 49m 52s

Epoch 7/99
----------
train Loss: 0.3443 Acc: 0.9110
val Loss: 0.0233 Acc: 0.9772
Training complete in 57m 55s

Epoch 8/99
----------
train Loss: 0.2645 Acc: 0.9325
val Loss: 0.0322 Acc: 0.9729
Training complete in 65m 58s

Epoch 9/99
----------
train Loss: 0.2118 Acc: 0.9498
val Loss: 0.0095 Acc: 0.9801
Training complete in 74m 1s

Epoch 10/99
----------
train Loss: 0.1709 Acc: 0.9582
val Loss: 0.0085 Acc: 0.9786
Training complete in 82m 5s

Epoch 11/99
----------
train Loss: 0.1492 Acc: 0.9647
val Loss: 0.0104 Acc: 0.9772
Training complete in 90m 6s

Epoch 12/99
----------
train Loss: 0.1302 Acc: 0.9729
val Loss: 0.0084 Acc: 0.9772
Training complete in 98m 9s

Epoch 13/99
----------
train Loss: 0.1104 Acc: 0.9763
val Loss: 0.0054 Acc: 0.9801
Training complete in 106m 12s

Epoch 14/99
----------
train Loss: 0.0956 Acc: 0.9819
val Loss: 0.0048 Acc: 0.9801
Training complete in 114m 16s

Epoch 15/99
----------
train Loss: 0.0868 Acc: 0.9854
val Loss: 0.0060 Acc: 0.9786
Training complete in 122m 19s

Epoch 16/99
----------
train Loss: 0.0842 Acc: 0.9852
val Loss: 0.0053 Acc: 0.9786
Training complete in 130m 23s

Epoch 17/99
----------
train Loss: 0.0785 Acc: 0.9877
val Loss: 0.0046 Acc: 0.9801
Training complete in 138m 25s

Epoch 18/99
----------
train Loss: 0.0778 Acc: 0.9870
val Loss: 0.0060 Acc: 0.9801
Training complete in 146m 27s

Epoch 19/99
----------
train Loss: 0.0752 Acc: 0.9890
val Loss: 0.0055 Acc: 0.9801
Training complete in 154m 30s

Epoch 20/99
----------
train Loss: 0.0677 Acc: 0.9905
val Loss: 0.0060 Acc: 0.9801
Training complete in 162m 32s

Epoch 21/99
----------
train Loss: 0.0649 Acc: 0.9907
val Loss: 0.0058 Acc: 0.9801
Training complete in 170m 34s

Epoch 22/99
----------
train Loss: 0.0625 Acc: 0.9916
val Loss: 0.0062 Acc: 0.9801
Training complete in 178m 37s

Epoch 23/99
----------
train Loss: 0.0589 Acc: 0.9927
val Loss: 0.0063 Acc: 0.9801
Training complete in 186m 40s

Epoch 24/99
----------
train Loss: 0.0556 Acc: 0.9937
val Loss: 0.0059 Acc: 0.9801
Training complete in 194m 42s

Epoch 25/99
----------
train Loss: 0.0562 Acc: 0.9945
val Loss: 0.0056 Acc: 0.9801
Training complete in 202m 44s

Epoch 26/99
----------
train Loss: 0.0548 Acc: 0.9936
val Loss: 0.0055 Acc: 0.9801
Training complete in 210m 45s

Epoch 27/99
----------
train Loss: 0.0553 Acc: 0.9929
val Loss: 0.0056 Acc: 0.9801
Training complete in 218m 47s

Epoch 28/99
----------
train Loss: 0.0511 Acc: 0.9950
val Loss: 0.0056 Acc: 0.9801
Training complete in 226m 49s

Epoch 29/99
----------
train Loss: 0.0478 Acc: 0.9957
val Loss: 0.0054 Acc: 0.9801
Training complete in 234m 51s

Epoch 30/99
----------
train Loss: 0.0479 Acc: 0.9956
val Loss: 0.0048 Acc: 0.9801
Training complete in 242m 52s

Epoch 31/99
----------
train Loss: 0.0485 Acc: 0.9953
val Loss: 0.0066 Acc: 0.9801
Training complete in 250m 51s

Epoch 32/99
----------
train Loss: 0.0477 Acc: 0.9959
val Loss: 0.0056 Acc: 0.9801
Training complete in 258m 51s

Epoch 33/99
----------
train Loss: 0.0452 Acc: 0.9960
val Loss: 0.0065 Acc: 0.9801
Training complete in 266m 50s

Epoch 34/99
----------
train Loss: 0.0436 Acc: 0.9955
val Loss: 0.0068 Acc: 0.9801
Training complete in 274m 50s

Epoch 35/99
----------
train Loss: 0.0437 Acc: 0.9962
val Loss: 0.0072 Acc: 0.9801
Training complete in 282m 51s

Epoch 36/99
----------
train Loss: 0.0433 Acc: 0.9964
val Loss: 0.0056 Acc: 0.9801
Training complete in 290m 51s

Epoch 37/99
----------
train Loss: 0.0421 Acc: 0.9972
val Loss: 0.0061 Acc: 0.9801
Training complete in 298m 46s

Epoch 38/99
----------
train Loss: 0.0412 Acc: 0.9968
val Loss: 0.0060 Acc: 0.9801
Training complete in 306m 36s

Epoch 39/99
----------
train Loss: 0.0439 Acc: 0.9962
val Loss: 0.0057 Acc: 0.9801
Training complete in 314m 26s

Epoch 40/99
----------
train Loss: 0.0428 Acc: 0.9961
val Loss: 0.0065 Acc: 0.9801
Training complete in 322m 15s

Epoch 41/99
----------
train Loss: 0.0396 Acc: 0.9972
val Loss: 0.0063 Acc: 0.9801
Training complete in 330m 2s

Epoch 42/99
----------
train Loss: 0.0407 Acc: 0.9970
val Loss: 0.0063 Acc: 0.9801
Training complete in 337m 51s

Epoch 43/99
----------
train Loss: 0.0411 Acc: 0.9966
val Loss: 0.0064 Acc: 0.9801
Training complete in 345m 40s

Epoch 44/99
----------
train Loss: 0.0409 Acc: 0.9970
val Loss: 0.0050 Acc: 0.9801
Training complete in 353m 28s

Epoch 45/99
----------
train Loss: 0.0383 Acc: 0.9976
val Loss: 0.0074 Acc: 0.9801
Training complete in 361m 15s

Epoch 46/99
----------
train Loss: 0.0353 Acc: 0.9981
val Loss: 0.0081 Acc: 0.9801
Training complete in 369m 4s

Epoch 47/99
----------
train Loss: 0.0382 Acc: 0.9975
val Loss: 0.0072 Acc: 0.9801
Training complete in 376m 52s

Epoch 48/99
----------
train Loss: 0.0384 Acc: 0.9973
val Loss: 0.0063 Acc: 0.9801
Training complete in 384m 39s

Epoch 49/99
----------
train Loss: 0.0362 Acc: 0.9977
val Loss: 0.0059 Acc: 0.9801
Training complete in 392m 28s

Epoch 50/99
----------
train Loss: 0.0357 Acc: 0.9973
val Loss: 0.0073 Acc: 0.9801
Training complete in 400m 15s

Epoch 51/99
----------
train Loss: 0.0346 Acc: 0.9981
val Loss: 0.0073 Acc: 0.9801
Training complete in 408m 2s

Epoch 52/99
----------
train Loss: 0.0339 Acc: 0.9978
val Loss: 0.0059 Acc: 0.9801
Training complete in 415m 50s

Epoch 53/99
----------
train Loss: 0.0342 Acc: 0.9979
val Loss: 0.0059 Acc: 0.9801
Training complete in 423m 37s

Epoch 54/99
----------
train Loss: 0.0358 Acc: 0.9973
val Loss: 0.0066 Acc: 0.9801
Training complete in 431m 26s

Epoch 55/99
----------
train Loss: 0.0369 Acc: 0.9975
val Loss: 0.0058 Acc: 0.9801
Training complete in 439m 13s

Epoch 56/99
----------
train Loss: 0.0363 Acc: 0.9976
val Loss: 0.0060 Acc: 0.9801
Training complete in 447m 1s

Epoch 57/99
----------
train Loss: 0.0364 Acc: 0.9977
val Loss: 0.0074 Acc: 0.9801
Training complete in 454m 48s

Epoch 58/99
----------
train Loss: 0.0344 Acc: 0.9976
val Loss: 0.0065 Acc: 0.9801
Training complete in 462m 34s

Epoch 59/99
----------
train Loss: 0.0362 Acc: 0.9978
val Loss: 0.0071 Acc: 0.9801
Training complete in 470m 24s

Epoch 60/99
----------
train Loss: 0.0348 Acc: 0.9978
val Loss: 0.0071 Acc: 0.9801
Training complete in 478m 10s

Epoch 61/99
----------
train Loss: 0.0346 Acc: 0.9980
val Loss: 0.0062 Acc: 0.9801
Training complete in 485m 56s

Epoch 62/99
----------
train Loss: 0.0336 Acc: 0.9977
val Loss: 0.0066 Acc: 0.9801
Training complete in 493m 42s

Epoch 63/99
----------
train Loss: 0.0351 Acc: 0.9974
val Loss: 0.0075 Acc: 0.9801
Training complete in 501m 29s

Epoch 64/99
----------
train Loss: 0.0336 Acc: 0.9981
val Loss: 0.0060 Acc: 0.9801
Training complete in 509m 15s

Epoch 65/99
----------
train Loss: 0.0332 Acc: 0.9981
val Loss: 0.0065 Acc: 0.9801
Training complete in 517m 1s

Epoch 66/99
----------
train Loss: 0.0353 Acc: 0.9979
val Loss: 0.0065 Acc: 0.9801
Training complete in 524m 48s

Epoch 67/99
----------
train Loss: 0.0336 Acc: 0.9979
val Loss: 0.0061 Acc: 0.9801
Training complete in 532m 34s

Epoch 68/99
----------
train Loss: 0.0314 Acc: 0.9984
val Loss: 0.0067 Acc: 0.9801
Training complete in 540m 21s

Epoch 69/99
----------
train Loss: 0.0312 Acc: 0.9984
val Loss: 0.0066 Acc: 0.9801
Training complete in 548m 10s

Epoch 70/99
----------
train Loss: 0.0320 Acc: 0.9982
val Loss: 0.0074 Acc: 0.9801
Training complete in 555m 55s

Epoch 71/99
----------
train Loss: 0.0343 Acc: 0.9978
val Loss: 0.0073 Acc: 0.9801
Training complete in 563m 40s

Epoch 72/99
----------
train Loss: 0.0318 Acc: 0.9984
val Loss: 0.0071 Acc: 0.9801
Training complete in 571m 27s

Epoch 73/99
----------
train Loss: 0.0322 Acc: 0.9981
val Loss: 0.0064 Acc: 0.9801
Training complete in 579m 14s

Epoch 74/99
----------
train Loss: 0.0319 Acc: 0.9981
val Loss: 0.0068 Acc: 0.9801
Training complete in 587m 0s

Epoch 75/99
----------
train Loss: 0.0329 Acc: 0.9979
val Loss: 0.0066 Acc: 0.9801
Training complete in 594m 47s

Epoch 76/99
----------
train Loss: 0.0315 Acc: 0.9984
val Loss: 0.0083 Acc: 0.9801
Training complete in 602m 33s

Epoch 77/99
----------
train Loss: 0.0331 Acc: 0.9982
val Loss: 0.0064 Acc: 0.9801
Training complete in 610m 20s

Epoch 78/99
----------
train Loss: 0.0329 Acc: 0.9979
val Loss: 0.0070 Acc: 0.9801
Training complete in 618m 7s

Epoch 79/99
----------
train Loss: 0.0308 Acc: 0.9981
val Loss: 0.0069 Acc: 0.9801
Training complete in 625m 55s

Epoch 80/99
----------
train Loss: 0.0325 Acc: 0.9980
val Loss: 0.0063 Acc: 0.9801
Training complete in 633m 41s

Epoch 81/99
----------
train Loss: 0.0305 Acc: 0.9979
val Loss: 0.0066 Acc: 0.9801
Training complete in 641m 26s

Epoch 82/99
----------
train Loss: 0.0293 Acc: 0.9992
val Loss: 0.0068 Acc: 0.9801
Training complete in 649m 13s

Epoch 83/99
----------
train Loss: 0.0311 Acc: 0.9985
val Loss: 0.0059 Acc: 0.9801
Training complete in 656m 59s

Epoch 84/99
----------
train Loss: 0.0300 Acc: 0.9984
val Loss: 0.0068 Acc: 0.9801
Training complete in 664m 46s

Epoch 85/99
----------
train Loss: 0.0304 Acc: 0.9985
val Loss: 0.0070 Acc: 0.9801
Training complete in 672m 32s

Epoch 86/99
----------
train Loss: 0.0300 Acc: 0.9985
val Loss: 0.0068 Acc: 0.9801
Training complete in 680m 18s

Epoch 87/99
----------
train Loss: 0.0297 Acc: 0.9989
val Loss: 0.0062 Acc: 0.9801
Training complete in 688m 4s

Epoch 88/99
----------
train Loss: 0.0303 Acc: 0.9986
val Loss: 0.0066 Acc: 0.9801
Training complete in 695m 49s

Epoch 89/99
----------
train Loss: 0.0307 Acc: 0.9984
val Loss: 0.0084 Acc: 0.9801
Training complete in 703m 36s

Epoch 90/99
----------
train Loss: 0.0301 Acc: 0.9985
val Loss: 0.0066 Acc: 0.9801
Training complete in 711m 22s

Epoch 91/99
----------
train Loss: 0.0308 Acc: 0.9983
val Loss: 0.0070 Acc: 0.9801
Training complete in 719m 6s

Epoch 92/99
----------
train Loss: 0.0309 Acc: 0.9980
val Loss: 0.0076 Acc: 0.9801
Training complete in 726m 52s

Epoch 93/99
----------
train Loss: 0.0303 Acc: 0.9984
val Loss: 0.0069 Acc: 0.9801
Training complete in 734m 38s

Epoch 94/99
----------
train Loss: 0.0299 Acc: 0.9982
val Loss: 0.0065 Acc: 0.9801
Training complete in 742m 24s

Epoch 95/99
----------
train Loss: 0.0291 Acc: 0.9986
val Loss: 0.0065 Acc: 0.9801
Training complete in 750m 10s

Epoch 96/99
----------
train Loss: 0.0306 Acc: 0.9984
val Loss: 0.0069 Acc: 0.9801
Training complete in 757m 56s

Epoch 97/99
----------
train Loss: 0.0297 Acc: 0.9986
val Loss: 0.0057 Acc: 0.9801
Training complete in 765m 42s

Epoch 98/99
----------
train Loss: 0.0313 Acc: 0.9981
val Loss: 0.0059 Acc: 0.9801
Training complete in 773m 28s

Epoch 99/99
----------
train Loss: 0.0294 Acc: 0.9987
val Loss: 0.0069 Acc: 0.9801
Training complete in 781m 16s

Training complete in 781m 16s
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7f8ad84eedd8>]
2.8221161365509033
Traceback (most recent call last):
  File "train.py", line 360, in <module>
    model = resneft50(len(class_names))
  File "/home/luguangjian/tmp20/reid/model.py", line 100, in __init__
    model_ft = resnest.resneft50(pretrained=True)
AttributeError: module 'resnest.resnest' has no attribute 'resneft50'
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7fe013adfc88>]
2.852027177810669
resneft50(
  (model): ResNeFt(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
    (fpn): FeaturePyramidNetwork(
      (P5_1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (P5_upsampled): Upsample(scale_factor=2.0, mode=nearest)
      (P5_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (P4_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      (P4_upsampled): Upsample(scale_factor=2.0, mode=nearest)
      (P4_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (P3_1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      (P3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (P6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (P7_1): ReLU()
      (P7_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (classificationModel): ClassificationModel(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (act1): ReLU()
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (act2): ReLU()
      (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (act3): ReLU()
      (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (act4): ReLU()
      (output): Conv2d(256, 1000, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (output_act): Sigmoid()
    )
  )
  (nsrelu): NSReLU()
  (Sum_ReLU_2): Sum_ReLU_2()
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/99
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7fdf70b3dcc0>]
3.175603151321411
resneft50(
  (model): ResNeFt(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
    (fpn): FeaturePyramidNetwork(
      (P5_1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (P5_upsampled): Upsample(scale_factor=2.0, mode=nearest)
      (P5_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (P4_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      (P4_upsampled): Upsample(scale_factor=2.0, mode=nearest)
      (P4_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (P3_1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      (P3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (P6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (P7_1): ReLU()
      (P7_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (classificationModel): ClassificationModel(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (act1): ReLU()
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (act2): ReLU()
      (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (act3): ReLU()
      (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (act4): ReLU()
      (output): Conv2d(256, 1000, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (output_act): Sigmoid()
    )
  )
  (nsrelu): NSReLU()
  (Sum_ReLU_2): Sum_ReLU_2()
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/99
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Traceback (most recent call last):
  File "train.py", line 450, in <module>
    model = train_model(model, optimizer_ft, exp_lr_scheduler, criterion, num_epochs=100, local_rank=opt.local_rank)
  File "train.py", line 237, in train_model
    outputs = model(inputs)
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/module.py", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/luguangjian/tmp21/reid/model.py", line 174, in forward
    x = self.classifier(x)
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/module.py", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/luguangjian/tmp21/reid/model.py", line 58, in forward
    x = self.add_block(x)
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/module.py", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/container.py", line 92, in forward
    input = module(input)
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/module.py", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/nn/functional.py", line 1370, in linear
    ret = torch.addmm(bias, input, weight.t())
RuntimeError: size mismatch, m1: [8 x 64], m2: [1280 x 512] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:290
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7fe2cecb7c88>]
2.932544708251953
resneft50(
  (model): ResNeFt(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
        (downsample): Sequential(
          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): SplAtConv2d(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
          (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (nsrelu): NSReLU()
          (leakyrelu): LeakyReLU(negative_slope=0.01)
          (relu): ReLU(inplace=True)
          (Sum_ReLU_2): Sum_ReLU_2()
          (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (rsoftmax): rSoftMax()
        )
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (leakyrelu): LeakyReLU(negative_slope=0.01)
        (relu): ReLU(inplace=True)
        (Sum_ReLU_2): Sum_ReLU_2()
        (nsrelu): NSReLU()
        (global_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_down): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv_up): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sig): Sigmoid()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
    (fpn): FeaturePyramidNetwork(
      (P5_1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (P5_upsampled): Upsample(scale_factor=2.0, mode=nearest)
      (P5_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (P4_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      (P4_upsampled): Upsample(scale_factor=2.0, mode=nearest)
      (P4_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (P3_1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      (P3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (P6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (P7_1): ReLU()
      (P7_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (classificationModel): ClassificationModel(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (act1): ReLU()
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (act2): ReLU()
      (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (act3): ReLU()
      (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (act4): ReLU()
      (output): Conv2d(256, 1000, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (output_act): Sigmoid()
    )
  )
  (nsrelu): NSReLU()
  (Sum_ReLU_2): Sum_ReLU_2()
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5, inplace=False)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=702, bias=True)
    )
  )
)
Epoch 0/99
----------
/home/luguangjian/lu/yes/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 0.9080 Acc: 0.1429
val Loss: 3.4533 Acc: 0.2650
Training complete in 5m 39s

Epoch 1/99
----------
train Loss: 0.9947 Acc: 0.4080
val Loss: 1.5483 Acc: 0.5869
Training complete in 10m 50s

Epoch 2/99
----------
train Loss: 1.0720 Acc: 0.5401
val Loss: 0.9905 Acc: 0.7422
Training complete in 16m 29s

Epoch 3/99
----------
train Loss: 1.1900 Acc: 0.6118
val Loss: 0.8449 Acc: 0.7721
Training complete in 21m 56s

Epoch 4/99
----------
train Loss: 1.2860 Acc: 0.6601
val Loss: 0.7054 Acc: 0.8077
Training complete in 27m 20s

Epoch 5/99
----------
train Loss: 1.1964 Acc: 0.7027
val Loss: 0.5409 Acc: 0.8476
Training complete in 32m 55s

Epoch 6/99
----------
train Loss: 0.9518 Acc: 0.7555
val Loss: 0.4581 Acc: 0.8661
Training complete in 38m 16s

Epoch 7/99
----------
train Loss: 0.7969 Acc: 0.7971
val Loss: 0.3321 Acc: 0.9060
Training complete in 43m 28s

Epoch 8/99
----------
train Loss: 0.6842 Acc: 0.8253
val Loss: 0.3465 Acc: 0.9003
Training complete in 48m 43s

Epoch 9/99
----------
train Loss: 0.5987 Acc: 0.8456
val Loss: 0.2585 Acc: 0.9302
Training complete in 54m 6s

Epoch 10/99
----------
train Loss: 0.5268 Acc: 0.8639
val Loss: 0.2190 Acc: 0.9345
Training complete in 59m 46s

Epoch 11/99
----------
train Loss: 0.4770 Acc: 0.8764
val Loss: 0.2107 Acc: 0.9459
Training complete in 65m 27s

Epoch 12/99
----------
train Loss: 0.4021 Acc: 0.8940
val Loss: 0.2159 Acc: 0.9359
Training complete in 70m 46s

Epoch 13/99
----------
train Loss: 0.3726 Acc: 0.9018
val Loss: 0.1889 Acc: 0.9530
Training complete in 76m 21s

Epoch 14/99
----------
train Loss: 0.3400 Acc: 0.9128
val Loss: 0.2078 Acc: 0.9473
Training complete in 82m 6s

Epoch 15/99
----------
train Loss: 0.2992 Acc: 0.9252
val Loss: 0.1688 Acc: 0.9516
Training complete in 87m 48s

Epoch 16/99
----------
train Loss: 0.2767 Acc: 0.9308
val Loss: 0.1798 Acc: 0.9444
Training complete in 93m 32s

Epoch 17/99
----------
train Loss: 0.2550 Acc: 0.9371
val Loss: 0.1558 Acc: 0.9558
Training complete in 99m 19s

Epoch 18/99
----------
train Loss: 0.2215 Acc: 0.9447
val Loss: 0.1388 Acc: 0.9516
Training complete in 104m 55s

Epoch 19/99
----------
train Loss: 0.1962 Acc: 0.9537
val Loss: 0.1226 Acc: 0.9687
Training complete in 110m 42s

Epoch 20/99
----------
train Loss: 0.1837 Acc: 0.9575
val Loss: 0.1353 Acc: 0.9658
Training complete in 116m 32s

Epoch 21/99
----------
train Loss: 0.1733 Acc: 0.9602
val Loss: 0.1155 Acc: 0.9630
Training complete in 122m 13s

Epoch 22/99
----------
train Loss: 0.1580 Acc: 0.9641
val Loss: 0.1314 Acc: 0.9601
Training complete in 127m 58s

Epoch 23/99
----------
train Loss: 0.1493 Acc: 0.9679
val Loss: 0.1119 Acc: 0.9672
Training complete in 133m 45s

Epoch 24/99
----------
train Loss: 0.1287 Acc: 0.9735
val Loss: 0.0977 Acc: 0.9715
Training complete in 139m 36s

Epoch 25/99
----------
train Loss: 0.1169 Acc: 0.9762
val Loss: 0.1165 Acc: 0.9644
Training complete in 145m 22s

Epoch 26/99
----------
train Loss: 0.1092 Acc: 0.9799
val Loss: 0.1117 Acc: 0.9687
Training complete in 151m 16s

Epoch 27/99
----------
train Loss: 0.1018 Acc: 0.9827
val Loss: 0.0971 Acc: 0.9701
Training complete in 157m 2s

Epoch 28/99
----------
train Loss: 0.1003 Acc: 0.9834
val Loss: 0.0911 Acc: 0.9630
Training complete in 162m 52s

Epoch 29/99
----------
train Loss: 0.0922 Acc: 0.9838
val Loss: 0.0959 Acc: 0.9715
Training complete in 168m 31s

Epoch 30/99
----------
train Loss: 0.0836 Acc: 0.9872
val Loss: 0.1039 Acc: 0.9715
Training complete in 174m 5s

Epoch 31/99
----------
train Loss: 0.0849 Acc: 0.9882
val Loss: 0.1062 Acc: 0.9701
Training complete in 179m 36s

Epoch 32/99
----------
train Loss: 0.0834 Acc: 0.9875
val Loss: 0.1157 Acc: 0.9729
Training complete in 185m 16s

Epoch 33/99
----------
train Loss: 0.0793 Acc: 0.9889
val Loss: 0.1102 Acc: 0.9701
Training complete in 191m 4s

Epoch 34/99
----------
train Loss: 0.0757 Acc: 0.9908
val Loss: 0.1054 Acc: 0.9687
Training complete in 196m 41s

Epoch 35/99
----------
train Loss: 0.0775 Acc: 0.9889
val Loss: 0.0992 Acc: 0.9744
Training complete in 202m 19s

Epoch 36/99
----------
train Loss: 0.0807 Acc: 0.9887
val Loss: 0.0974 Acc: 0.9744
Training complete in 207m 58s

Epoch 37/99
----------
train Loss: 0.0691 Acc: 0.9909
val Loss: 0.0878 Acc: 0.9744
Training complete in 213m 37s

Epoch 38/99
----------
train Loss: 0.0658 Acc: 0.9923
val Loss: 0.1107 Acc: 0.9658
Training complete in 219m 17s

Epoch 39/99
----------
train Loss: 0.0674 Acc: 0.9914
val Loss: 0.0952 Acc: 0.9729
Training complete in 225m 1s

Epoch 40/99
----------
train Loss: 0.0595 Acc: 0.9939
val Loss: 0.0993 Acc: 0.9687
Training complete in 230m 42s

Epoch 41/99
----------
train Loss: 0.0588 Acc: 0.9941
val Loss: 0.0872 Acc: 0.9715
Training complete in 236m 27s

Epoch 42/99
----------
train Loss: 0.0592 Acc: 0.9937
val Loss: 0.0891 Acc: 0.9744
Training complete in 242m 13s

Epoch 43/99
----------
train Loss: 0.0626 Acc: 0.9929
val Loss: 0.0899 Acc: 0.9758
Training complete in 247m 60s

Epoch 44/99
----------
train Loss: 0.0567 Acc: 0.9949
val Loss: 0.0997 Acc: 0.9715
Training complete in 253m 46s

Epoch 45/99
----------
train Loss: 0.0568 Acc: 0.9949
val Loss: 0.0982 Acc: 0.9701
Training complete in 259m 28s

Epoch 46/99
----------
train Loss: 0.0554 Acc: 0.9946
val Loss: 0.0929 Acc: 0.9744
Training complete in 264m 60s

Epoch 47/99
----------
train Loss: 0.0480 Acc: 0.9966
val Loss: 0.0871 Acc: 0.9772
Training complete in 270m 40s

Epoch 48/99
----------
train Loss: 0.0538 Acc: 0.9956
val Loss: 0.1036 Acc: 0.9729
Training complete in 276m 25s

Epoch 49/99
----------
train Loss: 0.0530 Acc: 0.9954
val Loss: 0.0995 Acc: 0.9729
Training complete in 282m 4s

Epoch 50/99
----------
train Loss: 0.0495 Acc: 0.9961
val Loss: 0.0994 Acc: 0.9758
Training complete in 287m 40s

Epoch 51/99
----------
train Loss: 0.0513 Acc: 0.9961
val Loss: 0.0954 Acc: 0.9758
Training complete in 293m 27s

Epoch 52/99
----------
train Loss: 0.0497 Acc: 0.9966
val Loss: 0.0997 Acc: 0.9758
Training complete in 299m 7s

Epoch 53/99
----------
train Loss: 0.0490 Acc: 0.9966
val Loss: 0.0930 Acc: 0.9758
Training complete in 304m 42s

Epoch 54/99
----------
train Loss: 0.0474 Acc: 0.9964
val Loss: 0.0992 Acc: 0.9701
Training complete in 310m 21s

Epoch 55/99
----------
train Loss: 0.0439 Acc: 0.9976
val Loss: 0.0907 Acc: 0.9772
Training complete in 316m 4s

Epoch 56/99
----------
train Loss: 0.0485 Acc: 0.9964
val Loss: 0.0992 Acc: 0.9772
Training complete in 321m 44s

Epoch 57/99
----------
train Loss: 0.0467 Acc: 0.9967
val Loss: 0.0858 Acc: 0.9758
Training complete in 327m 31s

Epoch 58/99
----------
train Loss: 0.0459 Acc: 0.9965
val Loss: 0.1001 Acc: 0.9744
Training complete in 333m 10s

Epoch 59/99
----------
train Loss: 0.0446 Acc: 0.9972
val Loss: 0.1056 Acc: 0.9744
Training complete in 338m 55s

Epoch 60/99
----------
train Loss: 0.0460 Acc: 0.9968
val Loss: 0.0946 Acc: 0.9758
Training complete in 344m 37s

Epoch 61/99
----------
train Loss: 0.0446 Acc: 0.9976
val Loss: 0.1007 Acc: 0.9744
Training complete in 350m 11s

Epoch 62/99
----------
train Loss: 0.0454 Acc: 0.9968
val Loss: 0.1021 Acc: 0.9715
Training complete in 355m 55s

Epoch 63/99
----------
train Loss: 0.0435 Acc: 0.9975
val Loss: 0.1061 Acc: 0.9729
Training complete in 361m 32s

Epoch 64/99
----------
train Loss: 0.0432 Acc: 0.9972
val Loss: 0.1002 Acc: 0.9729
Training complete in 367m 19s

Epoch 65/99
----------
train Loss: 0.0442 Acc: 0.9973
val Loss: 0.1047 Acc: 0.9729
Training complete in 373m 1s

Epoch 66/99
----------
train Loss: 0.0434 Acc: 0.9976
val Loss: 0.0976 Acc: 0.9715
Training complete in 378m 30s

Epoch 67/99
----------
train Loss: 0.0410 Acc: 0.9977
val Loss: 0.1023 Acc: 0.9744
Training complete in 384m 1s

Epoch 68/99
----------
train Loss: 0.0463 Acc: 0.9966
val Loss: 0.0998 Acc: 0.9715
Training complete in 389m 45s

Epoch 69/99
----------
train Loss: 0.0414 Acc: 0.9975
val Loss: 0.0951 Acc: 0.9729
Training complete in 395m 29s

Epoch 70/99
----------
train Loss: 0.0418 Acc: 0.9980
val Loss: 0.1084 Acc: 0.9715
Training complete in 401m 1s

Epoch 71/99
----------
train Loss: 0.0406 Acc: 0.9974
val Loss: 0.0994 Acc: 0.9758
Training complete in 406m 37s

Epoch 72/99
----------
train Loss: 0.0411 Acc: 0.9975
val Loss: 0.0990 Acc: 0.9729
Training complete in 412m 21s

Epoch 73/99
----------
train Loss: 0.0428 Acc: 0.9972
val Loss: 0.1024 Acc: 0.9729
Training complete in 417m 57s

Epoch 74/99
----------
train Loss: 0.0393 Acc: 0.9980
val Loss: 0.1011 Acc: 0.9729
Training complete in 423m 40s

Epoch 75/99
----------
train Loss: 0.0394 Acc: 0.9982
val Loss: 0.1024 Acc: 0.9744
Training complete in 429m 24s

Epoch 76/99
----------
train Loss: 0.0426 Acc: 0.9977
val Loss: 0.0997 Acc: 0.9744
Training complete in 434m 56s

Epoch 77/99
----------
train Loss: 0.0396 Acc: 0.9982
val Loss: 0.0968 Acc: 0.9715
Training complete in 440m 39s

Epoch 78/99
----------
train Loss: 0.0397 Acc: 0.9979
val Loss: 0.0951 Acc: 0.9744
Training complete in 446m 25s

Epoch 79/99
----------
train Loss: 0.0392 Acc: 0.9978
val Loss: 0.0965 Acc: 0.9729
Training complete in 452m 12s

Epoch 80/99
----------
train Loss: 0.0391 Acc: 0.9980
val Loss: 0.0925 Acc: 0.9758
Training complete in 457m 54s

Epoch 81/99
----------
train Loss: 0.0407 Acc: 0.9976
val Loss: 0.0934 Acc: 0.9744
Training complete in 463m 34s

Epoch 82/99
----------
train Loss: 0.0404 Acc: 0.9978
val Loss: 0.0964 Acc: 0.9744
Training complete in 469m 9s

Epoch 83/99
----------
train Loss: 0.0398 Acc: 0.9980
val Loss: 0.0946 Acc: 0.9729
Training complete in 474m 14s

Epoch 84/99
----------
train Loss: 0.0383 Acc: 0.9982
val Loss: 0.1038 Acc: 0.9729
Training complete in 479m 19s

Epoch 85/99
----------
train Loss: 0.0386 Acc: 0.9983
val Loss: 0.1038 Acc: 0.9729
Training complete in 485m 1s

Epoch 86/99
----------
train Loss: 0.0384 Acc: 0.9982
val Loss: 0.1008 Acc: 0.9744
Training complete in 490m 3s

Epoch 87/99
----------
train Loss: 0.0380 Acc: 0.9987
val Loss: 0.1034 Acc: 0.9715
Training complete in 495m 5s

Epoch 88/99
----------
train Loss: 0.0395 Acc: 0.9976
val Loss: 0.1076 Acc: 0.9729
Training complete in 500m 36s

Epoch 89/99
----------
train Loss: 0.0372 Acc: 0.9985
val Loss: 0.0997 Acc: 0.9729
Training complete in 506m 6s

Epoch 90/99
----------
train Loss: 0.0384 Acc: 0.9985
val Loss: 0.1076 Acc: 0.9715
Training complete in 511m 9s

Epoch 91/99
----------
train Loss: 0.0382 Acc: 0.9981
val Loss: 0.0986 Acc: 0.9744
Training complete in 516m 42s

Epoch 92/99
----------
train Loss: 0.0371 Acc: 0.9986
val Loss: 0.1051 Acc: 0.9729
Training complete in 522m 19s

Epoch 93/99
----------
train Loss: 0.0403 Acc: 0.9982
val Loss: 0.1049 Acc: 0.9729
Training complete in 527m 45s

Epoch 94/99
----------
train Loss: 0.0365 Acc: 0.9985
val Loss: 0.1024 Acc: 0.9744
Training complete in 532m 51s

Epoch 95/99
----------
train Loss: 0.0360 Acc: 0.9988
val Loss: 0.1062 Acc: 0.9729
Training complete in 538m 33s

Epoch 96/99
----------
train Loss: 0.0378 Acc: 0.9982
val Loss: 0.0945 Acc: 0.9744
Training complete in 543m 50s

Epoch 97/99
----------
train Loss: 0.0375 Acc: 0.9984
val Loss: 0.0925 Acc: 0.9744
Training complete in 548m 55s

Epoch 98/99
----------
train Loss: 0.0374 Acc: 0.9979
val Loss: 0.0949 Acc: 0.9744
Training complete in 553m 60s

Epoch 99/99
----------
train Loss: 0.0372 Acc: 0.9982
val Loss: 0.0992 Acc: 0.9715
Training complete in 559m 20s

Training complete in 559m 20s
